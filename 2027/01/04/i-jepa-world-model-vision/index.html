<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>I-JEPA: Yann LeCun’s Vision for World Models | Software Wrighter Lab Blog</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="I-JEPA: Yann LeCun’s Vision for World Models" />
<meta name="author" content="Software Wrighter" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Predict representations, not pixels. That’s the key insight. I-JEPA (Image-based Joint-Embedding Predictive Architecture) is Yann LeCun’s first step toward human-like AI—machines that build internal models of how the world works. Resource Link Paper Self-Supervised Learning from Images (arXiv) Code facebookresearch/ijepa Blog Meta AI Blog Video Coming soon The Vision: World Models Yann LeCun’s goal: machines that develop internal models of how the world operates. This enables: Faster learning with less data Complex task planning Adaptation to unfamiliar situations Current AI learns patterns. World models learn understanding. The Problem with Current Approaches Generative Models (MAE, BERT) Input: [Image with mask] Output: Reconstructed pixels Problem: Predicting every pixel wastes capacity on unpredictable details. Extra fingers on hands. Wrong textures. Noise. Invariance-Based Models (CLIP, SimCLR) Input: [Multiple augmented views] Output: Same representation for all views Problem: Hand-crafted augmentations bake in biases. The model learns the augmentations, not the world. I-JEPA: Predict Representations Input: [Context blocks from image] Output: Representations of masked target blocks I-JEPA predicts in abstract representation space, not pixel space. It learns what matters, ignoring what doesn’t. Architecture ┌─────────────────────────────────────┐ │ Target Encoder │ Full image → target representations │ (ViT, EMA updated) │ └─────────────────────────────────────┘ ↓ ┌─────────────────────────────────────┐ │ Predictor │ Context + mask → predicted targets └─────────────────────────────────────┘ ↑ ┌─────────────────────────────────────┐ │ Context Encoder │ Visible patches → context │ (ViT, trained) │ └─────────────────────────────────────┘ Component Role Context Encoder ViT processing visible image patches Target Encoder ViT processing full image (EMA updated) Predictor Forecasts target representations from context Multi-Block Masking The key innovation: predict large semantic blocks using spatially distributed context. This forces the model to capture high-level information—object structure, scene layout, semantic content—not pixel details. Results Metric I-JEPA Training 632M params, 16 A100s, &lt;72 hours Efficiency 2-10x fewer GPU-hours than alternatives Low-shot SOTA with 12 labeled examples/class Linear probe Outperforms pixel reconstruction Depth/counting Superior on low-level vision Comparison Approach Predicts Problem MAE Pixels Wastes capacity on noise SimCLR Invariances Limited by augmentations I-JEPA Representations Learns semantics Pretrained Models Architecture Resolution Dataset Epochs ViT-H/14 224×224 ImageNet-1K 300 ViT-H/16 448×448 ImageNet-1K 300 ViT-H/14 224×224 ImageNet-22K 66 ViT-g/16 224×224 ImageNet-22K 44 Running I-JEPA git clone https://github.com/facebookresearch/ijepa cd ijepa # Install dependencies pip install torch torchvision pyyaml numpy opencv-python # Single-GPU training python main.py \ --fname configs/in1k_vith14_ep300.yaml \ --devices cuda:0 cuda:1 # Multi-GPU (SLURM) python main_distributed.py \ --fname configs/in1k_vith14_ep300.yaml \ --nodes 2 --tasks-per-node 8 The JEPA Family I-JEPA spawned a family of architectures: Model Modality Innovation I-JEPA Images Original architecture V-JEPA Video Temporal prediction VL-JEPA Vision-Language Text embedding prediction V-JEPA (Video) Trained on 2M public videos. No pretrained encoders, no text supervision. Strong motion and appearance understanding. VL-JEPA (Vision-Language) Predicts continuous text embeddings instead of tokens. 50% fewer parameters than standard VLMs. Outperforms CLIP, SigLIP2 on video tasks. Why This Matters World models &gt; pattern matching. Understanding beats memorization. Representations &gt; pixels. Abstract prediction is more efficient. No augmentation bias. Learn from data, not hand-crafted transforms. Efficiency wins. 10x less compute for better results. Key Takeaways Predict representations, not pixels. Abstract space captures semantics, ignores noise. Multi-block masking forces understanding. Large semantic predictions require real comprehension. World models are the future. LeCun’s vision points toward truly intelligent systems. The JEPA family is growing. Images → Video → Vision-Language → ??? Resources I-JEPA Paper (arXiv:2301.08243) GitHub Repository Meta AI Blog Post V-JEPA Announcement VL-JEPA Paper Predict the representation, not the pixels. That’s how you build a world model." />
<meta property="og:description" content="Predict representations, not pixels. That’s the key insight. I-JEPA (Image-based Joint-Embedding Predictive Architecture) is Yann LeCun’s first step toward human-like AI—machines that build internal models of how the world works. Resource Link Paper Self-Supervised Learning from Images (arXiv) Code facebookresearch/ijepa Blog Meta AI Blog Video Coming soon The Vision: World Models Yann LeCun’s goal: machines that develop internal models of how the world operates. This enables: Faster learning with less data Complex task planning Adaptation to unfamiliar situations Current AI learns patterns. World models learn understanding. The Problem with Current Approaches Generative Models (MAE, BERT) Input: [Image with mask] Output: Reconstructed pixels Problem: Predicting every pixel wastes capacity on unpredictable details. Extra fingers on hands. Wrong textures. Noise. Invariance-Based Models (CLIP, SimCLR) Input: [Multiple augmented views] Output: Same representation for all views Problem: Hand-crafted augmentations bake in biases. The model learns the augmentations, not the world. I-JEPA: Predict Representations Input: [Context blocks from image] Output: Representations of masked target blocks I-JEPA predicts in abstract representation space, not pixel space. It learns what matters, ignoring what doesn’t. Architecture ┌─────────────────────────────────────┐ │ Target Encoder │ Full image → target representations │ (ViT, EMA updated) │ └─────────────────────────────────────┘ ↓ ┌─────────────────────────────────────┐ │ Predictor │ Context + mask → predicted targets └─────────────────────────────────────┘ ↑ ┌─────────────────────────────────────┐ │ Context Encoder │ Visible patches → context │ (ViT, trained) │ └─────────────────────────────────────┘ Component Role Context Encoder ViT processing visible image patches Target Encoder ViT processing full image (EMA updated) Predictor Forecasts target representations from context Multi-Block Masking The key innovation: predict large semantic blocks using spatially distributed context. This forces the model to capture high-level information—object structure, scene layout, semantic content—not pixel details. Results Metric I-JEPA Training 632M params, 16 A100s, &lt;72 hours Efficiency 2-10x fewer GPU-hours than alternatives Low-shot SOTA with 12 labeled examples/class Linear probe Outperforms pixel reconstruction Depth/counting Superior on low-level vision Comparison Approach Predicts Problem MAE Pixels Wastes capacity on noise SimCLR Invariances Limited by augmentations I-JEPA Representations Learns semantics Pretrained Models Architecture Resolution Dataset Epochs ViT-H/14 224×224 ImageNet-1K 300 ViT-H/16 448×448 ImageNet-1K 300 ViT-H/14 224×224 ImageNet-22K 66 ViT-g/16 224×224 ImageNet-22K 44 Running I-JEPA git clone https://github.com/facebookresearch/ijepa cd ijepa # Install dependencies pip install torch torchvision pyyaml numpy opencv-python # Single-GPU training python main.py \ --fname configs/in1k_vith14_ep300.yaml \ --devices cuda:0 cuda:1 # Multi-GPU (SLURM) python main_distributed.py \ --fname configs/in1k_vith14_ep300.yaml \ --nodes 2 --tasks-per-node 8 The JEPA Family I-JEPA spawned a family of architectures: Model Modality Innovation I-JEPA Images Original architecture V-JEPA Video Temporal prediction VL-JEPA Vision-Language Text embedding prediction V-JEPA (Video) Trained on 2M public videos. No pretrained encoders, no text supervision. Strong motion and appearance understanding. VL-JEPA (Vision-Language) Predicts continuous text embeddings instead of tokens. 50% fewer parameters than standard VLMs. Outperforms CLIP, SigLIP2 on video tasks. Why This Matters World models &gt; pattern matching. Understanding beats memorization. Representations &gt; pixels. Abstract prediction is more efficient. No augmentation bias. Learn from data, not hand-crafted transforms. Efficiency wins. 10x less compute for better results. Key Takeaways Predict representations, not pixels. Abstract space captures semantics, ignores noise. Multi-block masking forces understanding. Large semantic predictions require real comprehension. World models are the future. LeCun’s vision points toward truly intelligent systems. The JEPA family is growing. Images → Video → Vision-Language → ??? Resources I-JEPA Paper (arXiv:2301.08243) GitHub Repository Meta AI Blog Post V-JEPA Announcement VL-JEPA Paper Predict the representation, not the pixels. That’s how you build a world model." />
<link rel="canonical" href="http://localhost:5907/2027/01/04/i-jepa-world-model-vision/" />
<meta property="og:url" content="http://localhost:5907/2027/01/04/i-jepa-world-model-vision/" />
<meta property="og:site_name" content="Software Wrighter Lab Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2027-01-04T09:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="I-JEPA: Yann LeCun’s Vision for World Models" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Software Wrighter"},"dateModified":"2027-01-04T09:00:00-08:00","datePublished":"2027-01-04T09:00:00-08:00","description":"Predict representations, not pixels. That’s the key insight. I-JEPA (Image-based Joint-Embedding Predictive Architecture) is Yann LeCun’s first step toward human-like AI—machines that build internal models of how the world works. Resource Link Paper Self-Supervised Learning from Images (arXiv) Code facebookresearch/ijepa Blog Meta AI Blog Video Coming soon The Vision: World Models Yann LeCun’s goal: machines that develop internal models of how the world operates. This enables: Faster learning with less data Complex task planning Adaptation to unfamiliar situations Current AI learns patterns. World models learn understanding. The Problem with Current Approaches Generative Models (MAE, BERT) Input: [Image with mask] Output: Reconstructed pixels Problem: Predicting every pixel wastes capacity on unpredictable details. Extra fingers on hands. Wrong textures. Noise. Invariance-Based Models (CLIP, SimCLR) Input: [Multiple augmented views] Output: Same representation for all views Problem: Hand-crafted augmentations bake in biases. The model learns the augmentations, not the world. I-JEPA: Predict Representations Input: [Context blocks from image] Output: Representations of masked target blocks I-JEPA predicts in abstract representation space, not pixel space. It learns what matters, ignoring what doesn’t. Architecture ┌─────────────────────────────────────┐ │ Target Encoder │ Full image → target representations │ (ViT, EMA updated) │ └─────────────────────────────────────┘ ↓ ┌─────────────────────────────────────┐ │ Predictor │ Context + mask → predicted targets └─────────────────────────────────────┘ ↑ ┌─────────────────────────────────────┐ │ Context Encoder │ Visible patches → context │ (ViT, trained) │ └─────────────────────────────────────┘ Component Role Context Encoder ViT processing visible image patches Target Encoder ViT processing full image (EMA updated) Predictor Forecasts target representations from context Multi-Block Masking The key innovation: predict large semantic blocks using spatially distributed context. This forces the model to capture high-level information—object structure, scene layout, semantic content—not pixel details. Results Metric I-JEPA Training 632M params, 16 A100s, &lt;72 hours Efficiency 2-10x fewer GPU-hours than alternatives Low-shot SOTA with 12 labeled examples/class Linear probe Outperforms pixel reconstruction Depth/counting Superior on low-level vision Comparison Approach Predicts Problem MAE Pixels Wastes capacity on noise SimCLR Invariances Limited by augmentations I-JEPA Representations Learns semantics Pretrained Models Architecture Resolution Dataset Epochs ViT-H/14 224×224 ImageNet-1K 300 ViT-H/16 448×448 ImageNet-1K 300 ViT-H/14 224×224 ImageNet-22K 66 ViT-g/16 224×224 ImageNet-22K 44 Running I-JEPA git clone https://github.com/facebookresearch/ijepa cd ijepa # Install dependencies pip install torch torchvision pyyaml numpy opencv-python # Single-GPU training python main.py \\ --fname configs/in1k_vith14_ep300.yaml \\ --devices cuda:0 cuda:1 # Multi-GPU (SLURM) python main_distributed.py \\ --fname configs/in1k_vith14_ep300.yaml \\ --nodes 2 --tasks-per-node 8 The JEPA Family I-JEPA spawned a family of architectures: Model Modality Innovation I-JEPA Images Original architecture V-JEPA Video Temporal prediction VL-JEPA Vision-Language Text embedding prediction V-JEPA (Video) Trained on 2M public videos. No pretrained encoders, no text supervision. Strong motion and appearance understanding. VL-JEPA (Vision-Language) Predicts continuous text embeddings instead of tokens. 50% fewer parameters than standard VLMs. Outperforms CLIP, SigLIP2 on video tasks. Why This Matters World models &gt; pattern matching. Understanding beats memorization. Representations &gt; pixels. Abstract prediction is more efficient. No augmentation bias. Learn from data, not hand-crafted transforms. Efficiency wins. 10x less compute for better results. Key Takeaways Predict representations, not pixels. Abstract space captures semantics, ignores noise. Multi-block masking forces understanding. Large semantic predictions require real comprehension. World models are the future. LeCun’s vision points toward truly intelligent systems. The JEPA family is growing. Images → Video → Vision-Language → ??? Resources I-JEPA Paper (arXiv:2301.08243) GitHub Repository Meta AI Blog Post V-JEPA Announcement VL-JEPA Paper Predict the representation, not the pixels. That’s how you build a world model.","headline":"I-JEPA: Yann LeCun’s Vision for World Models","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:5907/2027/01/04/i-jepa-world-model-vision/"},"url":"http://localhost:5907/2027/01/04/i-jepa-world-model-vision/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:5907/feed.xml" title="Software Wrighter Lab Blog" /><!-- Theme toggle script - load early to prevent flash -->
  <script>
    (function() {
      var stored = localStorage.getItem('sw-lab-theme');
      var theme = stored;
      if (!theme) {
        theme = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
      }
      document.documentElement.setAttribute('data-theme', theme);
    })();
  </script>
  <!-- Font size - load early to prevent flash -->
  <script>
    (function() {
      var NEW_DEFAULT = 110;
      var OLD_DEFAULT = 150;
      var stored = localStorage.getItem('sw-lab-font-size');
      var ack = localStorage.getItem('sw-lab-prefs-ack');
      var size = stored ? parseInt(stored, 10) : null;

      // Migration: if on old default and not acknowledged, use new default
      if (size === OLD_DEFAULT && ack !== 'true') {
        size = NEW_DEFAULT;
      } else if (size === null) {
        size = NEW_DEFAULT;
      }

      // Apply to post content when DOM is ready
      document.addEventListener('DOMContentLoaded', function() {
        var pc = document.querySelector('.post-content');
        if (pc) pc.style.fontSize = size + '%';
        var pl = document.querySelector('.post-list');
        if (pl) pl.style.fontSize = size + '%';
      });
    })();
  </script>
  <script src="/assets/js/theme-toggle.js" defer></script>
  <script src="/assets/js/font-size.js" defer></script>
  <script src="/assets/js/preferences.js" defer></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">
      <img src="/assets/images/site/logo.jpg" alt="Software Wrighter Lab Blog" class="site-logo">
      <span class="site-title-text">Software Wrighter Lab Blog</span>
    </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/abstracts/">Abstracts</a><a class="page-link" href="/index-all/">Index</a><a class="page-link" href="/series/">Series</a><a class="page-link" href="/tags/">Tags</a><a class="page-link" href="/categories/">Categories</a><a class="page-link" href="/search/">Search</a><div class="font-size-controls">
  <button class="font-size-btn" id="font-decrease" title="Decrease font size" aria-label="Decrease font size">A-</button>
  <button class="font-size-btn" id="font-reset" title="Reset font size" aria-label="Reset font size">A</button>
  <button class="font-size-btn" id="font-increase" title="Increase font size" aria-label="Increase font size">A+</button>
</div>
<button class="theme-toggle" aria-label="Switch theme" title="Switch theme">
  <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
  </svg>
  <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
  </svg>
</button>
</div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">I-JEPA: Yann LeCun&#39;s Vision for World Models</h1><p class="post-meta">January 4, 2027 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">749 words</span> &bull; <span class="post-read-time">4 min read</span></em></p><div class="post-taxonomies"><span class="post-categories"><a href="/categories/#llm" class="category">llm</a><a href="/categories/#machine-learning" class="category">machine-learning</a><a href="/categories/#research" class="category">research</a></span><span class="post-tags"><a href="/tags/#i-jepa" class="tag">i-jepa</a><a href="/tags/#jepa" class="tag">jepa</a><a href="/tags/#meta" class="tag">meta</a><a href="/tags/#yann-lecun" class="tag">yann-lecun</a><a href="/tags/#self-supervised-learning" class="tag">self-supervised-learning</a><a href="/tags/#vision-transformer" class="tag">vision-transformer</a><a href="/tags/#world-models" class="tag">world-models</a></span></div></header><nav class="toc" data-toc-id="default">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content e-content" itemprop="articleBody">
    <p><img src="/assets/images/posts/hummingbird-flower.png" class="post-marker" alt="" /></p>

<p>Predict representations, not pixels. That’s the key insight.</p>

<p><strong>I-JEPA</strong> (Image-based Joint-Embedding Predictive Architecture) is Yann LeCun’s first step toward human-like AI—machines that build internal models of how the world works.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2301.08243">Self-Supervised Learning from Images (arXiv)</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/facebookresearch/ijepa">facebookresearch/ijepa</a></td>
      </tr>
      <tr>
        <td><strong>Blog</strong></td>
        <td><a href="https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/">Meta AI Blog</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td>Coming soon</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-vision-world-models">The Vision: World Models</h2>

<p>Yann LeCun’s goal: machines that develop internal models of how the world operates. This enables:</p>

<ul>
  <li>Faster learning with less data</li>
  <li>Complex task planning</li>
  <li>Adaptation to unfamiliar situations</li>
</ul>

<p>Current AI learns patterns. World models learn <em>understanding</em>.</p>

<h2 id="the-problem-with-current-approaches">The Problem with Current Approaches</h2>

<h3 id="generative-models-mae-bert">Generative Models (MAE, BERT)</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: [Image with mask]
Output: Reconstructed pixels
</code></pre></div></div>

<p>Problem: Predicting every pixel wastes capacity on unpredictable details. Extra fingers on hands. Wrong textures. Noise.</p>

<h3 id="invariance-based-models-clip-simclr">Invariance-Based Models (CLIP, SimCLR)</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: [Multiple augmented views]
Output: Same representation for all views
</code></pre></div></div>

<p>Problem: Hand-crafted augmentations bake in biases. The model learns the augmentations, not the world.</p>

<h2 id="i-jepa-predict-representations">I-JEPA: Predict Representations</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: [Context blocks from image]
Output: Representations of masked target blocks
</code></pre></div></div>

<p>I-JEPA predicts in <strong>abstract representation space</strong>, not pixel space. It learns what matters, ignoring what doesn’t.</p>

<h2 id="architecture">Architecture</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────────┐
│         Target Encoder              │  Full image → target representations
│         (ViT, EMA updated)          │
└─────────────────────────────────────┘
                 ↓
┌─────────────────────────────────────┐
│           Predictor                 │  Context + mask → predicted targets
└─────────────────────────────────────┘
                 ↑
┌─────────────────────────────────────┐
│        Context Encoder              │  Visible patches → context
│         (ViT, trained)              │
└─────────────────────────────────────┘
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Role</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Context Encoder</strong></td>
      <td>ViT processing visible image patches</td>
    </tr>
    <tr>
      <td><strong>Target Encoder</strong></td>
      <td>ViT processing full image (EMA updated)</td>
    </tr>
    <tr>
      <td><strong>Predictor</strong></td>
      <td>Forecasts target representations from context</td>
    </tr>
  </tbody>
</table>

<h3 id="multi-block-masking">Multi-Block Masking</h3>

<p>The key innovation: predict <strong>large semantic blocks</strong> using spatially distributed context.</p>

<p>This forces the model to capture high-level information—object structure, scene layout, semantic content—not pixel details.</p>

<h2 id="results">Results</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>I-JEPA</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Training</strong></td>
      <td>632M params, 16 A100s, &lt;72 hours</td>
    </tr>
    <tr>
      <td><strong>Efficiency</strong></td>
      <td>2-10x fewer GPU-hours than alternatives</td>
    </tr>
    <tr>
      <td><strong>Low-shot</strong></td>
      <td>SOTA with 12 labeled examples/class</td>
    </tr>
    <tr>
      <td><strong>Linear probe</strong></td>
      <td>Outperforms pixel reconstruction</td>
    </tr>
    <tr>
      <td><strong>Depth/counting</strong></td>
      <td>Superior on low-level vision</td>
    </tr>
  </tbody>
</table>

<h3 id="comparison">Comparison</h3>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>Predicts</th>
      <th>Problem</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>MAE</td>
      <td>Pixels</td>
      <td>Wastes capacity on noise</td>
    </tr>
    <tr>
      <td>SimCLR</td>
      <td>Invariances</td>
      <td>Limited by augmentations</td>
    </tr>
    <tr>
      <td><strong>I-JEPA</strong></td>
      <td><strong>Representations</strong></td>
      <td>Learns semantics</td>
    </tr>
  </tbody>
</table>

<h2 id="pretrained-models">Pretrained Models</h2>

<table>
  <thead>
    <tr>
      <th>Architecture</th>
      <th>Resolution</th>
      <th>Dataset</th>
      <th>Epochs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-H/14</td>
      <td>224×224</td>
      <td>ImageNet-1K</td>
      <td>300</td>
    </tr>
    <tr>
      <td>ViT-H/16</td>
      <td>448×448</td>
      <td>ImageNet-1K</td>
      <td>300</td>
    </tr>
    <tr>
      <td>ViT-H/14</td>
      <td>224×224</td>
      <td>ImageNet-22K</td>
      <td>66</td>
    </tr>
    <tr>
      <td>ViT-g/16</td>
      <td>224×224</td>
      <td>ImageNet-22K</td>
      <td>44</td>
    </tr>
  </tbody>
</table>

<h2 id="running-i-jepa">Running I-JEPA</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/facebookresearch/ijepa
<span class="nb">cd </span>ijepa

<span class="c"># Install dependencies</span>
pip <span class="nb">install </span>torch torchvision pyyaml numpy opencv-python

<span class="c"># Single-GPU training</span>
python main.py <span class="se">\</span>
  <span class="nt">--fname</span> configs/in1k_vith14_ep300.yaml <span class="se">\</span>
  <span class="nt">--devices</span> cuda:0 cuda:1

<span class="c"># Multi-GPU (SLURM)</span>
python main_distributed.py <span class="se">\</span>
  <span class="nt">--fname</span> configs/in1k_vith14_ep300.yaml <span class="se">\</span>
  <span class="nt">--nodes</span> 2 <span class="nt">--tasks-per-node</span> 8
</code></pre></div></div>

<h2 id="the-jepa-family">The JEPA Family</h2>

<p>I-JEPA spawned a family of architectures:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Modality</th>
      <th>Innovation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>I-JEPA</strong></td>
      <td>Images</td>
      <td>Original architecture</td>
    </tr>
    <tr>
      <td><strong>V-JEPA</strong></td>
      <td>Video</td>
      <td>Temporal prediction</td>
    </tr>
    <tr>
      <td><strong>VL-JEPA</strong></td>
      <td>Vision-Language</td>
      <td>Text embedding prediction</td>
    </tr>
  </tbody>
</table>

<h3 id="v-jepa-video">V-JEPA (Video)</h3>

<p>Trained on 2M public videos. No pretrained encoders, no text supervision. Strong motion and appearance understanding.</p>

<h3 id="vl-jepa-vision-language">VL-JEPA (Vision-Language)</h3>

<p>Predicts continuous text embeddings instead of tokens. 50% fewer parameters than standard VLMs. Outperforms CLIP, SigLIP2 on video tasks.</p>

<h2 id="why-this-matters">Why This Matters</h2>

<ol>
  <li>
    <p><strong>World models &gt; pattern matching.</strong> Understanding beats memorization.</p>
  </li>
  <li>
    <p><strong>Representations &gt; pixels.</strong> Abstract prediction is more efficient.</p>
  </li>
  <li>
    <p><strong>No augmentation bias.</strong> Learn from data, not hand-crafted transforms.</p>
  </li>
  <li>
    <p><strong>Efficiency wins.</strong> 10x less compute for better results.</p>
  </li>
</ol>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>Predict representations, not pixels.</strong> Abstract space captures semantics, ignores noise.</p>
  </li>
  <li>
    <p><strong>Multi-block masking forces understanding.</strong> Large semantic predictions require real comprehension.</p>
  </li>
  <li>
    <p><strong>World models are the future.</strong> LeCun’s vision points toward truly intelligent systems.</p>
  </li>
  <li>
    <p><strong>The JEPA family is growing.</strong> Images → Video → Vision-Language → ???</p>
  </li>
</ol>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2301.08243">I-JEPA Paper (arXiv:2301.08243)</a></li>
  <li><a href="https://github.com/facebookresearch/ijepa">GitHub Repository</a></li>
  <li><a href="https://ai.meta.com/blog/yann-lecun-ai-model-i-jepa/">Meta AI Blog Post</a></li>
  <li><a href="https://www.marktechpost.com/2025/02/22/meta-ai-releases-the-video-joint-embedding-predictive-architecture-v-jepa-model/">V-JEPA Announcement</a></li>
  <li><a href="https://arxiv.org/abs/2512.10942">VL-JEPA Paper</a></li>
</ul>

<hr />

<p><em>Predict the representation, not the pixels. That’s how you build a world model.</em></p>

  </div>





<img src="/assets/images/site/post-separator.png" class="post-separator" alt=""><a class="u-url" href="/2027/01/04/i-jepa-world-model-vision/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Software Wrighter Lab Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Mike Wright</li><li><a class="u-email" href="mailto:"></a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/softwarewrighter"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">softwarewrighter</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>AI coding agents, systems programming, and practical machine learning</p>
      </div>
    </div>

    <div class="footer-copyright">
      <p>Copyright &copy; 2026 Michael A. Wright</p>
    </div>

  </div>

  <button id="scroll-to-top" aria-label="Scroll to top" title="Scroll to top">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <polyline points="18 15 12 9 6 15"></polyline>
    </svg>
  </button>

  <style>
  #scroll-to-top {
    position: fixed;
    bottom: 2rem;
    right: 2rem;
    width: 44px;
    height: 44px;
    border-radius: 50%;
    border: none;
    background: var(--brand-color, #2a7ae2);
    color: white;
    cursor: pointer;
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.3s, visibility 0.3s, transform 0.2s;
    z-index: 1000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
  }

  #scroll-to-top:hover {
    transform: scale(1.1);
  }

  #scroll-to-top.visible {
    opacity: 1;
    visibility: visible;
  }

  #scroll-to-top svg {
    width: 24px;
    height: 24px;
  }
  </style>

  <script>
  (function() {
    const btn = document.getElementById('scroll-to-top');
    if (!btn) return;

    window.addEventListener('scroll', function() {
      if (window.scrollY > 300) {
        btn.classList.add('visible');
      } else {
        btn.classList.remove('visible');
      }
    });

    btn.addEventListener('click', function() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    });
  })();
  </script>

</footer>
</body>

</html>
