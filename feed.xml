<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://software-wrighter-lab.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://software-wrighter-lab.github.io/" rel="alternate" type="text/html" /><updated>2026-02-23T20:24:53-08:00</updated><id>https://software-wrighter-lab.github.io/feed.xml</id><title type="html">Software Wrighter Lab Blog</title><subtitle>AI coding agents, systems programming, and practical machine learning</subtitle><author><name>Mike Wright</name></author><entry><title type="html">midi-cli-rs: Extending with Custom Mood Packs</title><link href="https://software-wrighter-lab.github.io/2026/02/23/midi-cli-rs-extending-with-custom-mood-packs/" rel="alternate" type="text/html" title="midi-cli-rs: Extending with Custom Mood Packs" /><published>2026-02-23T14:00:00-08:00</published><updated>2026-02-23T14:00:00-08:00</updated><id>https://software-wrighter-lab.github.io/2026/02/23/midi-cli-rs-extending-with-custom-mood-packs</id><content type="html" xml:base="https://software-wrighter-lab.github.io/2026/02/23/midi-cli-rs-extending-with-custom-mood-packs/"><![CDATA[<p><img src="/assets/images/posts/block-galaxy.png" class="post-marker" alt="" /></p>

<p>Personal Software doesn’t stop at “it works.” It evolves. After building midi-cli-rs for AI agents to generate music, I wanted more moods—without recompiling Rust every time.</p>

<p>The solution: a plugin system that lets anyone create custom mood packs using simple TOML files.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Examples</strong></td>
        <td><a href="https://softwarewrighter.github.io/midi-cli-rs/">Listen to Samples</a></td>
      </tr>
      <tr>
        <td><strong>Wiki</strong></td>
        <td><a href="https://github.com/softwarewrighter/midi-cli-rs/wiki/Plugins-and-Extensibility">Plugin Documentation</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/watch?v=f13s4K60mV8">Plugin Moods Explainer</a><br /><a href="https://www.youtube.com/watch?v=f13s4K60mV8"><img src="https://img.youtube.com/vi/f13s4K60mV8/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/midi-cli-rs">midi-cli-rs</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-problem-with-built-in-only">The Problem with Built-in Only</h2>

<p>The original midi-cli-rs shipped with a handful of mood presets: suspense, eerie, upbeat, calm, ambient, jazz. Useful, but limited. What if you want synthwave? Chillout? Something faster or in a different key?</p>

<p>Hardcoding every possible mood isn’t practical. And asking users to modify Rust source code isn’t friendly.</p>

<h2 id="three-levels-of-extensibility">Three Levels of Extensibility</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th>Level</th>
      <th>What You Get</th>
      <th>What You Change</th>
      <th>Skill Required</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><span style="color:green">✓</span></td>
      <td><strong>Built-in Moods</strong></td>
      <td>9 curated generators</td>
      <td>Nothing—use as-is</td>
      <td>None</td>
    </tr>
    <tr>
      <td style="text-align: center"><span style="color:green">✓</span></td>
      <td><strong>Plugin Moods</strong></td>
      <td>Parameter variations</td>
      <td>TOML config files</td>
      <td>Text editing</td>
    </tr>
    <tr>
      <td style="text-align: center"><span style="color:red">✗</span></td>
      <td><strong>Custom Generators</strong></td>
      <td>New musical patterns</td>
      <td>Rust source code</td>
      <td>Programming (future)</td>
    </tr>
  </tbody>
</table>

<p>This post covers <strong>Plugin Moods</strong>—the middle tier. You can preset combinations of tempo, key, and intensity, but you’re still using the built-in generators’ musical logic. Want a “smooth-jazz” preset (slower, mellower)? Plugin mood. Want bebop or Latin jazz with different chord progressions? That requires a custom generator.</p>

<p>Custom generators (writing new Rust code) will be covered in a future post when the plugin editor ships.</p>

<h2 id="the-plugin-architecture">The Plugin Architecture</h2>

<p>Custom moods live in <code class="language-plaintext highlighter-rouge">~/.midi-cli-rs/moods/</code> as TOML files. Each file is a “mood pack” that can define multiple moods. The CLI discovers them automatically.</p>

<p>Here’s how it works:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>~/.midi-cli-rs/
└── moods/
    ├── electronic.toml    # Your synthwave, techno, etc.
    ├── cinematic.toml     # Epic, tension, wonder
    └── seasonal.toml      # Holiday themes
</code></pre></div></div>

<h2 id="creating-a-mood-pack">Creating a Mood Pack</h2>

<p>A plug-in mood pack has two parts: pack metadata and mood definitions.</p>

<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">[</span><span class="n">pack</span><span class="k">]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"electronic"</span>
<span class="n">version</span> <span class="o">=</span><span class="w"> </span><span class="s">"1.0.0"</span>
<span class="n">author</span> <span class="o">=</span><span class="w"> </span><span class="s">"Your Name"</span>
<span class="n">description</span> <span class="o">=</span><span class="w"> </span><span class="s">"Electronic music styles"</span>

<span class="k">[[</span><span class="n">moods</span><span class="k">]]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"synthwave"</span>
<span class="n">base_mood</span> <span class="o">=</span><span class="w"> </span><span class="s">"upbeat"</span>
<span class="n">default_tempo</span> <span class="o">=</span><span class="w"> </span><span class="mi">118</span>
<span class="n">default_key</span> <span class="o">=</span><span class="w"> </span><span class="s">"Am"</span>
<span class="n">default_intensity</span> <span class="o">=</span><span class="w"> </span><span class="mi">65</span>
<span class="n">description</span> <span class="o">=</span><span class="w"> </span><span class="s">"80s synthwave vibes"</span>
<span class="n">tags</span> <span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s">"electronic"</span><span class="p">,</span> <span class="s">"retro"</span><span class="p">]</span>

<span class="k">[[</span><span class="n">moods</span><span class="k">]]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"chillout"</span>
<span class="n">base_mood</span> <span class="o">=</span><span class="w"> </span><span class="s">"ambient"</span>
<span class="n">default_tempo</span> <span class="o">=</span><span class="w"> </span><span class="mi">85</span>
<span class="n">default_key</span> <span class="o">=</span><span class="w"> </span><span class="s">"Em"</span>
<span class="n">default_intensity</span> <span class="o">=</span><span class="w"> </span><span class="mi">40</span>
<span class="n">description</span> <span class="o">=</span><span class="w"> </span><span class="s">"Relaxed electronic"</span>
</code></pre></div></div>

<p>Each mood <strong>delegates</strong> to a built-in generator (<code class="language-plaintext highlighter-rouge">base_mood</code>) but overrides specific parameters. You get the musical logic of the built-in mood with your customizations applied.</p>

<h2 id="available-base-moods">Available Base Moods</h2>

<p>Your custom moods can extend any of the nine built-in generators:</p>

<table>
  <thead>
    <tr>
      <th>Base Mood</th>
      <th>Character</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">suspense</code></td>
      <td>Tense, building</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">eerie</code></td>
      <td>Dark, unsettling</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">upbeat</code></td>
      <td>Energetic, positive</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">calm</code></td>
      <td>Peaceful, slow</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">ambient</code></td>
      <td>Atmospheric, textural</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">jazz</code></td>
      <td>Swing, improvisation</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">chiptune</code></td>
      <td>8-bit, retro gaming</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">orchestral</code></td>
      <td>Classical instruments</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">show</code></td>
      <td>Broadway, theatrical</td>
    </tr>
  </tbody>
</table>

<h2 id="configuration-options">Configuration Options</h2>

<p>Each mood definition supports these overrides:</p>

<table>
  <thead>
    <tr>
      <th>Field</th>
      <th>Description</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">name</code></td>
      <td>CLI name (required)</td>
      <td><code class="language-plaintext highlighter-rouge">"synthwave"</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">base_mood</code></td>
      <td>Built-in to extend (required)</td>
      <td><code class="language-plaintext highlighter-rouge">"upbeat"</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">default_tempo</code></td>
      <td>BPM</td>
      <td><code class="language-plaintext highlighter-rouge">118</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">default_key</code></td>
      <td>Musical key</td>
      <td><code class="language-plaintext highlighter-rouge">"Am"</code>, <code class="language-plaintext highlighter-rouge">"C"</code>, <code class="language-plaintext highlighter-rouge">"Eb"</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">default_intensity</code></td>
      <td>0-100 energy level</td>
      <td><code class="language-plaintext highlighter-rouge">65</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">description</code></td>
      <td>Human-readable description</td>
      <td><code class="language-plaintext highlighter-rouge">"80s vibes"</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">tags</code></td>
      <td>Discovery tags</td>
      <td><code class="language-plaintext highlighter-rouge">["electronic", "retro"]</code></td>
    </tr>
  </tbody>
</table>

<h2 id="how-seeds-create-variation">How Seeds Create Variation</h2>

<p>Seeds aren’t random—they’re deterministic variation selectors. The same mood + same seed always produces identical output. But different seeds create observable musical differences across multiple dimensions:</p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Variation Range</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Tempo</td>
      <td>±15% from base</td>
    </tr>
    <tr>
      <td>Layer inclusion</td>
      <td>Which instruments appear</td>
    </tr>
    <tr>
      <td>Melodic contour</td>
      <td>16 different phrase shapes</td>
    </tr>
    <tr>
      <td>Note density</td>
      <td>0.6x to 1.4x</td>
    </tr>
    <tr>
      <td>Rest probability</td>
      <td>0% to 35% silence</td>
    </tr>
    <tr>
      <td>Phrase length</td>
      <td>3-8 notes</td>
    </tr>
    <tr>
      <td>Velocity</td>
      <td>-15 to +15 offset</td>
    </tr>
  </tbody>
</table>

<p>The system uses hash-based mixing with unique salts for each parameter. This means adjacent seeds (42 vs 43) produce completely different outputs—no gradual transitions between seeds.</p>

<p>When you combine plugin moods with seed variation, you get a matrix: your custom tempo/key/intensity settings applied across different seed-driven variations of the underlying generator’s patterns.</p>

<h2 id="using-custom-moods">Using Custom Moods</h2>

<p>Once your TOML file is in place, the mood appears automatically:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># List all moods (built-in + plugins)</span>
midi-cli-rs moods

<span class="c"># Generate with your custom mood</span>
midi-cli-rs preset <span class="nt">-m</span> synthwave <span class="nt">-d</span> 5 <span class="nt">-s</span> 42 <span class="nt">-o</span> output.wav
</code></pre></div></div>

<p>The seed system still works—same mood + same seed = identical output.</p>

<h2 id="example-electronic-pack">Example: Electronic Pack</h2>

<p>Here’s a complete pack with four electronic moods:</p>

<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">[</span><span class="n">pack</span><span class="k">]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"electronic"</span>
<span class="n">version</span> <span class="o">=</span><span class="w"> </span><span class="s">"1.0.0"</span>
<span class="n">description</span> <span class="o">=</span><span class="w"> </span><span class="s">"Electronic music styles"</span>

<span class="k">[[</span><span class="n">moods</span><span class="k">]]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"synthwave"</span>
<span class="n">base_mood</span> <span class="o">=</span><span class="w"> </span><span class="s">"upbeat"</span>
<span class="n">default_tempo</span> <span class="o">=</span><span class="w"> </span><span class="mi">118</span>
<span class="n">default_key</span> <span class="o">=</span><span class="w"> </span><span class="s">"Am"</span>
<span class="n">default_intensity</span> <span class="o">=</span><span class="w"> </span><span class="mi">65</span>

<span class="k">[[</span><span class="n">moods</span><span class="k">]]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"chillout"</span>
<span class="n">base_mood</span> <span class="o">=</span><span class="w"> </span><span class="s">"ambient"</span>
<span class="n">default_tempo</span> <span class="o">=</span><span class="w"> </span><span class="mi">85</span>
<span class="n">default_key</span> <span class="o">=</span><span class="w"> </span><span class="s">"Em"</span>
<span class="n">default_intensity</span> <span class="o">=</span><span class="w"> </span><span class="mi">40</span>

<span class="k">[[</span><span class="n">moods</span><span class="k">]]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"techno"</span>
<span class="n">base_mood</span> <span class="o">=</span><span class="w"> </span><span class="s">"upbeat"</span>
<span class="n">default_tempo</span> <span class="o">=</span><span class="w"> </span><span class="mi">130</span>
<span class="n">default_key</span> <span class="o">=</span><span class="w"> </span><span class="s">"Dm"</span>
<span class="n">default_intensity</span> <span class="o">=</span><span class="w"> </span><span class="mi">85</span>

<span class="k">[[</span><span class="n">moods</span><span class="k">]]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"8bit"</span>
<span class="n">base_mood</span> <span class="o">=</span><span class="w"> </span><span class="s">"chiptune"</span>
<span class="n">default_tempo</span> <span class="o">=</span><span class="w"> </span><span class="mi">140</span>
<span class="n">default_key</span> <span class="o">=</span><span class="w"> </span><span class="s">"C"</span>
<span class="n">default_intensity</span> <span class="o">=</span><span class="w"> </span><span class="mi">70</span>
</code></pre></div></div>

<p>Drop this in <code class="language-plaintext highlighter-rouge">~/.midi-cli-rs/moods/electronic.toml</code> and you have four new moods.</p>

<h2 id="whats-next">What’s Next</h2>

<p>This plugin system handles <strong>mood variations</strong>—different tempos, keys, and intensities applied to existing generators. A future update will add a <strong>plugin editor</strong> for creating entirely new musical patterns without writing Rust.</p>

<p>For now, the delegation model covers most use cases: want faster jazz? Darker ambient? Major-key suspense? Create a TOML file and you’re done.</p>

<h2 id="the-personal-software-pattern">The Personal Software Pattern</h2>

<p>This follows the Personal Software philosophy: start with something that works, then extend it as needs emerge. The plugin system wasn’t in the original design. It grew from actual use—wanting more moods without recompiling.</p>

<p>Good personal software leaves room to grow.</p>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 3 of the Personal Software series. <a href="/series/#personal-software">View all parts</a></td>
      <td><a href="/2026/02/20/midi-cli-rs-music-for-ai-agents/">← Previous: midi-cli-rs</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Personal software that evolves with your needs.</em></p>]]></content><author><name>Software Wrighter</name></author><category term="tools" /><category term="rust" /><category term="ai-agents" /><category term="vibe-coding" /><category term="rust" /><category term="midi" /><category term="music" /><category term="ai-agents" /><category term="cli" /><category term="plugins" /><category term="extensibility" /><category term="vibe-coding" /><category term="personal-software" /><summary type="html"><![CDATA[Personal Software doesn’t stop at “it works.” It evolves. After building midi-cli-rs for AI agents to generate music, I wanted more moods—without recompiling Rust every time. The solution: a plugin system that lets anyone create custom mood packs using simple TOML files. Resource Link Examples Listen to Samples Wiki Plugin Documentation Video Plugin Moods Explainer Code midi-cli-rs The Problem with Built-in Only The original midi-cli-rs shipped with a handful of mood presets: suspense, eerie, upbeat, calm, ambient, jazz. Useful, but limited. What if you want synthwave? Chillout? Something faster or in a different key? Hardcoding every possible mood isn’t practical. And asking users to modify Rust source code isn’t friendly. Three Levels of Extensibility   Level What You Get What You Change Skill Required ✓ Built-in Moods 9 curated generators Nothing—use as-is None ✓ Plugin Moods Parameter variations TOML config files Text editing ✗ Custom Generators New musical patterns Rust source code Programming (future) This post covers Plugin Moods—the middle tier. You can preset combinations of tempo, key, and intensity, but you’re still using the built-in generators’ musical logic. Want a “smooth-jazz” preset (slower, mellower)? Plugin mood. Want bebop or Latin jazz with different chord progressions? That requires a custom generator. Custom generators (writing new Rust code) will be covered in a future post when the plugin editor ships. The Plugin Architecture Custom moods live in ~/.midi-cli-rs/moods/ as TOML files. Each file is a “mood pack” that can define multiple moods. The CLI discovers them automatically. Here’s how it works: ~/.midi-cli-rs/ └── moods/ ├── electronic.toml # Your synthwave, techno, etc. ├── cinematic.toml # Epic, tension, wonder └── seasonal.toml # Holiday themes Creating a Mood Pack A plug-in mood pack has two parts: pack metadata and mood definitions. [pack] name = "electronic" version = "1.0.0" author = "Your Name" description = "Electronic music styles" [[moods]] name = "synthwave" base_mood = "upbeat" default_tempo = 118 default_key = "Am" default_intensity = 65 description = "80s synthwave vibes" tags = ["electronic", "retro"] [[moods]] name = "chillout" base_mood = "ambient" default_tempo = 85 default_key = "Em" default_intensity = 40 description = "Relaxed electronic" Each mood delegates to a built-in generator (base_mood) but overrides specific parameters. You get the musical logic of the built-in mood with your customizations applied. Available Base Moods Your custom moods can extend any of the nine built-in generators: Base Mood Character suspense Tense, building eerie Dark, unsettling upbeat Energetic, positive calm Peaceful, slow ambient Atmospheric, textural jazz Swing, improvisation chiptune 8-bit, retro gaming orchestral Classical instruments show Broadway, theatrical Configuration Options Each mood definition supports these overrides: Field Description Example name CLI name (required) "synthwave" base_mood Built-in to extend (required) "upbeat" default_tempo BPM 118 default_key Musical key "Am", "C", "Eb" default_intensity 0-100 energy level 65 description Human-readable description "80s vibes" tags Discovery tags ["electronic", "retro"] How Seeds Create Variation Seeds aren’t random—they’re deterministic variation selectors. The same mood + same seed always produces identical output. But different seeds create observable musical differences across multiple dimensions: Parameter Variation Range Tempo ±15% from base Layer inclusion Which instruments appear Melodic contour 16 different phrase shapes Note density 0.6x to 1.4x Rest probability 0% to 35% silence Phrase length 3-8 notes Velocity -15 to +15 offset The system uses hash-based mixing with unique salts for each parameter. This means adjacent seeds (42 vs 43) produce completely different outputs—no gradual transitions between seeds. When you combine plugin moods with seed variation, you get a matrix: your custom tempo/key/intensity settings applied across different seed-driven variations of the underlying generator’s patterns. Using Custom Moods Once your TOML file is in place, the mood appears automatically: # List all moods (built-in + plugins) midi-cli-rs moods # Generate with your custom mood midi-cli-rs preset -m synthwave -d 5 -s 42 -o output.wav The seed system still works—same mood + same seed = identical output. Example: Electronic Pack Here’s a complete pack with four electronic moods: [pack] name = "electronic" version = "1.0.0" description = "Electronic music styles" [[moods]] name = "synthwave" base_mood = "upbeat" default_tempo = 118 default_key = "Am" default_intensity = 65 [[moods]] name = "chillout" base_mood = "ambient" default_tempo = 85 default_key = "Em" default_intensity = 40 [[moods]] name = "techno" base_mood = "upbeat" default_tempo = 130 default_key = "Dm" default_intensity = 85 [[moods]] name = "8bit" base_mood = "chiptune" default_tempo = 140 default_key = "C" default_intensity = 70 Drop this in ~/.midi-cli-rs/moods/electronic.toml and you have four new moods. What’s Next This plugin system handles mood variations—different tempos, keys, and intensities applied to existing generators. A future update will add a plugin editor for creating entirely new musical patterns without writing Rust. For now, the delegation model covers most use cases: want faster jazz? Darker ambient? Major-key suspense? Create a TOML file and you’re done. The Personal Software Pattern This follows the Personal Software philosophy: start with something that works, then extend it as needs emerge. The plugin system wasn’t in the original design. It grew from actual use—wanting more moods without recompiling. Good personal software leaves room to grow. *Part 3 of the Personal Software series. View all parts ← Previous: midi-cli-rs* Personal software that evolves with your needs.]]></summary></entry><entry><title type="html">Five ML Concepts - #20</title><link href="https://software-wrighter-lab.github.io/2026/02/23/five-ml-concepts-20/" rel="alternate" type="text/html" title="Five ML Concepts - #20" /><published>2026-02-23T00:00:00-08:00</published><updated>2026-02-23T00:00:00-08:00</updated><id>https://software-wrighter-lab.github.io/2026/02/23/five-ml-concepts-20</id><content type="html" xml:base="https://software-wrighter-lab.github.io/2026/02/23/five-ml-concepts-20/"><![CDATA[<p><img src="/assets/images/posts/block-twenty.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/OklW3RTV3I4">Five ML Concepts #20</a><br /><a href="https://www.youtube.com/shorts/OklW3RTV3I4"><img src="https://img.youtube.com/vi/OklW3RTV3I4/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>VAEs</strong></td>
        <td><a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a> (Kingma &amp; Welling 2013)</td>
      </tr>
      <tr>
        <td><strong>Uncertainty Estimation</strong></td>
        <td><a href="https://arxiv.org/abs/1703.04977">What Uncertainties Do We Need in Bayesian Deep Learning?</a> (Kendall &amp; Gal 2017)</td>
      </tr>
      <tr>
        <td><strong>Interpretability</strong></td>
        <td><a href="https://arxiv.org/abs/1702.08608">Towards A Rigorous Science of Interpretable Machine Learning</a> (Doshi-Velez &amp; Kim 2017)</td>
      </tr>
      <tr>
        <td><strong>Gradient Noise</strong></td>
        <td><a href="https://arxiv.org/abs/1704.04289">Stochastic Gradient Descent as Approximate Bayesian Inference</a> (Mandt et al. 2017)</td>
      </tr>
      <tr>
        <td><strong>Human-in-the-Loop</strong></td>
        <td><a href="https://www.manning.com/books/human-in-the-loop-machine-learning">Human-in-the-Loop Machine Learning</a> (Monarch 2021)</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-variational-autoencoders-vaes">1. Variational Autoencoders (VAEs)</h3>

<p><strong>VAEs are probabilistic autoencoders that learn a structured latent distribution.</strong> By sampling from that distribution, they can generate new examples similar to the training data.</p>

<p>The key innovation is regularizing the latent space to be smooth and continuous.</p>

<blockquote>
  <p>Like learning not just to summarize books, but to create new ones in a similar style.</p>
</blockquote>

<h3 id="2-uncertainty-estimation">2. Uncertainty Estimation</h3>

<p><strong>Models can estimate how confident they should be in predictions.</strong> Some uncertainty comes from noisy data (aleatoric), and some from limited knowledge (epistemic).</p>

<p>Knowing when a model is uncertain enables safer decision-making.</p>

<blockquote>
  <p>Like a weather forecast giving seventy percent chance of rain instead of a simple yes or no.</p>
</blockquote>

<h3 id="3-why-interpretability-is-hard">3. Why Interpretability Is Hard</h3>

<p><strong>Neural networks represent information across many interacting parameters.</strong> No single component cleanly maps to a human concept.</p>

<p>Distributed representations enable powerful learning but resist simple explanations.</p>

<blockquote>
  <p>Like trying to explain a dream by pointing to individual neurons.</p>
</blockquote>

<h3 id="4-gradient-noise">4. Gradient Noise</h3>

<p><strong>When training with mini-batches, gradients vary from step to step.</strong> A little noise can help exploration, but too much can slow convergence.</p>

<p>Batch size, learning rate, and gradient clipping all influence this noise level.</p>

<blockquote>
  <p>Like getting slightly different directions each time you ask for help.</p>
</blockquote>

<h3 id="5-human-in-the-loop-systems">5. Human-in-the-Loop Systems</h3>

<p><strong>Humans review, supervise, or override model decisions in critical workflows.</strong> This improves safety and accountability in high-stakes applications.</p>

<p>The approach combines model efficiency with human judgment where it matters most.</p>

<blockquote>
  <p>Like a pilot monitoring autopilot and stepping in when necessary.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>VAEs</strong></td>
      <td>Generative models with structured latent spaces</td>
    </tr>
    <tr>
      <td><strong>Uncertainty Estimation</strong></td>
      <td>Know when you don’t know</td>
    </tr>
    <tr>
      <td><strong>Interpretability</strong></td>
      <td>Distributed representations resist explanation</td>
    </tr>
    <tr>
      <td><strong>Gradient Noise</strong></td>
      <td>Mini-batch variation in training</td>
    </tr>
    <tr>
      <td><strong>Human-in-the-Loop</strong></td>
      <td>Human oversight for critical decisions</td>
    </tr>
  </tbody>
</table>

<hr />

<p><em>Part 20 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></em></p>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>]]></content><author><name>Software Wrighter</name></author><category term="llm" /><category term="machine-learning" /><category term="explainers" /><category term="five-ml-concepts" /><category term="vae" /><category term="uncertainty-estimation" /><category term="interpretability" /><category term="gradient-noise" /><category term="human-in-the-loop" /><category term="ml-concepts" /><summary type="html"><![CDATA[5 machine learning concepts. Under 30 seconds each. Resource Link Papers Links in References section Video Five ML Concepts #20 References Concept Reference VAEs Auto-Encoding Variational Bayes (Kingma &amp; Welling 2013) Uncertainty Estimation What Uncertainties Do We Need in Bayesian Deep Learning? (Kendall &amp; Gal 2017) Interpretability Towards A Rigorous Science of Interpretable Machine Learning (Doshi-Velez &amp; Kim 2017) Gradient Noise Stochastic Gradient Descent as Approximate Bayesian Inference (Mandt et al. 2017) Human-in-the-Loop Human-in-the-Loop Machine Learning (Monarch 2021) Today’s Five 1. Variational Autoencoders (VAEs) VAEs are probabilistic autoencoders that learn a structured latent distribution. By sampling from that distribution, they can generate new examples similar to the training data. The key innovation is regularizing the latent space to be smooth and continuous. Like learning not just to summarize books, but to create new ones in a similar style. 2. Uncertainty Estimation Models can estimate how confident they should be in predictions. Some uncertainty comes from noisy data (aleatoric), and some from limited knowledge (epistemic). Knowing when a model is uncertain enables safer decision-making. Like a weather forecast giving seventy percent chance of rain instead of a simple yes or no. 3. Why Interpretability Is Hard Neural networks represent information across many interacting parameters. No single component cleanly maps to a human concept. Distributed representations enable powerful learning but resist simple explanations. Like trying to explain a dream by pointing to individual neurons. 4. Gradient Noise When training with mini-batches, gradients vary from step to step. A little noise can help exploration, but too much can slow convergence. Batch size, learning rate, and gradient clipping all influence this noise level. Like getting slightly different directions each time you ask for help. 5. Human-in-the-Loop Systems Humans review, supervise, or override model decisions in critical workflows. This improves safety and accountability in high-stakes applications. The approach combines model efficiency with human judgment where it matters most. Like a pilot monitoring autopilot and stepping in when necessary. Quick Reference Concept One-liner VAEs Generative models with structured latent spaces Uncertainty Estimation Know when you don’t know Interpretability Distributed representations resist explanation Gradient Noise Mini-batch variation in training Human-in-the-Loop Human oversight for critical decisions Part 20 of the Five ML Concepts series. View all parts Short, accurate ML explainers. Follow for more.]]></summary></entry><entry><title type="html">In-Context Learning Revisited: From Mystery to Engineering</title><link href="https://software-wrighter-lab.github.io/2026/02/22/icl-revisited-from-mystery-to-engineering/" rel="alternate" type="text/html" title="In-Context Learning Revisited: From Mystery to Engineering" /><published>2026-02-22T00:00:00-08:00</published><updated>2026-02-22T00:00:00-08:00</updated><id>https://software-wrighter-lab.github.io/2026/02/22/icl-revisited-from-mystery-to-engineering</id><content type="html" xml:base="https://software-wrighter-lab.github.io/2026/02/22/icl-revisited-from-mystery-to-engineering/"><![CDATA[<p><img src="/assets/images/posts/block-framework.png" class="post-marker" alt="" /></p>

<p>It was 2020 when GPT-3 shocked everyone. It could learn from examples in the query—without updating its weights. We called it In-Context Learning. But was it magic, or was it doing something deeper?</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/zWKmRxChRlA">ICL Revisited</a><br /><a href="https://www.youtube.com/shorts/zWKmRxChRlA"><img src="https://img.youtube.com/vi/zWKmRxChRlA/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>Papers</strong></td>
        <td><a href="#references">4 References</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="phase-1-the-empirical-discovery-2020">Phase 1: The Empirical Discovery (2020)</h2>

<p>The GPT-3 paper showed that large models could perform few-shot learning. Give them examples, and they generalize. No gradient updates. No retraining. Just forward passes.</p>

<p>The surprising part was that <strong>scaling alone</strong> seemed to unlock it.</p>

<h3 id="paper-language-models-are-few-shot-learners">Paper: Language Models are Few-Shot Learners</h3>

<p><strong>ELI5:</strong> Show a big language model a few examples of a task in your prompt, and it figures out how to do the task—without any retraining. Nobody told it to do this. It just emerged when models got big enough.</p>

<p><strong>Main idea:</strong> Scale unlocks emergent capabilities. ICL was discovered, not designed.</p>

<h2 id="phase-2-mechanistic-explanations-2022">Phase 2: Mechanistic Explanations (2022)</h2>

<p>By 2022, researchers began probing the internal mechanisms. Several papers proposed that transformers implement <strong>implicit meta-learning</strong>. The model appears to learn during inference by performing gradient-descent-like operations internally.</p>

<h3 id="paper-what-explains-in-context-learning-in-transformers">Paper: What Explains In-Context Learning in Transformers?</h3>

<p><strong>ELI5:</strong> When you give a transformer examples, its attention layers do something that looks like fitting a simple linear model to those examples—on the fly, during the forward pass. It’s not memorizing; it’s computing a mini-solution.</p>

<p><strong>Main idea:</strong> ICL works because attention can simulate linear regression internally.</p>

<h3 id="paper-transformers-learn-in-context-by-gradient-descent">Paper: Transformers Learn In-Context by Gradient Descent</h3>

<p><strong>ELI5:</strong> The transformer’s forward pass is secretly doing something similar to training. The attention mechanism acts like one step of gradient descent over the examples you provided. Learning happens inside inference.</p>

<p><strong>Main idea:</strong> ICL is implicit gradient descent—learning hidden inside prediction.</p>

<h2 id="phase-3-engineering-the-effect">Phase 3: Engineering the Effect</h2>

<p>Once researchers understood that ordering and structure affect ICL, prompt design became less of an art and more of an optimization problem. The quality and arrangement of demonstrations directly shape performance.</p>

<p>ICL became tunable. Researchers could now deliberately improve it rather than just observe it.</p>

<h2 id="phase-4-interactive-icl-2026">Phase 4: Interactive ICL (2026)</h2>

<p>Recent work pushes this further. Models are trained to predict natural language critiques and feedback. If a model can predict what a teacher would say, it can internalize that signal. External correction becomes an internal capability.</p>

<h3 id="paper-improving-interactive-in-context-learning-from-natural-language-feedback">Paper: Improving Interactive In-Context Learning from Natural Language Feedback</h3>

<p><strong>ELI5:</strong> Train a model to guess what feedback a human would give. Now the model has internalized the “teacher” and can improve itself without needing the actual teacher present. Self-correction without weight updates.</p>

<p><strong>Main idea:</strong> Models can learn to learn from feedback, making ICL interactive and self-improving.</p>

<h2 id="beyond-language">Beyond Language</h2>

<p>Newer work applies ICL to neuroscience discovery, showing that the mechanism is not limited to text tasks. It becomes a flexible reasoning substrate across domains. That’s when you know a concept has matured.</p>

<h2 id="the-arc">The Arc</h2>

<table>
  <thead>
    <tr>
      <th>Phase</th>
      <th>Era</th>
      <th>Key Insight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Discovery</strong></td>
      <td>2020</td>
      <td>Emerges from scale</td>
    </tr>
    <tr>
      <td><strong>Explanation</strong></td>
      <td>2022</td>
      <td>Implicit gradient descent</td>
    </tr>
    <tr>
      <td><strong>Engineering</strong></td>
      <td>2023-24</td>
      <td>Prompt design as optimization</td>
    </tr>
    <tr>
      <td><strong>Self-improvement</strong></td>
      <td>2026</td>
      <td>Learning from feedback</td>
    </tr>
  </tbody>
</table>

<h2 id="the-deeper-insight">The Deeper Insight</h2>

<p>In-Context Learning started as an emergent surprise. Now it’s becoming an engineered learning substrate inside transformers.</p>

<p>It was not magic. It was <strong>meta-learning hiding in plain sight</strong>.</p>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Paper</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Language Models are Few-Shot Learners (GPT-3)</td>
        <td><a href="https://arxiv.org/abs/2005.14165">arXiv:2005.14165</a></td>
      </tr>
      <tr>
        <td>What Explains In-Context Learning in Transformers?</td>
        <td><a href="https://arxiv.org/abs/2202.12837">arXiv:2202.12837</a></td>
      </tr>
      <tr>
        <td>Transformers Learn In-Context by Gradient Descent</td>
        <td><a href="https://arxiv.org/abs/2212.07677">arXiv:2212.07677</a></td>
      </tr>
      <tr>
        <td>Improving Interactive ICL from Natural Language Feedback</td>
        <td><a href="https://arxiv.org/abs/2602.16066">arXiv:2602.16066</a></td>
      </tr>
    </tbody>
  </table>

</div>

<hr />

<p><em>ICL started as “whoa, it works.” Now we understand “why it works.” Next: engineering it deliberately.</em></p>]]></content><author><name>Software Wrighter</name></author><category term="machine-learning" /><category term="llm" /><category term="research" /><category term="in-context-learning" /><category term="icl" /><category term="transformers" /><category term="meta-learning" /><category term="gpt" /><category term="few-shot-learning" /><summary type="html"><![CDATA[It was 2020 when GPT-3 shocked everyone. It could learn from examples in the query—without updating its weights. We called it In-Context Learning. But was it magic, or was it doing something deeper? Resource Link Video ICL Revisited Papers 4 References Phase 1: The Empirical Discovery (2020) The GPT-3 paper showed that large models could perform few-shot learning. Give them examples, and they generalize. No gradient updates. No retraining. Just forward passes. The surprising part was that scaling alone seemed to unlock it. Paper: Language Models are Few-Shot Learners ELI5: Show a big language model a few examples of a task in your prompt, and it figures out how to do the task—without any retraining. Nobody told it to do this. It just emerged when models got big enough. Main idea: Scale unlocks emergent capabilities. ICL was discovered, not designed. Phase 2: Mechanistic Explanations (2022) By 2022, researchers began probing the internal mechanisms. Several papers proposed that transformers implement implicit meta-learning. The model appears to learn during inference by performing gradient-descent-like operations internally. Paper: What Explains In-Context Learning in Transformers? ELI5: When you give a transformer examples, its attention layers do something that looks like fitting a simple linear model to those examples—on the fly, during the forward pass. It’s not memorizing; it’s computing a mini-solution. Main idea: ICL works because attention can simulate linear regression internally. Paper: Transformers Learn In-Context by Gradient Descent ELI5: The transformer’s forward pass is secretly doing something similar to training. The attention mechanism acts like one step of gradient descent over the examples you provided. Learning happens inside inference. Main idea: ICL is implicit gradient descent—learning hidden inside prediction. Phase 3: Engineering the Effect Once researchers understood that ordering and structure affect ICL, prompt design became less of an art and more of an optimization problem. The quality and arrangement of demonstrations directly shape performance. ICL became tunable. Researchers could now deliberately improve it rather than just observe it. Phase 4: Interactive ICL (2026) Recent work pushes this further. Models are trained to predict natural language critiques and feedback. If a model can predict what a teacher would say, it can internalize that signal. External correction becomes an internal capability. Paper: Improving Interactive In-Context Learning from Natural Language Feedback ELI5: Train a model to guess what feedback a human would give. Now the model has internalized the “teacher” and can improve itself without needing the actual teacher present. Self-correction without weight updates. Main idea: Models can learn to learn from feedback, making ICL interactive and self-improving. Beyond Language Newer work applies ICL to neuroscience discovery, showing that the mechanism is not limited to text tasks. It becomes a flexible reasoning substrate across domains. That’s when you know a concept has matured. The Arc Phase Era Key Insight Discovery 2020 Emerges from scale Explanation 2022 Implicit gradient descent Engineering 2023-24 Prompt design as optimization Self-improvement 2026 Learning from feedback The Deeper Insight In-Context Learning started as an emergent surprise. Now it’s becoming an engineered learning substrate inside transformers. It was not magic. It was meta-learning hiding in plain sight. References Paper Link Language Models are Few-Shot Learners (GPT-3) arXiv:2005.14165 What Explains In-Context Learning in Transformers? arXiv:2202.12837 Transformers Learn In-Context by Gradient Descent arXiv:2212.07677 Improving Interactive ICL from Natural Language Feedback arXiv:2602.16066 ICL started as “whoa, it works.” Now we understand “why it works.” Next: engineering it deliberately.]]></summary></entry><entry><title type="html">Five ML Concepts - #19</title><link href="https://software-wrighter-lab.github.io/2026/02/22/five-ml-concepts-19/" rel="alternate" type="text/html" title="Five ML Concepts - #19" /><published>2026-02-22T00:00:00-08:00</published><updated>2026-02-22T00:00:00-08:00</updated><id>https://software-wrighter-lab.github.io/2026/02/22/five-ml-concepts-19</id><content type="html" xml:base="https://software-wrighter-lab.github.io/2026/02/22/five-ml-concepts-19/"><![CDATA[<p><img src="/assets/images/posts/block-nineteen.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/ppoONTOdqJQ">Five ML Concepts #19</a><br /><a href="https://www.youtube.com/shorts/ppoONTOdqJQ"><img src="https://img.youtube.com/vi/ppoONTOdqJQ/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Autoencoders</strong></td>
        <td><a href="https://www.science.org/doi/10.1126/science.1127647">Reducing the Dimensionality of Data with Neural Networks</a> (Hinton &amp; Salakhutdinov 2006)</td>
      </tr>
      <tr>
        <td><strong>Correlation vs Causation</strong></td>
        <td><a href="https://www.cambridge.org/core/books/causality/B0046844FAE10CBF274D4ACBDAEB5F5B">Causality</a> (Pearl 2009)</td>
      </tr>
      <tr>
        <td><strong>Curriculum Learning</strong></td>
        <td><a href="https://dl.acm.org/doi/10.1145/1553374.1553380">Curriculum Learning</a> (Bengio et al. 2009)</td>
      </tr>
      <tr>
        <td><strong>Failure Analysis</strong></td>
        <td><a href="https://www.oreilly.com/library/view/practical-machine-learning/9781098102357/">Practical Machine Learning for Computer Vision</a> (Lakshmanan et al. 2021)</td>
      </tr>
      <tr>
        <td><strong>Covariate Shift</strong></td>
        <td><a href="https://mitpress.mit.edu/9780262170055/">Dataset Shift in Machine Learning</a> (Quinonero-Candela et al. 2009)</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-autoencoders">1. Autoencoders</h3>

<p><strong>Autoencoders are neural networks trained to compress inputs into a smaller representation and reconstruct them.</strong> The bottleneck forces the model to capture essential structure.</p>

<p>This learned compression is useful for dimensionality reduction, denoising, and feature learning.</p>

<blockquote>
  <p>Like summarizing a book into key points and then rebuilding the story from that summary.</p>
</blockquote>

<h3 id="2-correlation-vs-causation">2. Correlation vs Causation</h3>

<p><strong>Two variables can move together without one causing the other.</strong> Models typically learn correlations present in data, not true cause-and-effect relationships.</p>

<p>This matters because interventions based on correlation alone may not produce intended effects.</p>

<blockquote>
  <p>Like noticing umbrella sales rise with rain—umbrellas don’t cause rain.</p>
</blockquote>

<h3 id="3-curriculum-learning">3. Curriculum Learning</h3>

<p><strong>Training starts with easier examples and gradually introduces harder ones.</strong> This can improve stability and learning speed in some settings.</p>

<p>The approach mirrors how humans learn complex subjects incrementally.</p>

<blockquote>
  <p>Like teaching math by starting with addition before moving to calculus.</p>
</blockquote>

<h3 id="4-failure-analysis">4. Failure Analysis</h3>

<p><strong>Failure analysis groups model errors into categories to understand where performance breaks down.</strong> This helps target improvements instead of guessing.</p>

<p>Systematic error analysis often reveals actionable patterns invisible in aggregate metrics.</p>

<blockquote>
  <p>Like a teacher reviewing which types of questions students miss most often.</p>
</blockquote>

<h3 id="5-covariate-shift">5. Covariate Shift</h3>

<p><strong>Covariate shift occurs when the input distribution changes between training and deployment, while the task itself remains the same.</strong> The model may underperform because it sees unfamiliar inputs.</p>

<p>Monitoring input distributions helps detect this shift early.</p>

<blockquote>
  <p>Like training a driver in sunny weather and testing them in snow.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Autoencoders</strong></td>
      <td>Compress and reconstruct to learn structure</td>
    </tr>
    <tr>
      <td><strong>Correlation vs Causation</strong></td>
      <td>Co-occurrence isn’t cause</td>
    </tr>
    <tr>
      <td><strong>Curriculum Learning</strong></td>
      <td>Start easy, progress to hard</td>
    </tr>
    <tr>
      <td><strong>Failure Analysis</strong></td>
      <td>Categorize errors to guide fixes</td>
    </tr>
    <tr>
      <td><strong>Covariate Shift</strong></td>
      <td>New inputs, same task</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 19 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/23/five-ml-concepts-20/">Next: #20 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>]]></content><author><name>Software Wrighter</name></author><category term="llm" /><category term="machine-learning" /><category term="explainers" /><category term="five-ml-concepts" /><category term="autoencoders" /><category term="correlation-causation" /><category term="curriculum-learning" /><category term="failure-analysis" /><category term="covariate-shift" /><category term="ml-concepts" /><summary type="html"><![CDATA[5 machine learning concepts. Under 30 seconds each. Resource Link Papers Links in References section Video Five ML Concepts #19 References Concept Reference Autoencoders Reducing the Dimensionality of Data with Neural Networks (Hinton &amp; Salakhutdinov 2006) Correlation vs Causation Causality (Pearl 2009) Curriculum Learning Curriculum Learning (Bengio et al. 2009) Failure Analysis Practical Machine Learning for Computer Vision (Lakshmanan et al. 2021) Covariate Shift Dataset Shift in Machine Learning (Quinonero-Candela et al. 2009) Today’s Five 1. Autoencoders Autoencoders are neural networks trained to compress inputs into a smaller representation and reconstruct them. The bottleneck forces the model to capture essential structure. This learned compression is useful for dimensionality reduction, denoising, and feature learning. Like summarizing a book into key points and then rebuilding the story from that summary. 2. Correlation vs Causation Two variables can move together without one causing the other. Models typically learn correlations present in data, not true cause-and-effect relationships. This matters because interventions based on correlation alone may not produce intended effects. Like noticing umbrella sales rise with rain—umbrellas don’t cause rain. 3. Curriculum Learning Training starts with easier examples and gradually introduces harder ones. This can improve stability and learning speed in some settings. The approach mirrors how humans learn complex subjects incrementally. Like teaching math by starting with addition before moving to calculus. 4. Failure Analysis Failure analysis groups model errors into categories to understand where performance breaks down. This helps target improvements instead of guessing. Systematic error analysis often reveals actionable patterns invisible in aggregate metrics. Like a teacher reviewing which types of questions students miss most often. 5. Covariate Shift Covariate shift occurs when the input distribution changes between training and deployment, while the task itself remains the same. The model may underperform because it sees unfamiliar inputs. Monitoring input distributions helps detect this shift early. Like training a driver in sunny weather and testing them in snow. Quick Reference Concept One-liner Autoencoders Compress and reconstruct to learn structure Correlation vs Causation Co-occurrence isn’t cause Curriculum Learning Start easy, progress to hard Failure Analysis Categorize errors to guide fixes Covariate Shift New inputs, same task *Part 19 of the Five ML Concepts series. View all parts Next: #20 →* Short, accurate ML explainers. Follow for more.]]></summary></entry><entry><title type="html">JSON et al: A Deep Dive into Data Serialization Formats</title><link href="https://software-wrighter-lab.github.io/2026/02/21/json-et-al-data-serialization-formats/" rel="alternate" type="text/html" title="JSON et al: A Deep Dive into Data Serialization Formats" /><published>2026-02-21T14:00:00-08:00</published><updated>2026-02-21T14:00:00-08:00</updated><id>https://software-wrighter-lab.github.io/2026/02/21/json-et-al-data-serialization-formats</id><content type="html" xml:base="https://software-wrighter-lab.github.io/2026/02/21/json-et-al-data-serialization-formats/"><![CDATA[<p><img src="/assets/images/posts/block-brackets.png" class="post-marker no-invert" alt="" /></p>

<p>JSON is everywhere. APIs. Logs. Databases. Configuration files. But it’s not alone. A whole ecosystem of formats exists—each optimizing for different tradeoffs.</p>

<p>This post expands on the <a href="https://www.youtube.com/shorts/3ezjk1CnZEU">JSON et al</a> short, providing technical depth on each format: when it was created, where it’s specified, and what problems it solves.</p>

<hr />

<h2 id="the-tradeoff-triangle">The Tradeoff Triangle</h2>

<p>Before diving in, understand the fundamental constraint. Data formats balance three competing goals:</p>

<table>
  <thead>
    <tr>
      <th>Goal</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Human Readability</strong></td>
      <td>Can a developer read and edit it directly?</td>
    </tr>
    <tr>
      <td><strong>Compactness</strong></td>
      <td>How many bytes does it take to represent data?</td>
    </tr>
    <tr>
      <td><strong>Query Performance</strong></td>
      <td>How fast can you access specific fields?</td>
    </tr>
  </tbody>
</table>

<p>You usually only get two. JSON optimizes readability. Protobuf optimizes compactness. JSONB optimizes query performance. No format wins everywhere.</p>

<hr />

<h2 id="json-the-ubiquitous-baseline">JSON: The Ubiquitous Baseline</h2>

<p><strong>Created:</strong> 2001 (discovered/formalized by Douglas Crockford)
<strong>Specification:</strong> <a href="https://www.ecma-international.org/publications-and-standards/standards/ecma-404/">ECMA-404</a> (2013), <a href="https://datatracker.ietf.org/doc/html/rfc8259">RFC 8259</a> (2017)
<strong>File Extension:</strong> <code class="language-plaintext highlighter-rouge">.json</code></p>

<p>JSON (JavaScript Object Notation) emerged from JavaScript’s object literal syntax but became language-agnostic. Crockford didn’t invent it—he “discovered” it already existing in JavaScript and formalized the specification.</p>

<h3 id="technical-details">Technical Details</h3>

<ul>
  <li><strong>Encoding:</strong> UTF-8 text (UTF-16/32 allowed but rare)</li>
  <li><strong>Data Types:</strong> Objects <code class="language-plaintext highlighter-rouge">{}</code>, arrays <code class="language-plaintext highlighter-rouge">[]</code>, strings, numbers, booleans, <code class="language-plaintext highlighter-rouge">null</code></li>
  <li><strong>Schema:</strong> None required</li>
  <li><strong>Comments:</strong> Not allowed in strict JSON</li>
</ul>

<h3 id="strengths">Strengths</h3>

<ul>
  <li>Universal parser support (every language has one)</li>
  <li>Human readable without tools</li>
  <li>Web-native (JavaScript parses it natively)</li>
  <li>Simple specification (fits on a business card)</li>
</ul>

<h3 id="weaknesses">Weaknesses</h3>

<ul>
  <li>Verbose (field names repeated for every object)</li>
  <li>No binary data type (must base64-encode)</li>
  <li>No comments (frustrating for config files)</li>
  <li>Parsing overhead (tokenization + string decoding every time)</li>
</ul>

<h3 id="eli5">ELI5</h3>

<p>Like typing a long email instead of sending a terse text. Every message spells everything out—clear, but verbose.</p>

<h3 id="when-to-use">When to Use</h3>

<p>REST APIs, configuration (when comments aren’t needed), data interchange between systems, anywhere human readability matters more than efficiency.</p>

<hr />

<h2 id="jsonl--ndjson-streaming-json">JSONL / NDJSON: Streaming JSON</h2>

<p><strong>Created:</strong> ~2013 (formalized)
<strong>Specification:</strong> <a href="https://jsonlines.org/">JSON Lines</a>, <a href="http://ndjsonspec.org/">NDJSON</a>
<strong>File Extension:</strong> <code class="language-plaintext highlighter-rouge">.jsonl</code>, <code class="language-plaintext highlighter-rouge">.ndjson</code></p>

<p>JSONL (JSON Lines) and NDJSON (Newline-Delimited JSON) are the same concept: one valid JSON object per line, separated by newlines.</p>

<h3 id="technical-details-1">Technical Details</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{"name": "Alice", "score": 95}
{"name": "Bob", "score": 87}
{"name": "Carol", "score": 92}
</code></pre></div></div>

<p>No wrapping array. Each line is independently parseable.</p>

<h3 id="strengths-1">Strengths</h3>

<ul>
  <li><strong>Streaming:</strong> Process line-by-line without loading entire file</li>
  <li><strong>Append-only:</strong> Add records without rewriting the file</li>
  <li><strong>Parallel processing:</strong> Split by line, distribute to workers</li>
  <li><strong>Fault-tolerant:</strong> One corrupt line doesn’t invalidate the file</li>
</ul>

<h3 id="weaknesses-1">Weaknesses</h3>

<ul>
  <li>Not valid JSON (can’t parse with standard JSON parser)</li>
  <li>Still text-based (same verbosity as JSON)</li>
  <li>No random access by index</li>
</ul>

<h3 id="eli5-1">ELI5</h3>

<p>Like removing one comma per line to save some typing. Each line is self-contained, so you can grab and process them one at a time.</p>

<h3 id="when-to-use-1">When to Use</h3>

<p>Log files, big data pipelines (Spark, Pandas), ML datasets, event streams, anywhere you need to process records incrementally.</p>

<hr />

<h2 id="jsonb-binary-json-for-databases">JSONB: Binary JSON for Databases</h2>

<p><strong>Created:</strong> 2014 (PostgreSQL 9.4)
<strong>Specification:</strong> Implementation-specific (no universal standard)
<strong>Storage:</strong> Database column type</p>

<p>JSONB isn’t a file format—it’s a database storage optimization. PostgreSQL’s JSONB differs from MongoDB’s BSON, which differs from other implementations.</p>

<h3 id="postgresql-jsonb-details">PostgreSQL JSONB Details</h3>

<ul>
  <li><strong>Parsed once:</strong> Text converted to binary on INSERT</li>
  <li><strong>Keys sorted:</strong> Deterministic ordering for indexing</li>
  <li><strong>Duplicates removed:</strong> Last value wins</li>
  <li><strong>Offset table:</strong> O(log n) field lookup instead of O(n) text scanning</li>
</ul>

<h3 id="mongodb-bson">MongoDB BSON</h3>

<p><strong>Specification:</strong> <a href="https://bsonspec.org/">bsonspec.org</a></p>

<p>BSON (Binary JSON) is MongoDB’s serialization format. Unlike PostgreSQL’s JSONB, BSON is a standalone binary format:</p>

<ul>
  <li>Type-prefixed values</li>
  <li>Supports additional types (Date, Binary, ObjectId)</li>
  <li>Length-prefixed for fast skipping</li>
  <li>~10-15% smaller than JSON typically</li>
</ul>

<h3 id="strengths-2">Strengths</h3>

<ul>
  <li>Fast queries without re-parsing</li>
  <li>Indexable (GIN indexes on JSONB in PostgreSQL)</li>
  <li>Type coercion at storage time</li>
</ul>

<h3 id="weaknesses-2">Weaknesses</h3>

<ul>
  <li>Not portable (implementation-specific)</li>
  <li>Not human-readable</li>
  <li>INSERT overhead (parsing cost upfront)</li>
</ul>

<h3 id="eli5-2">ELI5</h3>

<p>Instead of cooking from scratch every time, you heat a pre-made meal. The prep work happens once (on INSERT), so serving (queries) is fast.</p>

<h3 id="when-to-use-2">When to Use</h3>

<p>Database storage where you query into JSON structures. PostgreSQL JSONB + GIN indexes enable fast <code class="language-plaintext highlighter-rouge">@&gt;</code> containment queries.</p>

<hr />

<h2 id="protocol-buffers-googles-schema-first-format">Protocol Buffers: Google’s Schema-First Format</h2>

<p><strong>Created:</strong> 2001 (internal Google), 2008 (open-sourced)
<strong>Specification:</strong> <a href="https://developers.google.com/protocol-buffers/docs/proto3">developers.google.com/protocol-buffers</a>
<strong>File Extension:</strong> <code class="language-plaintext highlighter-rouge">.proto</code> (schema), binary wire format</p>

<p>Protocol Buffers (Protobuf) is Google’s language-neutral, schema-required serialization format. It powers gRPC.</p>

<h3 id="technical-details-2">Technical Details</h3>

<p>Schema definition:</p>
<div class="language-protobuf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">message</span> <span class="nc">Sensor</span> <span class="p">{</span>
  <span class="kt">int32</span> <span class="na">temperature</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="kt">int32</span> <span class="na">humidity</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Wire format uses field numbers, not names:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Field 1: 72
Field 2: 40
</code></pre></div></div>

<h3 id="key-features">Key Features</h3>

<ul>
  <li><strong>Varint encoding:</strong> Small integers use fewer bytes</li>
  <li><strong>Field numbers:</strong> Enable backward compatibility</li>
  <li><strong>Code generation:</strong> <code class="language-plaintext highlighter-rouge">.proto</code> → language-specific classes</li>
  <li><strong>No self-description:</strong> Receiver must know schema</li>
</ul>

<h3 id="strengths-3">Strengths</h3>

<ul>
  <li>Extremely compact (3-10x smaller than JSON typically)</li>
  <li>Fast serialization/deserialization</li>
  <li>Strong versioning semantics</li>
  <li>gRPC integration</li>
</ul>

<h3 id="weaknesses-3">Weaknesses</h3>

<ul>
  <li>Requires schema agreement</li>
  <li>Not human-readable</li>
  <li>Tooling required for debugging</li>
  <li>Schema evolution has rules</li>
</ul>

<h3 id="eli5-3">ELI5</h3>

<p>Everyone agrees upfront what “field 1” means. You don’t waste space spelling out “temperature”—you just send the number 1 and the value. Both sides know the code.</p>

<h3 id="when-to-use-3">When to Use</h3>

<p>Microservices (gRPC), internal APIs, anywhere bandwidth and latency matter more than debuggability.</p>

<hr />

<h2 id="asn1-the-telecom-veteran">ASN.1: The Telecom Veteran</h2>

<p><strong>Created:</strong> 1984 (ITU-T X.208)
<strong>Specification:</strong> <a href="https://www.itu.int/rec/T-REC-X.680-X.683">ITU-T X.680-X.683</a>
<strong>Encoding Rules:</strong> BER, DER, PER, XER, and more</p>

<p>ASN.1 (Abstract Syntax Notation One) predates all modern formats. It defines both schema and encoding, with multiple encoding rules for different use cases.</p>

<h3 id="encoding-rules-comparison">Encoding Rules Comparison</h3>

<table>
  <thead>
    <tr>
      <th>Rule</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>BER</strong> (Basic Encoding Rules)</td>
      <td>Flexible, general purpose</td>
    </tr>
    <tr>
      <td><strong>DER</strong> (Distinguished Encoding Rules)</td>
      <td>Deterministic, for cryptography</td>
    </tr>
    <tr>
      <td><strong>PER</strong> (Packed Encoding Rules)</td>
      <td>Most compact, for bandwidth-constrained</td>
    </tr>
    <tr>
      <td><strong>XER</strong> (XML Encoding Rules)</td>
      <td>XML-based, for interop</td>
    </tr>
  </tbody>
</table>

<h3 id="where-you-see-asn1">Where You See ASN.1</h3>

<ul>
  <li><strong>X.509 certificates</strong> (SSL/TLS certs are DER-encoded ASN.1)</li>
  <li><strong>LDAP</strong> (directory services)</li>
  <li><strong>SNMP</strong> (network management)</li>
  <li><strong>Telecom protocols</strong> (SS7, GSM, LTE)</li>
</ul>

<h3 id="strengths-4">Strengths</h3>

<ul>
  <li>Bit-level precision</li>
  <li>Proven over 40 years</li>
  <li>Multiple encoding options</li>
  <li>Formal verification possible</li>
</ul>

<h3 id="weaknesses-4">Weaknesses</h3>

<ul>
  <li>Complex specification</li>
  <li>Steep learning curve</li>
  <li>Tooling can be expensive</li>
  <li>Security vulnerabilities in parsers (historically)</li>
</ul>

<h3 id="eli5-4">ELI5</h3>

<p>Same idea as Protobuf—everyone agrees upfront what each field number means. ASN.1 just got there 20 years earlier and handles even more edge cases.</p>

<h3 id="when-to-use-4">When to Use</h3>

<p>You probably won’t choose ASN.1 for new projects. You’ll encounter it in cryptography, certificates, and legacy telecom systems.</p>

<hr />

<h2 id="yaml-human-friendly-configuration">YAML: Human-Friendly Configuration</h2>

<p><strong>Created:</strong> 2001 (Clark Evans, Ingy döt Net, Oren Ben-Kiki)
<strong>Specification:</strong> <a href="https://yaml.org/spec/1.2.2/">yaml.org/spec/1.2.2</a>
<strong>File Extension:</strong> <code class="language-plaintext highlighter-rouge">.yaml</code>, <code class="language-plaintext highlighter-rouge">.yml</code></p>

<p>YAML (YAML Ain’t Markup Language) prioritizes human readability. It’s a superset of JSON—any valid JSON is valid YAML.</p>

<h3 id="technical-details-3">Technical Details</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Comments allowed!</span>
<span class="na">server</span><span class="pi">:</span>
  <span class="na">host</span><span class="pi">:</span> <span class="s">localhost</span>
  <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="na">features</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">auth</span>
    <span class="pi">-</span> <span class="s">logging</span>
</code></pre></div></div>

<h3 id="key-features-1">Key Features</h3>

<ul>
  <li><strong>Indentation-based:</strong> Whitespace matters</li>
  <li><strong>Comments:</strong> <code class="language-plaintext highlighter-rouge">#</code> for single-line</li>
  <li><strong>Anchors/aliases:</strong> <code class="language-plaintext highlighter-rouge">&amp;name</code> and <code class="language-plaintext highlighter-rouge">*name</code> for references</li>
  <li><strong>Multiple documents:</strong> <code class="language-plaintext highlighter-rouge">---</code> separator</li>
</ul>

<h3 id="strengths-5">Strengths</h3>

<ul>
  <li>Highly readable</li>
  <li>Comments supported</li>
  <li>Multi-line strings without escaping</li>
  <li>Complex data structures</li>
</ul>

<h3 id="weaknesses-5">Weaknesses</h3>

<ul>
  <li><strong>“Norway problem”:</strong> <code class="language-plaintext highlighter-rouge">NO</code> parses as boolean <code class="language-plaintext highlighter-rouge">false</code></li>
  <li>Whitespace sensitivity causes errors</li>
  <li>Multiple ways to express same data</li>
  <li>Security concerns (arbitrary code execution in some parsers)</li>
</ul>

<h3 id="eli5-5">ELI5</h3>

<p>Optimized for clarity, not bandwidth. YAML is for humans editing config files—not for machines exchanging data over networks.</p>

<h3 id="when-to-use-5">When to Use</h3>

<p>Configuration files (Kubernetes, Docker Compose, CI/CD), anywhere humans edit data directly and comments help.</p>

<hr />

<h2 id="toml-minimal-configuration">TOML: Minimal Configuration</h2>

<p><strong>Created:</strong> 2013 (Tom Preston-Werner)
<strong>Specification:</strong> <a href="https://toml.io/en/v1.0.0">toml.io</a>
<strong>File Extension:</strong> <code class="language-plaintext highlighter-rouge">.toml</code></p>

<p>TOML (Tom’s Obvious Minimal Language) emerged as a reaction to YAML’s complexity. It’s used by Rust (Cargo.toml), Python (pyproject.toml), and others.</p>

<h3 id="technical-details-4">Technical Details</h3>

<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">[</span><span class="n">server</span><span class="k">]</span>
<span class="n">host</span> <span class="o">=</span><span class="w"> </span><span class="s">"localhost"</span>
<span class="n">port</span> <span class="o">=</span><span class="w"> </span><span class="mi">8080</span>

<span class="k">[</span><span class="n">server</span><span class="k">.</span><span class="n">features</span><span class="k">]</span>
<span class="n">auth</span> <span class="o">=</span><span class="w"> </span><span class="kc">true</span>
<span class="n">logging</span> <span class="o">=</span><span class="w"> </span><span class="kc">true</span>
</code></pre></div></div>

<h3 id="key-features-2">Key Features</h3>

<ul>
  <li><strong>Explicit typing:</strong> Dates, times, arrays have clear syntax</li>
  <li><strong>Sections:</strong> <code class="language-plaintext highlighter-rouge">[section]</code> and <code class="language-plaintext highlighter-rouge">[section.subsection]</code></li>
  <li><strong>No anchors:</strong> Intentionally simpler than YAML</li>
  <li><strong>Deterministic:</strong> Same data = same representation</li>
</ul>

<h3 id="strengths-6">Strengths</h3>

<ul>
  <li>Easy to read and write</li>
  <li>Unambiguous parsing</li>
  <li>Clear error messages</li>
  <li>Growing ecosystem support</li>
</ul>

<h3 id="weaknesses-6">Weaknesses</h3>

<ul>
  <li>Less expressive than YAML</li>
  <li>Nested structures can be verbose</li>
  <li>Smaller ecosystem than JSON/YAML</li>
</ul>

<h3 id="eli5-6">ELI5</h3>

<p>Same goal as YAML—clarity for humans, not bandwidth for machines—but with stricter rules so you make fewer mistakes.</p>

<h3 id="when-to-use-6">When to Use</h3>

<p>Configuration files where YAML’s complexity isn’t needed. Rust projects (mandatory). Python packaging (pyproject.toml).</p>

<hr />

<h2 id="toon-token-optimized-for-llms">TOON: Token-Optimized for LLMs</h2>

<p><strong>Created:</strong> October 2025 (toon-format organization)
<strong>Specification:</strong> <a href="https://github.com/toon-format/toon">github.com/toon-format/toon</a> (v3.0)
<strong>File Extension:</strong> <code class="language-plaintext highlighter-rouge">.toon</code>
<strong>Media Type:</strong> <code class="language-plaintext highlighter-rouge">text/toon</code> (provisional)</p>

<p>TOON (Token Oriented Object Notation) is the newest format in this list, designed specifically for LLM input. It’s a lossless representation of JSON that minimizes tokens.</p>

<h3 id="technical-details-5">Technical Details</h3>

<p>TOON combines YAML-style indentation for nested objects with CSV-like tabular layouts for uniform arrays:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>users[2]{name,age}:
Alice,25
Bob,30
</code></pre></div></div>

<p>Equivalent JSON:</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="nl">"users"</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Alice"</span><span class="p">,</span><span class="w"> </span><span class="nl">"age"</span><span class="p">:</span><span class="w"> </span><span class="mi">25</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Bob"</span><span class="p">,</span><span class="w"> </span><span class="nl">"age"</span><span class="p">:</span><span class="w"> </span><span class="mi">30</span><span class="p">}]}</span><span class="w">
</span></code></pre></div></div>

<h3 id="key-features-3">Key Features</h3>

<ul>
  <li><strong>Header-based:</strong> Field names declared once, values follow</li>
  <li><strong>40% fewer tokens:</strong> Than equivalent JSON typically</li>
  <li><strong>Lossless:</strong> Round-trips to JSON perfectly</li>
  <li><strong>UTF-8 always:</strong> No encoding ambiguity</li>
</ul>

<h3 id="performance">Performance</h3>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>JSON</th>
      <th>TOON</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Accuracy</td>
      <td>69.7%</td>
      <td>73.9%</td>
    </tr>
    <tr>
      <td>Efficiency (acc/1K tokens)</td>
      <td>15.3</td>
      <td>26.9</td>
    </tr>
  </tbody>
</table>

<h3 id="strengths-7">Strengths</h3>

<ul>
  <li>Significant token savings at scale</li>
  <li>Better context window utilization</li>
  <li>Lower API costs for LLM applications</li>
  <li>Human-readable (unlike binary formats)</li>
</ul>

<h3 id="weaknesses-7">Weaknesses</h3>

<ul>
  <li>New format (October 2025)</li>
  <li>Limited tooling compared to JSON</li>
  <li>Requires conversion layer for existing systems</li>
  <li>Not yet widely adopted</li>
</ul>

<h3 id="eli5-7">ELI5</h3>

<p>Like having one header row for each column in a table instead of repeating the column name for every single row. You declare field names once, then just list the values.</p>

<h3 id="when-to-use-7">When to Use</h3>

<p>LLM prompts with structured data, RAG applications, anywhere token efficiency matters. Especially useful for large datasets with uniform object arrays.</p>

<h3 id="implementations">Implementations</h3>

<ul>
  <li><strong>TypeScript:</strong> Reference implementation</li>
  <li><strong>Python:</strong> <a href="https://pypi.org/project/toons/">toons</a> (Rust-based, fast)</li>
  <li><strong>Go, Rust, .NET:</strong> Available via toon-format org</li>
</ul>

<hr />

<h2 id="alternatives-not-in-the-video">Alternatives Not in the Video</h2>

<h3 id="messagepack">MessagePack</h3>

<p><strong>Created:</strong> 2008 (Sadayuki Furuhashi)
<strong>Specification:</strong> <a href="https://msgpack.org/">msgpack.org</a></p>

<p>Binary JSON without schema. Type-prefixed values, efficient numeric encoding.</p>

<p><strong>Use when:</strong> You want JSON semantics but smaller/faster.</p>

<h3 id="cbor">CBOR</h3>

<p><strong>Created:</strong> 2013 (IETF)
<strong>Specification:</strong> <a href="https://datatracker.ietf.org/doc/html/rfc8949">RFC 8949</a></p>

<p>Concise Binary Object Representation. Designed for constrained environments (IoT).</p>

<p><strong>Use when:</strong> Resource-constrained devices, need a standard binary format.</p>

<h3 id="apache-avro">Apache Avro</h3>

<p><strong>Created:</strong> 2009 (Apache, Doug Cutting)
<strong>Specification:</strong> <a href="https://avro.apache.org/docs/current/spec.html">avro.apache.org</a></p>

<p>Schema-based, row-oriented binary format. Schema embedded or stored separately. Strong schema evolution support.</p>

<p><strong>Use when:</strong> Big data pipelines (Hadoop, Kafka), schema evolution is critical.</p>

<h3 id="apache-parquet">Apache Parquet</h3>

<p><strong>Created:</strong> 2013 (Twitter + Cloudera)
<strong>Specification:</strong> <a href="https://parquet.apache.org/docs/file-format/">parquet.apache.org</a></p>

<p>Columnar storage format. Not for serialization—for analytics storage.</p>

<p><strong>Use when:</strong> Large-scale analytics, data warehousing, Spark/Pandas workflows.</p>

<h3 id="capn-proto">Cap’n Proto</h3>

<p><strong>Created:</strong> 2013 (Kenton Varda, ex-Protobuf author)
<strong>Specification:</strong> <a href="https://capnproto.org/">capnproto.org</a></p>

<p>Zero-copy serialization. The serialized form <em>is</em> the in-memory form.</p>

<p><strong>Use when:</strong> Extreme performance requirements, inter-process communication.</p>

<h3 id="flatbuffers">FlatBuffers</h3>

<p><strong>Created:</strong> 2014 (Google)
<strong>Specification:</strong> <a href="https://google.github.io/flatbuffers/">google.github.io/flatbuffers</a></p>

<p>Zero-copy like Cap’n Proto but with better tooling. Used in games, mobile.</p>

<p><strong>Use when:</strong> Games, mobile apps, anywhere memory allocation matters.</p>

<hr />

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Format</th>
      <th>Year</th>
      <th>Schema</th>
      <th>Binary</th>
      <th>Human-Readable</th>
      <th>Best For</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>JSON</td>
      <td>2001</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>APIs, interchange</td>
    </tr>
    <tr>
      <td>JSONL</td>
      <td>2013</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>Logs, streaming</td>
    </tr>
    <tr>
      <td>JSONB</td>
      <td>2014</td>
      <td>No</td>
      <td>Yes</td>
      <td>No</td>
      <td>Database queries</td>
    </tr>
    <tr>
      <td>Protobuf</td>
      <td>2008</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>No</td>
      <td>Microservices</td>
    </tr>
    <tr>
      <td>ASN.1</td>
      <td>1984</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>No</td>
      <td>Crypto, telecom</td>
    </tr>
    <tr>
      <td>YAML</td>
      <td>2001</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>Config files</td>
    </tr>
    <tr>
      <td>TOML</td>
      <td>2013</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>Simple config</td>
    </tr>
    <tr>
      <td>TOON</td>
      <td>2025</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>LLM prompts</td>
    </tr>
    <tr>
      <td>MessagePack</td>
      <td>2008</td>
      <td>No</td>
      <td>Yes</td>
      <td>No</td>
      <td>Fast JSON</td>
    </tr>
    <tr>
      <td>CBOR</td>
      <td>2013</td>
      <td>Optional</td>
      <td>Yes</td>
      <td>No</td>
      <td>IoT</td>
    </tr>
    <tr>
      <td>Avro</td>
      <td>2009</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>No</td>
      <td>Big data</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>No “best” format exists.</strong> Each optimizes for different constraints.</p>
  </li>
  <li>
    <p><strong>Text formats favor humans.</strong> JSON, YAML, TOML prioritize readability over efficiency.</p>
  </li>
  <li>
    <p><strong>Binary formats favor machines.</strong> Protobuf, MessagePack, CBOR prioritize compactness and speed.</p>
  </li>
  <li>
    <p><strong>Schema formats favor correctness.</strong> Protobuf, Avro, ASN.1 catch errors at compile time.</p>
  </li>
  <li>
    <p><strong>The tradeoff triangle is real.</strong> Readability, compactness, query performance—pick two.</p>
  </li>
</ol>

<p>The question isn’t “which format wins?” The question is: what problem are you solving?</p>

<hr />

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://www.ecma-international.org/publications-and-standards/standards/ecma-404/">ECMA-404 JSON Specification</a></li>
  <li><a href="https://datatracker.ietf.org/doc/html/rfc8259">RFC 8259 JSON</a></li>
  <li><a href="https://jsonlines.org/">JSON Lines Specification</a></li>
  <li><a href="https://www.postgresql.org/docs/current/datatype-json.html">PostgreSQL JSONB Documentation</a></li>
  <li><a href="https://developers.google.com/protocol-buffers">Protocol Buffers Documentation</a></li>
  <li><a href="https://yaml.org/spec/1.2.2/">YAML 1.2.2 Specification</a></li>
  <li><a href="https://toml.io/en/v1.0.0">TOML v1.0.0 Specification</a></li>
  <li><a href="https://datatracker.ietf.org/doc/html/rfc8949">RFC 8949 CBOR</a></li>
  <li><a href="https://github.com/msgpack/msgpack/blob/master/spec.md">MessagePack Specification</a></li>
  <li><a href="https://avro.apache.org/docs/current/spec.html">Apache Avro Specification</a></li>
</ul>

<hr />

<p><em>Data formats are design decisions. Choose based on your constraints, not trends.</em></p>

<p><em>Questions? Find me on <a href="https://www.youtube.com/@SoftwareWrighter">YouTube @SoftwareWrighter</a>.</em></p>]]></content><author><name>Software Wrighter</name></author><category term="data-formats" /><category term="fundamentals" /><category term="json" /><category term="jsonb" /><category term="jsonl" /><category term="protobuf" /><category term="yaml" /><category term="toml" /><category term="serialization" /><summary type="html"><![CDATA[JSON is everywhere. APIs. Logs. Databases. Configuration files. But it’s not alone. A whole ecosystem of formats exists—each optimizing for different tradeoffs. This post expands on the JSON et al short, providing technical depth on each format: when it was created, where it’s specified, and what problems it solves. The Tradeoff Triangle Before diving in, understand the fundamental constraint. Data formats balance three competing goals: Goal Description Human Readability Can a developer read and edit it directly? Compactness How many bytes does it take to represent data? Query Performance How fast can you access specific fields? You usually only get two. JSON optimizes readability. Protobuf optimizes compactness. JSONB optimizes query performance. No format wins everywhere. JSON: The Ubiquitous Baseline Created: 2001 (discovered/formalized by Douglas Crockford) Specification: ECMA-404 (2013), RFC 8259 (2017) File Extension: .json JSON (JavaScript Object Notation) emerged from JavaScript’s object literal syntax but became language-agnostic. Crockford didn’t invent it—he “discovered” it already existing in JavaScript and formalized the specification. Technical Details Encoding: UTF-8 text (UTF-16/32 allowed but rare) Data Types: Objects {}, arrays [], strings, numbers, booleans, null Schema: None required Comments: Not allowed in strict JSON Strengths Universal parser support (every language has one) Human readable without tools Web-native (JavaScript parses it natively) Simple specification (fits on a business card) Weaknesses Verbose (field names repeated for every object) No binary data type (must base64-encode) No comments (frustrating for config files) Parsing overhead (tokenization + string decoding every time) ELI5 Like typing a long email instead of sending a terse text. Every message spells everything out—clear, but verbose. When to Use REST APIs, configuration (when comments aren’t needed), data interchange between systems, anywhere human readability matters more than efficiency. JSONL / NDJSON: Streaming JSON Created: ~2013 (formalized) Specification: JSON Lines, NDJSON File Extension: .jsonl, .ndjson JSONL (JSON Lines) and NDJSON (Newline-Delimited JSON) are the same concept: one valid JSON object per line, separated by newlines. Technical Details {"name": "Alice", "score": 95} {"name": "Bob", "score": 87} {"name": "Carol", "score": 92} No wrapping array. Each line is independently parseable. Strengths Streaming: Process line-by-line without loading entire file Append-only: Add records without rewriting the file Parallel processing: Split by line, distribute to workers Fault-tolerant: One corrupt line doesn’t invalidate the file Weaknesses Not valid JSON (can’t parse with standard JSON parser) Still text-based (same verbosity as JSON) No random access by index ELI5 Like removing one comma per line to save some typing. Each line is self-contained, so you can grab and process them one at a time. When to Use Log files, big data pipelines (Spark, Pandas), ML datasets, event streams, anywhere you need to process records incrementally. JSONB: Binary JSON for Databases Created: 2014 (PostgreSQL 9.4) Specification: Implementation-specific (no universal standard) Storage: Database column type JSONB isn’t a file format—it’s a database storage optimization. PostgreSQL’s JSONB differs from MongoDB’s BSON, which differs from other implementations. PostgreSQL JSONB Details Parsed once: Text converted to binary on INSERT Keys sorted: Deterministic ordering for indexing Duplicates removed: Last value wins Offset table: O(log n) field lookup instead of O(n) text scanning MongoDB BSON Specification: bsonspec.org BSON (Binary JSON) is MongoDB’s serialization format. Unlike PostgreSQL’s JSONB, BSON is a standalone binary format: Type-prefixed values Supports additional types (Date, Binary, ObjectId) Length-prefixed for fast skipping ~10-15% smaller than JSON typically Strengths Fast queries without re-parsing Indexable (GIN indexes on JSONB in PostgreSQL) Type coercion at storage time Weaknesses Not portable (implementation-specific) Not human-readable INSERT overhead (parsing cost upfront) ELI5 Instead of cooking from scratch every time, you heat a pre-made meal. The prep work happens once (on INSERT), so serving (queries) is fast. When to Use Database storage where you query into JSON structures. PostgreSQL JSONB + GIN indexes enable fast @&gt; containment queries. Protocol Buffers: Google’s Schema-First Format Created: 2001 (internal Google), 2008 (open-sourced) Specification: developers.google.com/protocol-buffers File Extension: .proto (schema), binary wire format Protocol Buffers (Protobuf) is Google’s language-neutral, schema-required serialization format. It powers gRPC. Technical Details Schema definition: message Sensor { int32 temperature = 1; int32 humidity = 2; } Wire format uses field numbers, not names: Field 1: 72 Field 2: 40 Key Features Varint encoding: Small integers use fewer bytes Field numbers: Enable backward compatibility Code generation: .proto → language-specific classes No self-description: Receiver must know schema Strengths Extremely compact (3-10x smaller than JSON typically) Fast serialization/deserialization Strong versioning semantics gRPC integration Weaknesses Requires schema agreement Not human-readable Tooling required for debugging Schema evolution has rules ELI5 Everyone agrees upfront what “field 1” means. You don’t waste space spelling out “temperature”—you just send the number 1 and the value. Both sides know the code. When to Use Microservices (gRPC), internal APIs, anywhere bandwidth and latency matter more than debuggability. ASN.1: The Telecom Veteran Created: 1984 (ITU-T X.208) Specification: ITU-T X.680-X.683 Encoding Rules: BER, DER, PER, XER, and more ASN.1 (Abstract Syntax Notation One) predates all modern formats. It defines both schema and encoding, with multiple encoding rules for different use cases. Encoding Rules Comparison Rule Use Case BER (Basic Encoding Rules) Flexible, general purpose DER (Distinguished Encoding Rules) Deterministic, for cryptography PER (Packed Encoding Rules) Most compact, for bandwidth-constrained XER (XML Encoding Rules) XML-based, for interop Where You See ASN.1 X.509 certificates (SSL/TLS certs are DER-encoded ASN.1) LDAP (directory services) SNMP (network management) Telecom protocols (SS7, GSM, LTE) Strengths Bit-level precision Proven over 40 years Multiple encoding options Formal verification possible Weaknesses Complex specification Steep learning curve Tooling can be expensive Security vulnerabilities in parsers (historically) ELI5 Same idea as Protobuf—everyone agrees upfront what each field number means. ASN.1 just got there 20 years earlier and handles even more edge cases. When to Use You probably won’t choose ASN.1 for new projects. You’ll encounter it in cryptography, certificates, and legacy telecom systems. YAML: Human-Friendly Configuration Created: 2001 (Clark Evans, Ingy döt Net, Oren Ben-Kiki) Specification: yaml.org/spec/1.2.2 File Extension: .yaml, .yml YAML (YAML Ain’t Markup Language) prioritizes human readability. It’s a superset of JSON—any valid JSON is valid YAML. Technical Details # Comments allowed! server: host: localhost port: 8080 features: - auth - logging Key Features Indentation-based: Whitespace matters Comments: # for single-line Anchors/aliases: &amp;name and *name for references Multiple documents: --- separator Strengths Highly readable Comments supported Multi-line strings without escaping Complex data structures Weaknesses “Norway problem”: NO parses as boolean false Whitespace sensitivity causes errors Multiple ways to express same data Security concerns (arbitrary code execution in some parsers) ELI5 Optimized for clarity, not bandwidth. YAML is for humans editing config files—not for machines exchanging data over networks. When to Use Configuration files (Kubernetes, Docker Compose, CI/CD), anywhere humans edit data directly and comments help. TOML: Minimal Configuration Created: 2013 (Tom Preston-Werner) Specification: toml.io File Extension: .toml TOML (Tom’s Obvious Minimal Language) emerged as a reaction to YAML’s complexity. It’s used by Rust (Cargo.toml), Python (pyproject.toml), and others. Technical Details [server] host = "localhost" port = 8080 [server.features] auth = true logging = true Key Features Explicit typing: Dates, times, arrays have clear syntax Sections: [section] and [section.subsection] No anchors: Intentionally simpler than YAML Deterministic: Same data = same representation Strengths Easy to read and write Unambiguous parsing Clear error messages Growing ecosystem support Weaknesses Less expressive than YAML Nested structures can be verbose Smaller ecosystem than JSON/YAML ELI5 Same goal as YAML—clarity for humans, not bandwidth for machines—but with stricter rules so you make fewer mistakes. When to Use Configuration files where YAML’s complexity isn’t needed. Rust projects (mandatory). Python packaging (pyproject.toml). TOON: Token-Optimized for LLMs Created: October 2025 (toon-format organization) Specification: github.com/toon-format/toon (v3.0) File Extension: .toon Media Type: text/toon (provisional) TOON (Token Oriented Object Notation) is the newest format in this list, designed specifically for LLM input. It’s a lossless representation of JSON that minimizes tokens. Technical Details TOON combines YAML-style indentation for nested objects with CSV-like tabular layouts for uniform arrays: users[2]{name,age}: Alice,25 Bob,30 Equivalent JSON: {"users": [{"name": "Alice", "age": 25}, {"name": "Bob", "age": 30}]} Key Features Header-based: Field names declared once, values follow 40% fewer tokens: Than equivalent JSON typically Lossless: Round-trips to JSON perfectly UTF-8 always: No encoding ambiguity Performance Metric JSON TOON Accuracy 69.7% 73.9% Efficiency (acc/1K tokens) 15.3 26.9 Strengths Significant token savings at scale Better context window utilization Lower API costs for LLM applications Human-readable (unlike binary formats) Weaknesses New format (October 2025) Limited tooling compared to JSON Requires conversion layer for existing systems Not yet widely adopted ELI5 Like having one header row for each column in a table instead of repeating the column name for every single row. You declare field names once, then just list the values. When to Use LLM prompts with structured data, RAG applications, anywhere token efficiency matters. Especially useful for large datasets with uniform object arrays. Implementations TypeScript: Reference implementation Python: toons (Rust-based, fast) Go, Rust, .NET: Available via toon-format org Alternatives Not in the Video MessagePack Created: 2008 (Sadayuki Furuhashi) Specification: msgpack.org Binary JSON without schema. Type-prefixed values, efficient numeric encoding. Use when: You want JSON semantics but smaller/faster. CBOR Created: 2013 (IETF) Specification: RFC 8949 Concise Binary Object Representation. Designed for constrained environments (IoT). Use when: Resource-constrained devices, need a standard binary format. Apache Avro Created: 2009 (Apache, Doug Cutting) Specification: avro.apache.org Schema-based, row-oriented binary format. Schema embedded or stored separately. Strong schema evolution support. Use when: Big data pipelines (Hadoop, Kafka), schema evolution is critical. Apache Parquet Created: 2013 (Twitter + Cloudera) Specification: parquet.apache.org Columnar storage format. Not for serialization—for analytics storage. Use when: Large-scale analytics, data warehousing, Spark/Pandas workflows. Cap’n Proto Created: 2013 (Kenton Varda, ex-Protobuf author) Specification: capnproto.org Zero-copy serialization. The serialized form is the in-memory form. Use when: Extreme performance requirements, inter-process communication. FlatBuffers Created: 2014 (Google) Specification: google.github.io/flatbuffers Zero-copy like Cap’n Proto but with better tooling. Used in games, mobile. Use when: Games, mobile apps, anywhere memory allocation matters. Quick Reference Format Year Schema Binary Human-Readable Best For JSON 2001 No No Yes APIs, interchange JSONL 2013 No No Yes Logs, streaming JSONB 2014 No Yes No Database queries Protobuf 2008 Yes Yes No Microservices ASN.1 1984 Yes Yes No Crypto, telecom YAML 2001 No No Yes Config files TOML 2013 No No Yes Simple config TOON 2025 No No Yes LLM prompts MessagePack 2008 No Yes No Fast JSON CBOR 2013 Optional Yes No IoT Avro 2009 Yes Yes No Big data Key Takeaways No “best” format exists. Each optimizes for different constraints. Text formats favor humans. JSON, YAML, TOML prioritize readability over efficiency. Binary formats favor machines. Protobuf, MessagePack, CBOR prioritize compactness and speed. Schema formats favor correctness. Protobuf, Avro, ASN.1 catch errors at compile time. The tradeoff triangle is real. Readability, compactness, query performance—pick two. The question isn’t “which format wins?” The question is: what problem are you solving? Resources ECMA-404 JSON Specification RFC 8259 JSON JSON Lines Specification PostgreSQL JSONB Documentation Protocol Buffers Documentation YAML 1.2.2 Specification TOML v1.0.0 Specification RFC 8949 CBOR MessagePack Specification Apache Avro Specification Data formats are design decisions. Choose based on your constraints, not trends. Questions? Find me on YouTube @SoftwareWrighter.]]></summary></entry><entry><title type="html">Five ML Concepts - #18</title><link href="https://software-wrighter-lab.github.io/2026/02/21/five-ml-concepts-18/" rel="alternate" type="text/html" title="Five ML Concepts - #18" /><published>2026-02-21T00:00:00-08:00</published><updated>2026-02-21T00:00:00-08:00</updated><id>https://software-wrighter-lab.github.io/2026/02/21/five-ml-concepts-18</id><content type="html" xml:base="https://software-wrighter-lab.github.io/2026/02/21/five-ml-concepts-18/"><![CDATA[<p><img src="/assets/images/posts/block-eighteen.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/osj1GQxG4uo">Five ML Concepts #18</a><br /><a href="https://www.youtube.com/shorts/osj1GQxG4uo"><img src="https://img.youtube.com/vi/osj1GQxG4uo/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Preference Learning</strong></td>
        <td><a href="https://arxiv.org/abs/2009.01325">Learning to summarize from human feedback</a> (Stiennon et al. 2020)</td>
      </tr>
      <tr>
        <td><strong>Ensembling</strong></td>
        <td><a href="https://link.springer.com/chapter/10.1007/3-540-45014-9_1">Ensemble Methods in Machine Learning</a> (Dietterich 2000)</td>
      </tr>
      <tr>
        <td><strong>ML Fragility</strong></td>
        <td><a href="https://arxiv.org/abs/1903.12261">Distribution Shift</a> (Quinonero-Candela et al. 2009)</td>
      </tr>
      <tr>
        <td><strong>Epoch</strong></td>
        <td><a href="https://www.deeplearningbook.org/">Deep Learning</a> (Goodfellow et al. 2016), Chapter 8</td>
      </tr>
      <tr>
        <td><strong>Cost vs Quality</strong></td>
        <td><a href="https://arxiv.org/abs/2009.06732">Efficient Transformers: A Survey</a> (Tay et al. 2022)</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-preference-learning">1. Preference Learning</h3>

<p><strong>Instead of learning from fixed labels, models are trained from comparisons between outputs.</strong> This helps align model behavior with human judgments.</p>

<p>The approach works well when absolute quality is hard to define but relative preferences are easier to express.</p>

<blockquote>
  <p>Like learning to cook by asking which dish tastes better.</p>
</blockquote>

<h3 id="2-ensembling">2. Ensembling</h3>

<p><strong>Ensembling combines predictions from multiple models.</strong> Different models make different errors, and combining them can improve robustness.</p>

<p>Common strategies include voting, averaging, and stacking models together.</p>

<blockquote>
  <p>Like asking several experts and averaging their opinions.</p>
</blockquote>

<h3 id="3-why-ml-is-fragile">3. Why ML Is Fragile</h3>

<p><strong>Models rely on statistical patterns learned from data.</strong> When those patterns shift, performance can degrade quickly.</p>

<p>This fragility emerges because models optimize for training distributions, not arbitrary future scenarios.</p>

<blockquote>
  <p>Like a spell checker that works on common words but struggles with unusual ones.</p>
</blockquote>

<h3 id="4-epoch">4. Epoch</h3>

<p><strong>An epoch is one complete pass through the training dataset.</strong> Multiple epochs allow the model to refine its weights over repeated passes.</p>

<p>Training typically continues for many epochs until validation performance stops improving.</p>

<blockquote>
  <p>Like reading a textbook from beginning to end more than once.</p>
</blockquote>

<h3 id="5-cost-vs-quality-tradeoffs">5. Cost vs Quality Tradeoffs</h3>

<p><strong>Increasing model size or compute often improves performance, but also increases cost and latency.</strong> Engineers balance quality against budget and responsiveness.</p>

<p>Production systems often use smaller, faster models rather than the largest available.</p>

<blockquote>
  <p>Like choosing between a luxury car and an economy car depending on your needs.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Preference Learning</strong></td>
      <td>Train from comparisons, not labels</td>
    </tr>
    <tr>
      <td><strong>Ensembling</strong></td>
      <td>Combine models for robustness</td>
    </tr>
    <tr>
      <td><strong>ML Fragility</strong></td>
      <td>Statistical models break on distribution shift</td>
    </tr>
    <tr>
      <td><strong>Epoch</strong></td>
      <td>One pass through training data</td>
    </tr>
    <tr>
      <td><strong>Cost vs Quality</strong></td>
      <td>Bigger isn’t always better in production</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 18 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/22/five-ml-concepts-19/">Next: #19 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>]]></content><author><name>Software Wrighter</name></author><category term="llm" /><category term="machine-learning" /><category term="explainers" /><category term="five-ml-concepts" /><category term="preference-learning" /><category term="ensembling" /><category term="ml-fragility" /><category term="epoch" /><category term="cost-quality-tradeoffs" /><category term="ml-concepts" /><summary type="html"><![CDATA[5 machine learning concepts. Under 30 seconds each. Resource Link Papers Links in References section Video Five ML Concepts #18 References Concept Reference Preference Learning Learning to summarize from human feedback (Stiennon et al. 2020) Ensembling Ensemble Methods in Machine Learning (Dietterich 2000) ML Fragility Distribution Shift (Quinonero-Candela et al. 2009) Epoch Deep Learning (Goodfellow et al. 2016), Chapter 8 Cost vs Quality Efficient Transformers: A Survey (Tay et al. 2022) Today’s Five 1. Preference Learning Instead of learning from fixed labels, models are trained from comparisons between outputs. This helps align model behavior with human judgments. The approach works well when absolute quality is hard to define but relative preferences are easier to express. Like learning to cook by asking which dish tastes better. 2. Ensembling Ensembling combines predictions from multiple models. Different models make different errors, and combining them can improve robustness. Common strategies include voting, averaging, and stacking models together. Like asking several experts and averaging their opinions. 3. Why ML Is Fragile Models rely on statistical patterns learned from data. When those patterns shift, performance can degrade quickly. This fragility emerges because models optimize for training distributions, not arbitrary future scenarios. Like a spell checker that works on common words but struggles with unusual ones. 4. Epoch An epoch is one complete pass through the training dataset. Multiple epochs allow the model to refine its weights over repeated passes. Training typically continues for many epochs until validation performance stops improving. Like reading a textbook from beginning to end more than once. 5. Cost vs Quality Tradeoffs Increasing model size or compute often improves performance, but also increases cost and latency. Engineers balance quality against budget and responsiveness. Production systems often use smaller, faster models rather than the largest available. Like choosing between a luxury car and an economy car depending on your needs. Quick Reference Concept One-liner Preference Learning Train from comparisons, not labels Ensembling Combine models for robustness ML Fragility Statistical models break on distribution shift Epoch One pass through training data Cost vs Quality Bigger isn’t always better in production *Part 18 of the Five ML Concepts series. View all parts Next: #19 →* Short, accurate ML explainers. Follow for more.]]></summary></entry><entry><title type="html">Five ML Concepts - #17</title><link href="https://software-wrighter-lab.github.io/2026/02/20/five-ml-concepts-17/" rel="alternate" type="text/html" title="Five ML Concepts - #17" /><published>2026-02-20T00:00:00-08:00</published><updated>2026-02-20T00:00:00-08:00</updated><id>https://software-wrighter-lab.github.io/2026/02/20/five-ml-concepts-17</id><content type="html" xml:base="https://software-wrighter-lab.github.io/2026/02/20/five-ml-concepts-17/"><![CDATA[<p><img src="/assets/images/posts/block-seventeen.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/Xk2hkc0bgi4">Five ML Concepts #17</a><br /><a href="https://www.youtube.com/shorts/Xk2hkc0bgi4"><img src="https://img.youtube.com/vi/Xk2hkc0bgi4/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Benchmark Leakage</strong></td>
        <td><a href="https://arxiv.org/abs/1512.00567">Rethinking the Inception Architecture for Computer Vision</a> (Szegedy et al. 2016)</td>
      </tr>
      <tr>
        <td><strong>Concept/Data Drift</strong></td>
        <td><a href="https://ieeexplore.ieee.org/document/8496795">Learning under Concept Drift: A Review</a> (Lu et al. 2018)</td>
      </tr>
      <tr>
        <td><strong>Weight Decay</strong></td>
        <td><a href="https://arxiv.org/abs/1711.05101">Decoupled Weight Decay Regularization</a> (Loshchilov &amp; Hutter 2019)</td>
      </tr>
      <tr>
        <td><strong>Scaling Laws</strong></td>
        <td><a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a> (Kaplan et al. 2020)</td>
      </tr>
      <tr>
        <td><strong>Shadow Deployment</strong></td>
        <td><a href="https://www.oreilly.com/library/view/reliable-machine-learning/9781098106218/">Reliable Machine Learning</a> (Cathy Chen et al. 2022)</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-benchmark-leakage">1. Benchmark Leakage</h3>

<p><strong>When benchmark or test data influences training, tuning, or model selection, evaluation results become unreliable.</strong> This inflates reported performance beyond real-world capability.</p>

<p>Strict separation between development and evaluation data is essential for honest assessment.</p>

<blockquote>
  <p>Like practicing with the exact questions that will appear on the final exam.</p>
</blockquote>

<h3 id="2-concept-drift-vs-data-drift">2. Concept Drift vs Data Drift</h3>

<p><strong>Data drift occurs when input distributions change. Concept drift occurs when the relationship between inputs and outputs changes.</strong> Both can degrade model performance over time.</p>

<p>Data drift: customers buy different products. Concept drift: what “good” means has changed.</p>

<blockquote>
  <p>Like customers buying different products versus products changing what they mean.</p>
</blockquote>

<h3 id="3-weight-decay">3. Weight Decay</h3>

<p><strong>A regularization method that penalizes large weights, often implemented as L2 regularization.</strong> This encourages simpler models that generalize better.</p>

<p>Weight decay adds a term proportional to the squared magnitude of weights to the loss function.</p>

<blockquote>
  <p>Like encouraging shorter, simpler answers instead of overly complicated ones.</p>
</blockquote>

<h3 id="4-scaling-laws">4. Scaling Laws</h3>

<p><strong>Empirical relationships showing how performance tends to improve as model size, data, or compute increase.</strong> These relationships follow predictable power-law curves.</p>

<p>Scaling laws help predict resource requirements for target performance levels.</p>

<blockquote>
  <p>Like noticing that adding horsepower often increases a car’s speed, but with diminishing returns.</p>
</blockquote>

<h3 id="5-shadow-deployment">5. Shadow Deployment</h3>

<p><strong>Running a new model in parallel with production without affecting live user decisions.</strong> The shadow model processes real traffic but its outputs are only logged, not served.</p>

<p>This allows safe evaluation before full deployment.</p>

<blockquote>
  <p>Like a new chef preparing the same dishes in the back kitchen before serving customers.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Benchmark Leakage</strong></td>
      <td>Test data contaminating training/selection</td>
    </tr>
    <tr>
      <td><strong>Concept vs Data Drift</strong></td>
      <td>Changed relationships vs changed inputs</td>
    </tr>
    <tr>
      <td><strong>Weight Decay</strong></td>
      <td>L2 penalty discourages large weights</td>
    </tr>
    <tr>
      <td><strong>Scaling Laws</strong></td>
      <td>Performance scales predictably with resources</td>
    </tr>
    <tr>
      <td><strong>Shadow Deployment</strong></td>
      <td>Test safely alongside production</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 17 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/21/five-ml-concepts-18/">Next: #18 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>]]></content><author><name>Software Wrighter</name></author><category term="llm" /><category term="machine-learning" /><category term="explainers" /><category term="five-ml-concepts" /><category term="benchmark-leakage" /><category term="concept-drift" /><category term="data-drift" /><category term="weight-decay" /><category term="scaling-laws" /><category term="shadow-deployment" /><category term="ml-concepts" /><summary type="html"><![CDATA[5 machine learning concepts. Under 30 seconds each. Resource Link Papers Links in References section Video Five ML Concepts #17 References Concept Reference Benchmark Leakage Rethinking the Inception Architecture for Computer Vision (Szegedy et al. 2016) Concept/Data Drift Learning under Concept Drift: A Review (Lu et al. 2018) Weight Decay Decoupled Weight Decay Regularization (Loshchilov &amp; Hutter 2019) Scaling Laws Scaling Laws for Neural Language Models (Kaplan et al. 2020) Shadow Deployment Reliable Machine Learning (Cathy Chen et al. 2022) Today’s Five 1. Benchmark Leakage When benchmark or test data influences training, tuning, or model selection, evaluation results become unreliable. This inflates reported performance beyond real-world capability. Strict separation between development and evaluation data is essential for honest assessment. Like practicing with the exact questions that will appear on the final exam. 2. Concept Drift vs Data Drift Data drift occurs when input distributions change. Concept drift occurs when the relationship between inputs and outputs changes. Both can degrade model performance over time. Data drift: customers buy different products. Concept drift: what “good” means has changed. Like customers buying different products versus products changing what they mean. 3. Weight Decay A regularization method that penalizes large weights, often implemented as L2 regularization. This encourages simpler models that generalize better. Weight decay adds a term proportional to the squared magnitude of weights to the loss function. Like encouraging shorter, simpler answers instead of overly complicated ones. 4. Scaling Laws Empirical relationships showing how performance tends to improve as model size, data, or compute increase. These relationships follow predictable power-law curves. Scaling laws help predict resource requirements for target performance levels. Like noticing that adding horsepower often increases a car’s speed, but with diminishing returns. 5. Shadow Deployment Running a new model in parallel with production without affecting live user decisions. The shadow model processes real traffic but its outputs are only logged, not served. This allows safe evaluation before full deployment. Like a new chef preparing the same dishes in the back kitchen before serving customers. Quick Reference Concept One-liner Benchmark Leakage Test data contaminating training/selection Concept vs Data Drift Changed relationships vs changed inputs Weight Decay L2 penalty discourages large weights Scaling Laws Performance scales predictably with resources Shadow Deployment Test safely alongside production *Part 17 of the Five ML Concepts series. View all parts Next: #18 →* Short, accurate ML explainers. Follow for more.]]></summary></entry><entry><title type="html">midi-cli-rs: Music Generation for AI Coding Agents</title><link href="https://software-wrighter-lab.github.io/2026/02/20/midi-cli-rs-music-for-ai-agents/" rel="alternate" type="text/html" title="midi-cli-rs: Music Generation for AI Coding Agents" /><published>2026-02-20T00:00:00-08:00</published><updated>2026-02-20T00:00:00-08:00</updated><id>https://software-wrighter-lab.github.io/2026/02/20/midi-cli-rs-music-for-ai-agents</id><content type="html" xml:base="https://software-wrighter-lab.github.io/2026/02/20/midi-cli-rs-music-for-ai-agents/"><![CDATA[<p><img src="/assets/images/posts/block-notes.png" class="post-marker" alt="" /></p>

<p>AI coding agents can write code, generate images, and produce text. But what about music? When I needed background audio for explainer videos, I wanted a tool that AI agents could use directly—no music theory required.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://youtu.be/nDNcbKE8KtE">midi-cli-rs Explainer</a><br /><a href="https://youtu.be/nDNcbKE8KtE"><img src="https://img.youtube.com/vi/nDNcbKE8KtE/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>Examples</strong></td>
        <td><a href="https://softwarewrighter.github.io/midi-cli-rs/">Listen to Samples</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/midi-cli-rs">midi-cli-rs</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-problem">The Problem</h2>

<p>Generating music programmatically is hard. Traditional approaches require understanding music theory, MIDI specifications, instrument mappings, and audio synthesis. That’s a lot to ask of an AI agent that just needs a 5-second intro.</p>

<p>I wanted something simpler: a CLI tool where an agent could say “give me 5 seconds of suspenseful music” and get a usable WAV file.</p>

<h2 id="the-solution-mood-presets">The Solution: Mood Presets</h2>

<p>midi-cli-rs solves this with <strong>mood presets</strong>—curated musical generators that produce complete compositions from a single command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Generate a 5-second suspenseful intro</span>
midi-cli-rs preset <span class="nt">--mood</span> suspense <span class="nt">--duration</span> 5 <span class="nt">-o</span> intro.wav

<span class="c"># Upbeat outro with specific key</span>
midi-cli-rs preset <span class="nt">-m</span> upbeat <span class="nt">-d</span> 7 <span class="nt">--key</span> C <span class="nt">--seed</span> 42 <span class="nt">-o</span> outro.wav
</code></pre></div></div>

<p>Six moods are available:</p>

<table>
  <thead>
    <tr>
      <th>Mood</th>
      <th>Character</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">suspense</code></td>
      <td>Low drones, tremolo strings, tension</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">eerie</code></td>
      <td>Sparse tones, diminished harmony</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">upbeat</code></td>
      <td>Rhythmic chords, energetic</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">calm</code></td>
      <td>Warm pads, gentle arpeggios</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">ambient</code></td>
      <td>Textural drones, pentatonic bells</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">jazz</code></td>
      <td>Walking bass, brushed drums, piano trio</td>
    </tr>
  </tbody>
</table>

<p>Each mood generates multi-layer compositions with appropriate instruments, rhythms, and harmonies. The <code class="language-plaintext highlighter-rouge">--seed</code> parameter ensures reproducibility—same seed, same output. Different seeds produce meaningful variations in melody contour, rhythm patterns, and instrument choices.</p>

<h2 id="melodic-variation">Melodic Variation</h2>

<p>The presets don’t just randomize notes—they use a <strong>contour-based variation system</strong>. Changing the seed produces melodies that follow different shapes (ascending, descending, arch, wave) while staying musically coherent. This means you can generate multiple versions of a mood and pick the one that fits best.</p>

<h2 id="how-it-works">How It Works</h2>

<p>The tool generates MIDI programmatically, then renders to WAV using FluidSynth:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mood Preset → MIDI Generation → FluidSynth → WAV Output
</code></pre></div></div>

<p><strong>MIDI generation</strong> uses the <code class="language-plaintext highlighter-rouge">midly</code> crate to create standard MIDI files. Each preset generates multiple tracks with different instruments, note patterns, and dynamics.</p>

<p><strong>Audio rendering</strong> calls FluidSynth as a subprocess with a SoundFont (instrument samples). This avoids LGPL licensing complications—subprocess execution doesn’t trigger copyleft.</p>

<h2 id="note-level-control">Note-Level Control</h2>

<p>When presets aren’t enough, you can specify exact notes:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Note format: PITCH:DURATION:VELOCITY[@OFFSET]</span>
midi-cli-rs generate <span class="se">\</span>
    <span class="nt">--notes</span> <span class="s2">"C4:0.5:80@0,E4:0.5:80@0.5,G4:0.5:80@1,C5:1:90@1.5"</span> <span class="se">\</span>
    <span class="nt">-i</span> piano <span class="nt">-t</span> 120 <span class="nt">-o</span> arpeggio.wav
</code></pre></div></div>

<p>Or use JSON for complex multi-track arrangements:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s1">'{"tempo":90,"instrument":"piano","notes":[
  {"pitch":"C4","duration":0.5,"velocity":80,"offset":0},
  {"pitch":"E4","duration":0.5,"velocity":80,"offset":0.5},
  {"pitch":"G4","duration":1,"velocity":90,"offset":1}
]}'</span> | midi-cli-rs generate <span class="nt">--json</span> <span class="nt">-o</span> output.wav
</code></pre></div></div>

<h2 id="web-ui">Web UI</h2>

<p>For interactive composition, there’s a browser-based interface:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>midi-cli-rs serve  <span class="c"># Starts on http://127.0.0.1:3105</span>
</code></pre></div></div>

<p>The <strong>Presets tab</strong> lets you adjust mood, key, duration, intensity, and tempo with immediate audio preview. Click the clock button to generate a time-based seed for unique but reproducible results.</p>

<p>The <strong>Melodies tab</strong> provides note-by-note composition with keyboard shortcuts:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">a-g</code> for note pitch</li>
  <li><code class="language-plaintext highlighter-rouge">[</code> / <code class="language-plaintext highlighter-rouge">]</code> to adjust duration</li>
  <li><code class="language-plaintext highlighter-rouge">+</code> / <code class="language-plaintext highlighter-rouge">-</code> to change octave</li>
  <li><code class="language-plaintext highlighter-rouge">Tab</code> to navigate between notes</li>
</ul>

<h2 id="for-ai-agents">For AI Agents</h2>

<p>The CLI is designed for AI agent usage:</p>

<ol>
  <li><strong>Simple commands</strong>: One line generates complete audio</li>
  <li><strong>Reproducible</strong>: Seed values ensure consistent output</li>
  <li><strong>Self-documenting</strong>: <code class="language-plaintext highlighter-rouge">--help</code> includes agent-specific instructions</li>
  <li><strong>Composable</strong>: Generate tracks separately, combine with ffmpeg</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># AI agent workflow</span>
midi-cli-rs preset <span class="nt">-m</span> suspense <span class="nt">-d</span> 5 <span class="nt">--seed</span> 1 <span class="nt">-o</span> intro.wav
midi-cli-rs preset <span class="nt">-m</span> upbeat <span class="nt">-d</span> 10 <span class="nt">--seed</span> 2 <span class="nt">-o</span> main.wav
ffmpeg <span class="nt">-i</span> intro.wav <span class="nt">-i</span> main.wav <span class="nt">-filter_complex</span> <span class="nv">concat</span><span class="o">=</span><span class="nv">n</span><span class="o">=</span>2:v<span class="o">=</span>0:a<span class="o">=</span>1 final.wav
</code></pre></div></div>

<h2 id="soundfont-quality-matters">SoundFont Quality Matters</h2>

<p>The quality of generated audio depends heavily on the SoundFont used. SoundFonts are collections of audio samples for each instrument—a tiny SoundFont with compressed samples will sound thin and artificial, while a larger one with high-quality recordings produces professional results.</p>

<table>
  <thead>
    <tr>
      <th>SoundFont</th>
      <th>Size</th>
      <th>Quality</th>
      <th>License</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>TimGM6mb</td>
      <td>~6MB</td>
      <td>Basic</td>
      <td>GPL v2</td>
    </tr>
    <tr>
      <td>GeneralUser GS</td>
      <td>~30MB</td>
      <td>Good</td>
      <td>Permissive</td>
    </tr>
    <tr>
      <td>FluidR3_GM</td>
      <td>~140MB</td>
      <td>Very Good</td>
      <td>MIT</td>
    </tr>
    <tr>
      <td>MuseScore_General</td>
      <td>~200MB</td>
      <td>Excellent</td>
      <td>MIT</td>
    </tr>
  </tbody>
</table>

<p>For anything beyond quick prototypes, use a quality SoundFont. The difference is dramatic—the same MIDI file can sound like a toy keyboard or a real instrument depending on the samples.</p>

<p>The tool auto-detects SoundFonts in common locations (<code class="language-plaintext highlighter-rouge">~/.soundfonts/</code>, <code class="language-plaintext highlighter-rouge">/opt/homebrew/share/soundfonts/</code>, etc.), or specify one explicitly with <code class="language-plaintext highlighter-rouge">--soundfont</code>.</p>

<h2 id="technical-details">Technical Details</h2>

<p>Built with Rust 2024 edition using permissively licensed dependencies:</p>

<table>
  <thead>
    <tr>
      <th>Crate</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>midly</td>
      <td>MIDI file generation</td>
    </tr>
    <tr>
      <td>clap</td>
      <td>CLI argument parsing</td>
    </tr>
    <tr>
      <td>serde</td>
      <td>JSON serialization</td>
    </tr>
    <tr>
      <td>rand</td>
      <td>Randomization for presets</td>
    </tr>
    <tr>
      <td>axum</td>
      <td>Web server (for <code class="language-plaintext highlighter-rouge">serve</code> command)</td>
    </tr>
  </tbody>
</table>

<p>FluidSynth is called as a subprocess for WAV rendering, keeping the main codebase MIT-licensed.</p>

<h2 id="try-it">Try It</h2>

<p>Listen to <a href="https://softwarewrighter.github.io/midi-cli-rs/">sample outputs</a>, or build locally:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/softwarewrighter/midi-cli-rs.git
<span class="nb">cd </span>midi-cli-rs
cargo build <span class="nt">--release</span>
./target/release/midi-cli-rs preset <span class="nt">-m</span> jazz <span class="nt">-d</span> 5 <span class="nt">-o</span> jazz.wav
</code></pre></div></div>

<p>Requires FluidSynth for WAV output (<code class="language-plaintext highlighter-rouge">brew install fluid-synth</code> on macOS).</p>

<hr />

<p><em>Music generation shouldn’t require a music degree. With mood presets, AI agents can add audio to their creative toolkit.</em></p>]]></content><author><name>Software Wrighter</name></author><category term="tools" /><category term="rust" /><category term="ai-agents" /><category term="vibe-coding" /><category term="rust" /><category term="midi" /><category term="music" /><category term="ai-agents" /><category term="cli" /><category term="fluidsynth" /><category term="vibe-coding" /><category term="claude-code" /><category term="personal-software" /><summary type="html"><![CDATA[AI coding agents can write code, generate images, and produce text. But what about music? When I needed background audio for explainer videos, I wanted a tool that AI agents could use directly—no music theory required. Resource Link Video midi-cli-rs Explainer Examples Listen to Samples Code midi-cli-rs The Problem Generating music programmatically is hard. Traditional approaches require understanding music theory, MIDI specifications, instrument mappings, and audio synthesis. That’s a lot to ask of an AI agent that just needs a 5-second intro. I wanted something simpler: a CLI tool where an agent could say “give me 5 seconds of suspenseful music” and get a usable WAV file. The Solution: Mood Presets midi-cli-rs solves this with mood presets—curated musical generators that produce complete compositions from a single command: # Generate a 5-second suspenseful intro midi-cli-rs preset --mood suspense --duration 5 -o intro.wav # Upbeat outro with specific key midi-cli-rs preset -m upbeat -d 7 --key C --seed 42 -o outro.wav Six moods are available: Mood Character suspense Low drones, tremolo strings, tension eerie Sparse tones, diminished harmony upbeat Rhythmic chords, energetic calm Warm pads, gentle arpeggios ambient Textural drones, pentatonic bells jazz Walking bass, brushed drums, piano trio Each mood generates multi-layer compositions with appropriate instruments, rhythms, and harmonies. The --seed parameter ensures reproducibility—same seed, same output. Different seeds produce meaningful variations in melody contour, rhythm patterns, and instrument choices. Melodic Variation The presets don’t just randomize notes—they use a contour-based variation system. Changing the seed produces melodies that follow different shapes (ascending, descending, arch, wave) while staying musically coherent. This means you can generate multiple versions of a mood and pick the one that fits best. How It Works The tool generates MIDI programmatically, then renders to WAV using FluidSynth: Mood Preset → MIDI Generation → FluidSynth → WAV Output MIDI generation uses the midly crate to create standard MIDI files. Each preset generates multiple tracks with different instruments, note patterns, and dynamics. Audio rendering calls FluidSynth as a subprocess with a SoundFont (instrument samples). This avoids LGPL licensing complications—subprocess execution doesn’t trigger copyleft. Note-Level Control When presets aren’t enough, you can specify exact notes: # Note format: PITCH:DURATION:VELOCITY[@OFFSET] midi-cli-rs generate \ --notes "C4:0.5:80@0,E4:0.5:80@0.5,G4:0.5:80@1,C5:1:90@1.5" \ -i piano -t 120 -o arpeggio.wav Or use JSON for complex multi-track arrangements: echo '{"tempo":90,"instrument":"piano","notes":[ {"pitch":"C4","duration":0.5,"velocity":80,"offset":0}, {"pitch":"E4","duration":0.5,"velocity":80,"offset":0.5}, {"pitch":"G4","duration":1,"velocity":90,"offset":1} ]}' | midi-cli-rs generate --json -o output.wav Web UI For interactive composition, there’s a browser-based interface: midi-cli-rs serve # Starts on http://127.0.0.1:3105 The Presets tab lets you adjust mood, key, duration, intensity, and tempo with immediate audio preview. Click the clock button to generate a time-based seed for unique but reproducible results. The Melodies tab provides note-by-note composition with keyboard shortcuts: a-g for note pitch [ / ] to adjust duration + / - to change octave Tab to navigate between notes For AI Agents The CLI is designed for AI agent usage: Simple commands: One line generates complete audio Reproducible: Seed values ensure consistent output Self-documenting: --help includes agent-specific instructions Composable: Generate tracks separately, combine with ffmpeg # AI agent workflow midi-cli-rs preset -m suspense -d 5 --seed 1 -o intro.wav midi-cli-rs preset -m upbeat -d 10 --seed 2 -o main.wav ffmpeg -i intro.wav -i main.wav -filter_complex concat=n=2:v=0:a=1 final.wav SoundFont Quality Matters The quality of generated audio depends heavily on the SoundFont used. SoundFonts are collections of audio samples for each instrument—a tiny SoundFont with compressed samples will sound thin and artificial, while a larger one with high-quality recordings produces professional results. SoundFont Size Quality License TimGM6mb ~6MB Basic GPL v2 GeneralUser GS ~30MB Good Permissive FluidR3_GM ~140MB Very Good MIT MuseScore_General ~200MB Excellent MIT For anything beyond quick prototypes, use a quality SoundFont. The difference is dramatic—the same MIDI file can sound like a toy keyboard or a real instrument depending on the samples. The tool auto-detects SoundFonts in common locations (~/.soundfonts/, /opt/homebrew/share/soundfonts/, etc.), or specify one explicitly with --soundfont. Technical Details Built with Rust 2024 edition using permissively licensed dependencies: Crate Purpose midly MIDI file generation clap CLI argument parsing serde JSON serialization rand Randomization for presets axum Web server (for serve command) FluidSynth is called as a subprocess for WAV rendering, keeping the main codebase MIT-licensed. Try It Listen to sample outputs, or build locally: git clone https://github.com/softwarewrighter/midi-cli-rs.git cd midi-cli-rs cargo build --release ./target/release/midi-cli-rs preset -m jazz -d 5 -o jazz.wav Requires FluidSynth for WAV output (brew install fluid-synth on macOS). Music generation shouldn’t require a music degree. With mood presets, AI agents can add audio to their creative toolkit.]]></summary></entry><entry><title type="html">TBT (4/?): ToonTalk - Teaching Robots to Program</title><link href="https://software-wrighter-lab.github.io/2026/02/19/tbt-toontalk-visual-programming/" rel="alternate" type="text/html" title="TBT (4/?): ToonTalk - Teaching Robots to Program" /><published>2026-02-19T00:00:00-08:00</published><updated>2026-02-19T00:00:00-08:00</updated><id>https://software-wrighter-lab.github.io/2026/02/19/tbt-toontalk-visual-programming</id><content type="html" xml:base="https://software-wrighter-lab.github.io/2026/02/19/tbt-toontalk-visual-programming/"><![CDATA[<p><img src="/assets/images/posts/block-robot.png" class="post-marker" alt="" /></p>

<p>I first discovered ToonTalk during the Windows XP era—probably around 2003 or 2004. It was unlike anything I’d seen: a programming environment disguised as a video game where you trained robots by showing them what to do. The concept stuck with me for two decades.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://youtu.be/qrcWMOfHN2s">ToonTalk in Rust</a><br /><a href="https://youtu.be/qrcWMOfHN2s"><img src="https://img.youtube.com/vi/qrcWMOfHN2s/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>tt-rs Demo</strong></td>
        <td><a href="https://sw-fun.github.io/tt-rs/">Live Demo</a></td>
      </tr>
      <tr>
        <td><strong>tt-rs Repo</strong></td>
        <td><a href="https://github.com/sw-fun/tt-rs">tt-rs</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="what-is-toontalk">What is ToonTalk?</h2>

<p>ToonTalk is a visual programming environment created by <a href="https://toontalk.com/English/kenkahn.htm">Ken Kahn</a> in 1995. The “Toon” stands for cartoon—every abstract programming concept is mapped to a concrete, animated metaphor:</p>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>ToonTalk Metaphor</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Variables</td>
      <td>Boxes with numbered holes</td>
    </tr>
    <tr>
      <td>Values</td>
      <td>Numbers, text, images in boxes</td>
    </tr>
    <tr>
      <td>Comparison</td>
      <td>Scales that tip when values differ</td>
    </tr>
    <tr>
      <td>Functions</td>
      <td>Robots that watch and learn</td>
    </tr>
    <tr>
      <td>Message passing</td>
      <td>Birds that carry items to nests</td>
    </tr>
    <tr>
      <td>Garbage collection</td>
      <td>Trucks that haul away unused items</td>
    </tr>
  </tbody>
</table>

<p>The design was influenced by games like <em>The Legend of Zelda</em> and <em>Robot Odyssey</em>—the kind of games that made you think while you played.</p>

<h2 id="programming-by-demonstration">Programming by Demonstration</h2>

<p>The core idea is radical: you don’t write code, you <em>show</em> a robot what to do.</p>

<ol>
  <li>Create a robot and put it in “training mode”</li>
  <li>Perform actions while the robot watches (move items, compare values, etc.)</li>
  <li>The robot records your actions as a program</li>
  <li>Give the robot a box matching the training pattern—it executes the learned behavior</li>
</ol>

<p>This is programming by demonstration. The robot generalizes from your example, matching patterns and applying transformations. It’s the same conceptual model as teaching a child: “Watch what I do, then you try.”</p>

<h2 id="three-generations">Three Generations</h2>

<p>ToonTalk has existed in three forms:</p>

<table>
  <thead>
    <tr>
      <th>Version</th>
      <th>Era</th>
      <th>Technology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Original ToonTalk</strong></td>
      <td>1995-2009</td>
      <td>C++, 3D desktop application</td>
    </tr>
    <tr>
      <td><strong>ToonTalk Reborn</strong></td>
      <td>2014-2017</td>
      <td>JavaScript/jQuery web app</td>
    </tr>
    <tr>
      <td><strong>tt-rs</strong></td>
      <td>2025-2026</td>
      <td>Rust/WebAssembly/Yew</td>
    </tr>
  </tbody>
</table>

<p>The original was a full 3D world—cities, houses, helicopters, even bombs for debugging. Ken Kahn later created <a href="https://github.com/ToonTalk/ToonTalk">ToonTalk Reborn</a>, a simplified JavaScript version that runs in browsers.</p>

<h2 id="why-i-built-tt-rs">Why I Built tt-rs</h2>

<p>When I rediscovered ToonTalk Reborn a few years ago, I wanted to experiment with the concepts myself. But diving into a large jQuery codebase wasn’t appealing. So I did what any reasonable person would do: I vibe coded my own version in Rust.</p>

<p><strong>tt-rs</strong> is a modern reimplementation using:</p>

<ul>
  <li><strong>Rust</strong> for core logic</li>
  <li><strong>WebAssembly</strong> for browser execution</li>
  <li><strong>Yew</strong> for reactive UI</li>
  <li><strong>SVG/CSS</strong> for graphics and animations</li>
</ul>

<p>It’s not a port—it’s a fresh implementation inspired by the same ideas. Building it myself lets me understand the concepts deeply and experiment with variations.</p>

<h2 id="three-learning-levels">Three Learning Levels</h2>

<p>The demo introduces concepts progressively through three levels:</p>

<table>
  <thead>
    <tr>
      <th>Level</th>
      <th>Concepts</th>
      <th>Widgets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>tt1</strong></td>
      <td>Basics</td>
      <td>Numbers, boxes, scales, wand, vacuum</td>
    </tr>
    <tr>
      <td><strong>tt2</strong></td>
      <td>Messaging</td>
      <td>Birds and nests for communication</td>
    </tr>
    <tr>
      <td><strong>tt3</strong></td>
      <td>Automation</td>
      <td>Sensors (time, random) + robots</td>
    </tr>
  </tbody>
</table>

<p>Level one covers the fundamentals: numbers with arithmetic, boxes as containers, scales for comparison, and tools for copying and removing. Level two adds asynchronous messaging—birds carry items to their paired nests. Level three brings sensors that produce values and robots that automate actions.</p>

<h2 id="current-features">Current Features</h2>

<p>The <a href="https://sw-fun.github.io/tt-rs/">live demo</a> includes:</p>

<p><strong>Widgets:</strong></p>
<ul>
  <li><strong>Numbers</strong>: Rational arithmetic with +, -, *, / operators</li>
  <li><strong>Boxes</strong>: Configurable containers with 0-9 holes (resize with keyboard)</li>
  <li><strong>Text</strong>: Basic text display</li>
  <li><strong>Scales</strong>: Visual comparison that tips when values differ</li>
  <li><strong>Robot</strong>: Training mode, action recording, execution</li>
  <li><strong>Bird/Nest</strong>: Message passing with pairing and delivery</li>
  <li><strong>Sensors</strong>: Time (milliseconds) and random number generation</li>
</ul>

<p><strong>Tools:</strong></p>
<ul>
  <li><strong>Wand</strong>: Copy any widget</li>
  <li><strong>Vacuum</strong>: Remove widgets</li>
  <li><strong>Magnifier</strong>: Inspect nest message queues and robot actions</li>
</ul>

<p><strong>Interactions:</strong></p>
<ul>
  <li>Drag-and-drop with visual feedback</li>
  <li>Box joining (drop box on edge of another)</li>
  <li>Box splitting (drop box on a number)</li>
  <li>Contextual help panel with level-specific content</li>
  <li>Puzzle system with animated “Show Me” demos</li>
</ul>

<h2 id="robot-training">Robot Training</h2>

<p>The core feature is programming by demonstration:</p>

<ol>
  <li><strong>Click robot</strong> to enter training mode (yellow glow indicates “I’m watching”)</li>
  <li><strong>Perform actions</strong> while the robot records (arithmetic, copy, remove, move to box)</li>
  <li><strong>Click robot</strong> again to stop training</li>
  <li><strong>Click robot</strong> to replay—it executes the recorded sequence</li>
</ol>

<p>The tutorials demonstrate this workflow step by step. In the “Train Robot” tutorial, you teach a robot to move a number into a box. In “Robot Sensors,” you train a robot to generate random numbers, apply modulo, and send results to a nest via a bird.</p>

<h2 id="interactive-tutorials">Interactive Tutorials</h2>

<p>Each tutorial has two parts:</p>

<ol>
  <li><strong>Show Me</strong>: Watch an animated demonstration where a cursor walks through the solution</li>
  <li><strong>Practice</strong>: Try it yourself with the same widgets</li>
</ol>

<p>The tutorials cover:</p>
<ul>
  <li>Fill a box with numbers</li>
  <li>Add numbers together</li>
  <li>Copy widgets with the wand</li>
  <li>Send messages with birds and nests</li>
  <li>Train your first robot</li>
  <li>Combine robots with sensors</li>
</ul>

<h2 id="whats-next">What’s Next</h2>

<p>The immediate priorities:</p>

<ol>
  <li><strong>Pattern matching</strong> - Robot generalizes from specific values to “any number”</li>
  <li><strong>Watched execution</strong> - See robot work step-by-step with animated cursor</li>
  <li><strong>Persistence</strong> - Save and load workspaces</li>
</ol>

<p>Long term, I’d like to add the 3D elements from the original—the cities, the houses, the helicopter view. But that’s a much larger project.</p>

<h2 id="the-enduring-appeal">The Enduring Appeal</h2>

<p>What makes ToonTalk fascinating isn’t just the visual metaphors—it’s the <em>computational model</em>. Under the hood, ToonTalk implements concurrent constraint logic programming. The robots are essentially guarded Horn clauses. The birds and nests implement the actor model.</p>

<p>Heavy concepts, but you don’t need to know any of that to use it. You just train robots by example. The abstraction is complete.</p>

<p>That’s why it stuck with me for twenty years. Good abstractions are rare. When you find one, it’s worth understanding deeply.</p>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>ToonTalk Website</strong></td>
        <td><a href="https://www.toontalk.com/">toontalk.com</a></td>
      </tr>
      <tr>
        <td><strong>ToonTalk on Wikipedia</strong></td>
        <td><a href="https://en.wikipedia.org/wiki/ToonTalk">Wikipedia</a></td>
      </tr>
      <tr>
        <td><strong>ToonTalk Reborn (JS)</strong></td>
        <td><a href="https://github.com/ToonTalk/ToonTalk">github.com/ToonTalk/ToonTalk</a></td>
      </tr>
      <tr>
        <td><strong>ToonTalk Reborn Demo</strong></td>
        <td><a href="https://toontalk.github.io/ToonTalk/">toontalk.github.io/ToonTalk</a></td>
      </tr>
      <tr>
        <td><strong>ToonTalk Reborn Wiki</strong></td>
        <td><a href="https://github.com/ToonTalk/ToonTalk/wiki">Wiki</a></td>
      </tr>
      <tr>
        <td><strong>Ken Kahn’s Page</strong></td>
        <td><a href="https://toontalk.com/English/kenkahn.htm">Ken Kahn</a></td>
      </tr>
      <tr>
        <td><strong>Original Paper (1995)</strong></td>
        <td><a href="https://eric.ed.gov/?id=ED392435">ERIC - ToonTalk: An Animated Programming Environment</a></td>
      </tr>
      <tr>
        <td><strong>Ken Kahn’s Research</strong></td>
        <td><a href="https://www.academia.edu/2795458/ToonTalk_and_Logo">Academia.edu</a></td>
      </tr>
    </tbody>
  </table>

</div>

<hr />

<p><em>Part 4 of the Throwback Thursday series. <a href="/series/#throwback-thursday">View all parts</a></em></p>

<p><em>Some ideas are worth rediscovering. ToonTalk is one of them.</em></p>]]></content><author><name>Software Wrighter</name></author><category term="tbt" /><category term="visual-programming" /><category term="vibe-coding" /><category term="tbt" /><category term="toontalk" /><category term="visual-programming" /><category term="rust" /><category term="webassembly" /><category term="education" /><category term="vibe-coding" /><summary type="html"><![CDATA[I first discovered ToonTalk during the Windows XP era—probably around 2003 or 2004. It was unlike anything I’d seen: a programming environment disguised as a video game where you trained robots by showing them what to do. The concept stuck with me for two decades. Resource Link Video ToonTalk in Rust tt-rs Demo Live Demo tt-rs Repo tt-rs What is ToonTalk? ToonTalk is a visual programming environment created by Ken Kahn in 1995. The “Toon” stands for cartoon—every abstract programming concept is mapped to a concrete, animated metaphor: Concept ToonTalk Metaphor Variables Boxes with numbered holes Values Numbers, text, images in boxes Comparison Scales that tip when values differ Functions Robots that watch and learn Message passing Birds that carry items to nests Garbage collection Trucks that haul away unused items The design was influenced by games like The Legend of Zelda and Robot Odyssey—the kind of games that made you think while you played. Programming by Demonstration The core idea is radical: you don’t write code, you show a robot what to do. Create a robot and put it in “training mode” Perform actions while the robot watches (move items, compare values, etc.) The robot records your actions as a program Give the robot a box matching the training pattern—it executes the learned behavior This is programming by demonstration. The robot generalizes from your example, matching patterns and applying transformations. It’s the same conceptual model as teaching a child: “Watch what I do, then you try.” Three Generations ToonTalk has existed in three forms: Version Era Technology Original ToonTalk 1995-2009 C++, 3D desktop application ToonTalk Reborn 2014-2017 JavaScript/jQuery web app tt-rs 2025-2026 Rust/WebAssembly/Yew The original was a full 3D world—cities, houses, helicopters, even bombs for debugging. Ken Kahn later created ToonTalk Reborn, a simplified JavaScript version that runs in browsers. Why I Built tt-rs When I rediscovered ToonTalk Reborn a few years ago, I wanted to experiment with the concepts myself. But diving into a large jQuery codebase wasn’t appealing. So I did what any reasonable person would do: I vibe coded my own version in Rust. tt-rs is a modern reimplementation using: Rust for core logic WebAssembly for browser execution Yew for reactive UI SVG/CSS for graphics and animations It’s not a port—it’s a fresh implementation inspired by the same ideas. Building it myself lets me understand the concepts deeply and experiment with variations. Three Learning Levels The demo introduces concepts progressively through three levels: Level Concepts Widgets tt1 Basics Numbers, boxes, scales, wand, vacuum tt2 Messaging Birds and nests for communication tt3 Automation Sensors (time, random) + robots Level one covers the fundamentals: numbers with arithmetic, boxes as containers, scales for comparison, and tools for copying and removing. Level two adds asynchronous messaging—birds carry items to their paired nests. Level three brings sensors that produce values and robots that automate actions. Current Features The live demo includes: Widgets: Numbers: Rational arithmetic with +, -, *, / operators Boxes: Configurable containers with 0-9 holes (resize with keyboard) Text: Basic text display Scales: Visual comparison that tips when values differ Robot: Training mode, action recording, execution Bird/Nest: Message passing with pairing and delivery Sensors: Time (milliseconds) and random number generation Tools: Wand: Copy any widget Vacuum: Remove widgets Magnifier: Inspect nest message queues and robot actions Interactions: Drag-and-drop with visual feedback Box joining (drop box on edge of another) Box splitting (drop box on a number) Contextual help panel with level-specific content Puzzle system with animated “Show Me” demos Robot Training The core feature is programming by demonstration: Click robot to enter training mode (yellow glow indicates “I’m watching”) Perform actions while the robot records (arithmetic, copy, remove, move to box) Click robot again to stop training Click robot to replay—it executes the recorded sequence The tutorials demonstrate this workflow step by step. In the “Train Robot” tutorial, you teach a robot to move a number into a box. In “Robot Sensors,” you train a robot to generate random numbers, apply modulo, and send results to a nest via a bird. Interactive Tutorials Each tutorial has two parts: Show Me: Watch an animated demonstration where a cursor walks through the solution Practice: Try it yourself with the same widgets The tutorials cover: Fill a box with numbers Add numbers together Copy widgets with the wand Send messages with birds and nests Train your first robot Combine robots with sensors What’s Next The immediate priorities: Pattern matching - Robot generalizes from specific values to “any number” Watched execution - See robot work step-by-step with animated cursor Persistence - Save and load workspaces Long term, I’d like to add the 3D elements from the original—the cities, the houses, the helicopter view. But that’s a much larger project. The Enduring Appeal What makes ToonTalk fascinating isn’t just the visual metaphors—it’s the computational model. Under the hood, ToonTalk implements concurrent constraint logic programming. The robots are essentially guarded Horn clauses. The birds and nests implement the actor model. Heavy concepts, but you don’t need to know any of that to use it. You just train robots by example. The abstraction is complete. That’s why it stuck with me for twenty years. Good abstractions are rare. When you find one, it’s worth understanding deeply. References Resource Link ToonTalk Website toontalk.com ToonTalk on Wikipedia Wikipedia ToonTalk Reborn (JS) github.com/ToonTalk/ToonTalk ToonTalk Reborn Demo toontalk.github.io/ToonTalk ToonTalk Reborn Wiki Wiki Ken Kahn’s Page Ken Kahn Original Paper (1995) ERIC - ToonTalk: An Animated Programming Environment Ken Kahn’s Research Academia.edu Part 4 of the Throwback Thursday series. View all parts Some ideas are worth rediscovering. ToonTalk is one of them.]]></summary></entry><entry><title type="html">Five ML Concepts - #16</title><link href="https://software-wrighter-lab.github.io/2026/02/19/five-ml-concepts-16/" rel="alternate" type="text/html" title="Five ML Concepts - #16" /><published>2026-02-19T00:00:00-08:00</published><updated>2026-02-19T00:00:00-08:00</updated><id>https://software-wrighter-lab.github.io/2026/02/19/five-ml-concepts-16</id><content type="html" xml:base="https://software-wrighter-lab.github.io/2026/02/19/five-ml-concepts-16/"><![CDATA[<p><img src="/assets/images/posts/block-sixteen.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/HdFa9C3ahkw">Five ML Concepts #16</a><br /><a href="https://www.youtube.com/shorts/HdFa9C3ahkw"><img src="https://img.youtube.com/vi/HdFa9C3ahkw/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Train/Val/Test Split</strong></td>
        <td><a href="https://www.deeplearningbook.org/">Deep Learning</a> (Goodfellow et al. 2016), Chapter 5</td>
      </tr>
      <tr>
        <td><strong>Overconfidence</strong></td>
        <td><a href="https://arxiv.org/abs/1706.04599">On Calibration of Modern Neural Networks</a> (Guo et al. 2017)</td>
      </tr>
      <tr>
        <td><strong>Batch Normalization</strong></td>
        <td><a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training</a> (Ioffe &amp; Szegedy 2015)</td>
      </tr>
      <tr>
        <td><strong>Optimization vs Generalization</strong></td>
        <td><a href="https://arxiv.org/abs/1611.03530">Understanding Deep Learning Requires Rethinking Generalization</a> (Zhang et al. 2017)</td>
      </tr>
      <tr>
        <td><strong>A/B Testing</strong></td>
        <td><a href="https://www.exp-platform.com/Documents/GusijDMKD.pdf">Controlled Experiments on the Web</a> (Kohavi et al. 2009)</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-train--validation--test-split">1. Train / Validation / Test Split</h3>

<p><strong>Data is divided into training, validation, and test sets.</strong> Training learns patterns, validation tunes hyperparameters, test evaluates final performance.</p>

<p>Never use test data for any decisions during development—it should only be touched once.</p>

<blockquote>
  <p>Like practicing on homework, checking with practice tests, then taking the real exam.</p>
</blockquote>

<h3 id="2-overconfidence">2. Overconfidence</h3>

<p><strong>Models can assign very high probabilities to incorrect predictions.</strong> This is often related to poor calibration and can be dangerous in high-stakes applications.</p>

<p>Temperature scaling and other calibration methods can help align confidence with accuracy.</p>

<blockquote>
  <p>Like a student who is absolutely certain of a wrong answer.</p>
</blockquote>

<h3 id="3-batch-normalization">3. Batch Normalization</h3>

<p><strong>Normalizes layer activations during training to improve stability and convergence.</strong> Each mini-batch’s activations are normalized to have zero mean and unit variance.</p>

<p>This reduces internal covariate shift and often allows higher learning rates.</p>

<blockquote>
  <p>Like keeping everyone on a similar pace during training so no one runs too far ahead.</p>
</blockquote>

<h3 id="4-optimization-vs-generalization">4. Optimization vs Generalization</h3>

<p><strong>Training loss can decrease while test performance does not improve.</strong> Good optimization does not guarantee good generalization.</p>

<p>A model can perfectly fit training data while failing on new examples—this is overfitting.</p>

<blockquote>
  <p>Like memorizing last year’s exam instead of understanding the subject.</p>
</blockquote>

<h3 id="5-ab-testing-models">5. A/B Testing Models</h3>

<p><strong>Comparing two model versions using controlled live traffic experiments.</strong> Users are randomly assigned to see predictions from model A or model B.</p>

<p>Statistical analysis determines which model performs better on real-world metrics.</p>

<blockquote>
  <p>Like taste-testing two recipes with real customers to see which works better.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Train/Val/Test</strong></td>
      <td>Separate data for learning, tuning, and evaluation</td>
    </tr>
    <tr>
      <td><strong>Overconfidence</strong></td>
      <td>High probability on wrong predictions</td>
    </tr>
    <tr>
      <td><strong>Batch Normalization</strong></td>
      <td>Normalize activations for stable training</td>
    </tr>
    <tr>
      <td><strong>Optimization vs Generalization</strong></td>
      <td>Low train loss ≠ good test performance</td>
    </tr>
    <tr>
      <td><strong>A/B Testing</strong></td>
      <td>Compare models with live experiments</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 16 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/20/five-ml-concepts-17/">Next: #17 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>]]></content><author><name>Software Wrighter</name></author><category term="llm" /><category term="machine-learning" /><category term="explainers" /><category term="five-ml-concepts" /><category term="train-val-test" /><category term="overconfidence" /><category term="batch-normalization" /><category term="generalization" /><category term="ab-testing" /><category term="ml-concepts" /><summary type="html"><![CDATA[5 machine learning concepts. Under 30 seconds each. Resource Link Papers Links in References section Video Five ML Concepts #16 References Concept Reference Train/Val/Test Split Deep Learning (Goodfellow et al. 2016), Chapter 5 Overconfidence On Calibration of Modern Neural Networks (Guo et al. 2017) Batch Normalization Batch Normalization: Accelerating Deep Network Training (Ioffe &amp; Szegedy 2015) Optimization vs Generalization Understanding Deep Learning Requires Rethinking Generalization (Zhang et al. 2017) A/B Testing Controlled Experiments on the Web (Kohavi et al. 2009) Today’s Five 1. Train / Validation / Test Split Data is divided into training, validation, and test sets. Training learns patterns, validation tunes hyperparameters, test evaluates final performance. Never use test data for any decisions during development—it should only be touched once. Like practicing on homework, checking with practice tests, then taking the real exam. 2. Overconfidence Models can assign very high probabilities to incorrect predictions. This is often related to poor calibration and can be dangerous in high-stakes applications. Temperature scaling and other calibration methods can help align confidence with accuracy. Like a student who is absolutely certain of a wrong answer. 3. Batch Normalization Normalizes layer activations during training to improve stability and convergence. Each mini-batch’s activations are normalized to have zero mean and unit variance. This reduces internal covariate shift and often allows higher learning rates. Like keeping everyone on a similar pace during training so no one runs too far ahead. 4. Optimization vs Generalization Training loss can decrease while test performance does not improve. Good optimization does not guarantee good generalization. A model can perfectly fit training data while failing on new examples—this is overfitting. Like memorizing last year’s exam instead of understanding the subject. 5. A/B Testing Models Comparing two model versions using controlled live traffic experiments. Users are randomly assigned to see predictions from model A or model B. Statistical analysis determines which model performs better on real-world metrics. Like taste-testing two recipes with real customers to see which works better. Quick Reference Concept One-liner Train/Val/Test Separate data for learning, tuning, and evaluation Overconfidence High probability on wrong predictions Batch Normalization Normalize activations for stable training Optimization vs Generalization Low train loss ≠ good test performance A/B Testing Compare models with live experiments *Part 16 of the Five ML Concepts series. View all parts Next: #17 →* Short, accurate ML explainers. Follow for more.]]></summary></entry></feed>