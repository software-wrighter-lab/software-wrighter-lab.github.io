<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Software Wrighter Lab Blog | AI coding agents, systems programming, and practical machine learning</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Software Wrighter Lab Blog" />
<meta name="author" content="Mike Wright" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="AI coding agents, systems programming, and practical machine learning" />
<meta property="og:description" content="AI coding agents, systems programming, and practical machine learning" />
<link rel="canonical" href="https://software-wrighter-lab.github.io/" />
<meta property="og:url" content="https://software-wrighter-lab.github.io/" />
<meta property="og:site_name" content="Software Wrighter Lab Blog" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Software Wrighter Lab Blog" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"Mike Wright"},"description":"AI coding agents, systems programming, and practical machine learning","headline":"Software Wrighter Lab Blog","name":"Software Wrighter Lab Blog","url":"https://software-wrighter-lab.github.io/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://software-wrighter-lab.github.io/feed.xml" title="Software Wrighter Lab Blog" /><!-- Theme toggle script - load early to prevent flash -->
  <script>
    (function() {
      var stored = localStorage.getItem('sw-lab-theme');
      var theme = stored;
      if (!theme) {
        theme = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
      }
      document.documentElement.setAttribute('data-theme', theme);
    })();
  </script>
  <!-- Font size - load early to prevent flash -->
  <script>
    (function() {
      var NEW_DEFAULT = 110;
      var OLD_DEFAULT = 150;
      var stored = localStorage.getItem('sw-lab-font-size');
      var ack = localStorage.getItem('sw-lab-prefs-ack');
      var size = stored ? parseInt(stored, 10) : null;

      // Migration: if on old default and not acknowledged, use new default
      if (size === OLD_DEFAULT && ack !== 'true') {
        size = NEW_DEFAULT;
      } else if (size === null) {
        size = NEW_DEFAULT;
      }

      // Apply to post content when DOM is ready
      document.addEventListener('DOMContentLoaded', function() {
        var pc = document.querySelector('.post-content');
        if (pc) pc.style.fontSize = size + '%';
        var pl = document.querySelector('.post-list');
        if (pl) pl.style.fontSize = size + '%';
      });
    })();
  </script>
  <script src="/assets/js/theme-toggle.js" defer></script>
  <script src="/assets/js/font-size.js" defer></script>
  <script src="/assets/js/preferences.js" defer></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">
      <img src="/assets/images/site/logo.jpg" alt="Software Wrighter Lab Blog" class="site-logo">
      <span class="site-title-text">Software Wrighter Lab Blog</span>
    </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/abstracts/">Abstracts</a><a class="page-link" href="/index-all/">Index</a><a class="page-link" href="/series/">Series</a><a class="page-link" href="/tags/">Tags</a><a class="page-link" href="/categories/">Categories</a><a class="page-link" href="/search/">Search</a><div class="font-size-controls">
  <button class="font-size-btn" id="font-decrease" title="Decrease font size" aria-label="Decrease font size">A-</button>
  <button class="font-size-btn" id="font-reset" title="Reset font size" aria-label="Reset font size">A</button>
  <button class="font-size-btn" id="font-increase" title="Increase font size" aria-label="Increase font size">A+</button>
</div>
<button class="theme-toggle" aria-label="Switch theme" title="Switch theme">
  <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
  </svg>
  <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
  </svg>
</button>
</div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <style>
.pagination-controls {
  display: flex;
  align-items: center;
  justify-content: space-between;
  flex-wrap: wrap;
  gap: 1em;
  margin: 0.5em 0 1em 0;
  padding: 0.75em 1em;
  background: #f5f5f5;
  border-radius: 8px;
}

#pagination-top {
  margin-top: 0;
}

[data-theme="dark"] .pagination-controls {
  background: #2a2a2a;
}

.pagination-controls label {
  font-size: 0.9em;
  margin-right: 0.5em;
}

.pagination-controls select {
  padding: 0.4em 0.8em;
  border-radius: 4px;
  border: 1px solid #ccc;
  background: #fff;
  font-size: 0.9em;
}

[data-theme="dark"] .pagination-controls select {
  background: #333;
  border-color: #555;
  color: #eee;
}

.page-numbers {
  display: flex;
  align-items: center;
  gap: 0.25em;
  flex-wrap: wrap;
}

.page-numbers button {
  padding: 0.4em 0.8em;
  border: 1px solid #ccc;
  background: #fff;
  border-radius: 4px;
  cursor: pointer;
  font-size: 0.9em;
  min-width: 2.5em;
}

.page-numbers button:hover:not(:disabled) {
  background: #e8e8e8;
}

.page-numbers button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.page-numbers button.active {
  background: #0066cc;
  color: #fff;
  border-color: #0066cc;
}

[data-theme="dark"] .page-numbers button {
  background: #333;
  border-color: #555;
  color: #eee;
}

[data-theme="dark"] .page-numbers button:hover:not(:disabled) {
  background: #444;
}

[data-theme="dark"] .page-numbers button.active {
  background: #0066cc;
  border-color: #0066cc;
}

.page-info {
  font-size: 0.85em;
  color: #666;
}

[data-theme="dark"] .page-info {
  color: #999;
}

/* First 10 posts visible immediately (no JS wait) */
.post-item {
  display: none;
}

.post-item:nth-child(-n+10) {
  display: block;
}

/* Once JS loads, it controls visibility */
.js-pagination .post-item {
  display: none;
}

.js-pagination .post-item.visible {
  display: block;
}

.archive-notice {
  margin-top: 2em;
  padding: 1em;
  background: #fff8e1;
  border-radius: 8px;
  font-size: 0.9em;
}

[data-theme="dark"] .archive-notice {
  background: #3a3500;
}
</style>

<div class="home"><div class="pagination-controls" id="pagination-top">
      <div>
        <label for="per-page-select">Posts per page:</label>
        <select id="per-page-select">
          <option value="5">5</option>
          <option value="10" selected>10</option>
          <option value="25">25</option>
          <option value="50">50</option>
          <option value="100">100</option>
        </select>
      </div>
      <div class="page-numbers" id="page-numbers-top"></div>
      <div class="page-info" id="page-info-top"></div>
    </div>

    <ul class="post-list" id="post-list"><li class="post-item" data-index="0"><p class="post-meta">February 23, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">1271 words</span> &bull; <span class="post-read-time">7 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Personal Software grows. midi-cli-rs now supports custom mood packs---TOML files that extend built-in moods with your own musical variations. No Rust required. Define tempo, key, intensity, and let the generators handle the rest.</div><h3>
          <a class="post-link" href="/2026/02/23/midi-cli-rs-extending-with-custom-mood-packs/">
            midi-cli-rs: Extending with Custom Mood Packs
          </a>
        </h3><nav class="toc" data-toc-id="midi-cli-rs-extending-with-custom-mood-packs">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-galaxy.png" class="post-marker" alt="" /></p>

<p>Personal Software doesn’t stop at “it works.” It evolves. After building midi-cli-rs for AI agents to generate music, I wanted more moods—without recompiling Rust every time.</p>

<p>The solution: a plugin system that lets anyone create custom mood packs using simple TOML files.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Examples</strong></td>
        <td><a href="https://softwarewrighter.github.io/midi-cli-rs/">Listen to Samples</a></td>
      </tr>
      <tr>
        <td><strong>Wiki</strong></td>
        <td><a href="https://github.com/softwarewrighter/midi-cli-rs/wiki/Plugins-and-Extensibility">Plugin Documentation</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://youtu.be/nDNcbKE8KtE">midi-cli-rs Explainer</a><br /><a href="https://youtu.be/nDNcbKE8KtE"><img src="https://img.youtube.com/vi/nDNcbKE8KtE/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/midi-cli-rs">midi-cli-rs</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-problem-with-built-in-only">The Problem with Built-in Only</h2>

<p>The original midi-cli-rs shipped with a handful of mood presets: suspense, eerie, upbeat, calm, ambient, jazz. Useful, but limited. What if you want synthwave? Chillout? Something faster or in a different key?</p>

<p>Hardcoding every possible mood isn’t practical. And asking users to modify Rust source code isn’t friendly.</p>

<h2 id="three-levels-of-extensibility">Three Levels of Extensibility</h2>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th>Level</th>
      <th>What You Get</th>
      <th>What You Change</th>
      <th>Skill Required</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><span style="color:green">✓</span></td>
      <td><strong>Built-in Moods</strong></td>
      <td>9 curated generators</td>
      <td>Nothing—use as-is</td>
      <td>None</td>
    </tr>
    <tr>
      <td style="text-align: center"><span style="color:green">✓</span></td>
      <td><strong>Plugin Moods</strong></td>
      <td>Parameter variations</td>
      <td>TOML config files</td>
      <td>Text editing</td>
    </tr>
    <tr>
      <td style="text-align: center"><span style="color:red">✗</span></td>
      <td><strong>Custom Generators</strong></td>
      <td>New musical patterns</td>
      <td>Rust source code</td>
      <td>Programming (future)</td>
    </tr>
  </tbody>
</table>

<p>This post covers <strong>Plugin Moods</strong>—the middle tier. You can preset combinations of tempo, key, and intensity, but you’re still using the built-in generators’ musical logic. Want a “smooth-jazz” preset (slower, mellower)? Plugin mood. Want bebop or Latin jazz with different chord progressions? That requires a custom generator.</p>

<p>Custom generators (writing new Rust code) will be covered in a future post when the plugin editor ships.</p>

<h2 id="the-plugin-architecture">The Plugin Architecture</h2>

<p>Custom moods live in <code class="language-plaintext highlighter-rouge">~/.midi-cli-rs/moods/</code> as TOML files. Each file is a “mood pack” that can define multiple moods. The CLI discovers them automatically.</p>

<p>Here’s how it works:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>~/.midi-cli-rs/
└── moods/
    ├── electronic.toml    # Your synthwave, techno, etc.
    ├── cinematic.toml     # Epic, tension, wonder
    └── seasonal.toml      # Holiday themes
</code></pre></div></div>

<h2 id="creating-a-mood-pack">Creating a Mood Pack</h2>

<p>A mood pack has two parts: pack metadata and mood definitions.</p>

<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">[</span><span class="n">pack</span><span class="k">]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"electronic"</span>
<span class="n">version</span> <span class="o">=</span><span class="w"> </span><span class="s">"1.0.0"</span>
<span class="n">author</span> <span class="o">=</span><span class="w"> </span><span class="s">"Your Name"</span>
<span class="n">description</span> <span class="o">=</span><span class="w"> </span><span class="s">"Electronic music styles"</span>

<span class="k">[[</span><span class="n">moods</span><span class="k">]]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"synthwave"</span>
<span class="n">base_mood</span> <span class="o">=</span><span class="w"> </span><span class="s">"upbeat"</span>
<span class="n">default_tempo</span> <span class="o">=</span><span class="w"> </span><span class="mi">118</span>
<span class="n">default_key</span> <span class="o">=</span><span class="w"> </span><span class="s">"Am"</span>
<span class="n">default_intensity</span> <span class="o">=</span><span class="w"> </span><span class="mi">65</span>
<span class="n">description</span> <span class="o">=</span><span class="w"> </span><span class="s">"80s synthwave vibes"</span>
<span class="n">tags</span> <span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="s">"electronic"</span><span class="p">,</span> <span class="s">"retro"</span><span class="p">]</span>

<span class="k">[[</span><span class="n">moods</span><span class="k">]]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"chillout"</span>
<span class="n">base_mood</span> <span class="o">=</span><span class="w"> </span><span class="s">"ambient"</span>
<span class="n">default_tempo</span> <span class="o">=</span><span class="w"> </span><span class="mi">85</span>
<span class="n">default_key</span> <span class="o">=</span><span class="w"> </span><span class="s">"Em"</span>
<span class="n">default_intensity</span> <span class="o">=</span><span class="w"> </span><span class="mi">40</span>
<span class="n">description</span> <span class="o">=</span><span class="w"> </span><span class="s">"Relaxed electronic"</span>
</code></pre></div></div>

<p>Each mood <strong>delegates</strong> to a built-in generator (<code class="language-plaintext highlighter-rouge">base_mood</code>) but overrides specific parameters. You get the musical logic of the built-in mood with your customizations applied.</p>

<h2 id="available-base-moods">Available Base Moods</h2>

<p>Your custom moods can extend any of the nine built-in generators:</p>

<table>
  <thead>
    <tr>
      <th>Base Mood</th>
      <th>Character</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">suspense</code></td>
      <td>Tense, building</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">eerie</code></td>
      <td>Dark, unsettling</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">upbeat</code></td>
      <td>Energetic, positive</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">calm</code></td>
      <td>Peaceful, slow</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">ambient</code></td>
      <td>Atmospheric, textural</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">jazz</code></td>
      <td>Swing, improvisation</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">chiptune</code></td>
      <td>8-bit, retro gaming</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">orchestral</code></td>
      <td>Classical instruments</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">show</code></td>
      <td>Broadway, theatrical</td>
    </tr>
  </tbody>
</table>

<h2 id="configuration-options">Configuration Options</h2>

<p>Each mood definition supports these overrides:</p>

<table>
  <thead>
    <tr>
      <th>Field</th>
      <th>Description</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">name</code></td>
      <td>CLI name (required)</td>
      <td><code class="language-plaintext highlighter-rouge">"synthwave"</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">base_mood</code></td>
      <td>Built-in to extend (required)</td>
      <td><code class="language-plaintext highlighter-rouge">"upbeat"</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">default_tempo</code></td>
      <td>BPM</td>
      <td><code class="language-plaintext highlighter-rouge">118</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">default_key</code></td>
      <td>Musical key</td>
      <td><code class="language-plaintext highlighter-rouge">"Am"</code>, <code class="language-plaintext highlighter-rouge">"C"</code>, <code class="language-plaintext highlighter-rouge">"Eb"</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">default_intensity</code></td>
      <td>0-100 energy level</td>
      <td><code class="language-plaintext highlighter-rouge">65</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">description</code></td>
      <td>Human-readable description</td>
      <td><code class="language-plaintext highlighter-rouge">"80s vibes"</code></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">tags</code></td>
      <td>Discovery tags</td>
      <td><code class="language-plaintext highlighter-rouge">["electronic", "retro"]</code></td>
    </tr>
  </tbody>
</table>

<h2 id="how-seeds-create-variation">How Seeds Create Variation</h2>

<p>Seeds aren’t random—they’re deterministic variation selectors. The same mood + same seed always produces identical output. But different seeds create observable musical differences across multiple dimensions:</p>

<table>
  <thead>
    <tr>
      <th>Parameter</th>
      <th>Variation Range</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Tempo</td>
      <td>±15% from base</td>
    </tr>
    <tr>
      <td>Layer inclusion</td>
      <td>Which instruments appear</td>
    </tr>
    <tr>
      <td>Melodic contour</td>
      <td>16 different phrase shapes</td>
    </tr>
    <tr>
      <td>Note density</td>
      <td>0.6x to 1.4x</td>
    </tr>
    <tr>
      <td>Rest probability</td>
      <td>0% to 35% silence</td>
    </tr>
    <tr>
      <td>Phrase length</td>
      <td>3-8 notes</td>
    </tr>
    <tr>
      <td>Velocity</td>
      <td>-15 to +15 offset</td>
    </tr>
  </tbody>
</table>

<p>The system uses hash-based mixing with unique salts for each parameter. This means adjacent seeds (42 vs 43) produce completely different outputs—no gradual transitions between seeds.</p>

<p>When you combine plugin moods with seed variation, you get a matrix: your custom tempo/key/intensity settings applied across different seed-driven variations of the underlying generator’s patterns.</p>

<h2 id="using-custom-moods">Using Custom Moods</h2>

<p>Once your TOML file is in place, the mood appears automatically:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># List all moods (built-in + plugins)</span>
midi-cli-rs moods

<span class="c"># Generate with your custom mood</span>
midi-cli-rs preset <span class="nt">-m</span> synthwave <span class="nt">-d</span> 5 <span class="nt">-s</span> 42 <span class="nt">-o</span> output.wav
</code></pre></div></div>

<p>The seed system still works—same mood + same seed = identical output.</p>

<h2 id="example-electronic-pack">Example: Electronic Pack</h2>

<p>Here’s a complete pack with four electronic moods:</p>

<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">[</span><span class="n">pack</span><span class="k">]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"electronic"</span>
<span class="n">version</span> <span class="o">=</span><span class="w"> </span><span class="s">"1.0.0"</span>
<span class="n">description</span> <span class="o">=</span><span class="w"> </span><span class="s">"Electronic music styles"</span>

<span class="k">[[</span><span class="n">moods</span><span class="k">]]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"synthwave"</span>
<span class="n">base_mood</span> <span class="o">=</span><span class="w"> </span><span class="s">"upbeat"</span>
<span class="n">default_tempo</span> <span class="o">=</span><span class="w"> </span><span class="mi">118</span>
<span class="n">default_key</span> <span class="o">=</span><span class="w"> </span><span class="s">"Am"</span>
<span class="n">default_intensity</span> <span class="o">=</span><span class="w"> </span><span class="mi">65</span>

<span class="k">[[</span><span class="n">moods</span><span class="k">]]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"chillout"</span>
<span class="n">base_mood</span> <span class="o">=</span><span class="w"> </span><span class="s">"ambient"</span>
<span class="n">default_tempo</span> <span class="o">=</span><span class="w"> </span><span class="mi">85</span>
<span class="n">default_key</span> <span class="o">=</span><span class="w"> </span><span class="s">"Em"</span>
<span class="n">default_intensity</span> <span class="o">=</span><span class="w"> </span><span class="mi">40</span>

<span class="k">[[</span><span class="n">moods</span><span class="k">]]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"techno"</span>
<span class="n">base_mood</span> <span class="o">=</span><span class="w"> </span><span class="s">"upbeat"</span>
<span class="n">default_tempo</span> <span class="o">=</span><span class="w"> </span><span class="mi">130</span>
<span class="n">default_key</span> <span class="o">=</span><span class="w"> </span><span class="s">"Dm"</span>
<span class="n">default_intensity</span> <span class="o">=</span><span class="w"> </span><span class="mi">85</span>

<span class="k">[[</span><span class="n">moods</span><span class="k">]]</span>
<span class="n">name</span> <span class="o">=</span><span class="w"> </span><span class="s">"8bit"</span>
<span class="n">base_mood</span> <span class="o">=</span><span class="w"> </span><span class="s">"chiptune"</span>
<span class="n">default_tempo</span> <span class="o">=</span><span class="w"> </span><span class="mi">140</span>
<span class="n">default_key</span> <span class="o">=</span><span class="w"> </span><span class="s">"C"</span>
<span class="n">default_intensity</span> <span class="o">=</span><span class="w"> </span><span class="mi">70</span>
</code></pre></div></div>

<p>Drop this in <code class="language-plaintext highlighter-rouge">~/.midi-cli-rs/moods/electronic.toml</code> and you have four new moods.</p>

<h2 id="whats-next">What’s Next</h2>

<p>This plugin system handles <strong>mood variations</strong>—different tempos, keys, and intensities applied to existing generators. A future update will add a <strong>plugin editor</strong> for creating entirely new musical patterns without writing Rust.</p>

<p>For now, the delegation model covers most use cases: want faster jazz? Darker ambient? Major-key suspense? Create a TOML file and you’re done.</p>

<h2 id="the-personal-software-pattern">The Personal Software Pattern</h2>

<p>This follows the Personal Software philosophy: start with something that works, then extend it as needs emerge. The plugin system wasn’t in the original design. It grew from actual use—wanting more moods without recompiling.</p>

<p>Good personal software leaves room to grow.</p>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 3 of the Personal Software series. <a href="/series/#personal-software">View all parts</a></td>
      <td><a href="/2026/02/20/midi-cli-rs-music-for-ai-agents/">← Previous: midi-cli-rs</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Personal software that evolves with your needs.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-nDNcbKE8KtE">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-nDNcbKE8KtE"
      src="https://www.youtube.com/embed/nDNcbKE8KtE?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-nDNcbKE8KtE';
  const playerId = 'yt-player-nDNcbKE8KtE';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="1"><p class="post-meta">February 23, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">468 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: VAEs (generative with structured latents), Uncertainty Estimation (know when you don't know), Interpretability (distributed representations resist explanation), Gradient Noise (mini-batch variation), Human-in-the-Loop (human oversight for critical decisions).</div><h3>
          <a class="post-link" href="/2026/02/23/five-ml-concepts-20/">
            Five ML Concepts - #20
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-20">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-twenty.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/OklW3RTV3I4">Five ML Concepts #20</a><br /><a href="https://www.youtube.com/shorts/OklW3RTV3I4"><img src="https://img.youtube.com/vi/OklW3RTV3I4/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>VAEs</strong></td>
        <td><a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a> (Kingma &amp; Welling 2013)</td>
      </tr>
      <tr>
        <td><strong>Uncertainty Estimation</strong></td>
        <td><a href="https://arxiv.org/abs/1703.04977">What Uncertainties Do We Need in Bayesian Deep Learning?</a> (Kendall &amp; Gal 2017)</td>
      </tr>
      <tr>
        <td><strong>Interpretability</strong></td>
        <td><a href="https://arxiv.org/abs/1702.08608">Towards A Rigorous Science of Interpretable Machine Learning</a> (Doshi-Velez &amp; Kim 2017)</td>
      </tr>
      <tr>
        <td><strong>Gradient Noise</strong></td>
        <td><a href="https://arxiv.org/abs/1704.04289">Stochastic Gradient Descent as Approximate Bayesian Inference</a> (Mandt et al. 2017)</td>
      </tr>
      <tr>
        <td><strong>Human-in-the-Loop</strong></td>
        <td><a href="https://www.manning.com/books/human-in-the-loop-machine-learning">Human-in-the-Loop Machine Learning</a> (Monarch 2021)</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-variational-autoencoders-vaes">1. Variational Autoencoders (VAEs)</h3>

<p><strong>VAEs are probabilistic autoencoders that learn a structured latent distribution.</strong> By sampling from that distribution, they can generate new examples similar to the training data.</p>

<p>The key innovation is regularizing the latent space to be smooth and continuous.</p>

<blockquote>
  <p>Like learning not just to summarize books, but to create new ones in a similar style.</p>
</blockquote>

<h3 id="2-uncertainty-estimation">2. Uncertainty Estimation</h3>

<p><strong>Models can estimate how confident they should be in predictions.</strong> Some uncertainty comes from noisy data (aleatoric), and some from limited knowledge (epistemic).</p>

<p>Knowing when a model is uncertain enables safer decision-making.</p>

<blockquote>
  <p>Like a weather forecast giving seventy percent chance of rain instead of a simple yes or no.</p>
</blockquote>

<h3 id="3-why-interpretability-is-hard">3. Why Interpretability Is Hard</h3>

<p><strong>Neural networks represent information across many interacting parameters.</strong> No single component cleanly maps to a human concept.</p>

<p>Distributed representations enable powerful learning but resist simple explanations.</p>

<blockquote>
  <p>Like trying to explain a dream by pointing to individual neurons.</p>
</blockquote>

<h3 id="4-gradient-noise">4. Gradient Noise</h3>

<p><strong>When training with mini-batches, gradients vary from step to step.</strong> A little noise can help exploration, but too much can slow convergence.</p>

<p>Batch size, learning rate, and gradient clipping all influence this noise level.</p>

<blockquote>
  <p>Like getting slightly different directions each time you ask for help.</p>
</blockquote>

<h3 id="5-human-in-the-loop-systems">5. Human-in-the-Loop Systems</h3>

<p><strong>Humans review, supervise, or override model decisions in critical workflows.</strong> This improves safety and accountability in high-stakes applications.</p>

<p>The approach combines model efficiency with human judgment where it matters most.</p>

<blockquote>
  <p>Like a pilot monitoring autopilot and stepping in when necessary.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>VAEs</strong></td>
      <td>Generative models with structured latent spaces</td>
    </tr>
    <tr>
      <td><strong>Uncertainty Estimation</strong></td>
      <td>Know when you don’t know</td>
    </tr>
    <tr>
      <td><strong>Interpretability</strong></td>
      <td>Distributed representations resist explanation</td>
    </tr>
    <tr>
      <td><strong>Gradient Noise</strong></td>
      <td>Mini-batch variation in training</td>
    </tr>
    <tr>
      <td><strong>Human-in-the-Loop</strong></td>
      <td>Human oversight for critical decisions</td>
    </tr>
  </tbody>
</table>

<hr />

<p><em>Part 20 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></em></p>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-OklW3RTV3I4">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-OklW3RTV3I4"
      src="https://www.youtube.com/embed/OklW3RTV3I4?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-OklW3RTV3I4';
  const playerId = 'yt-player-OklW3RTV3I4';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="2"><p class="post-meta">February 22, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">643 words</span> &bull; <span class="post-read-time">4 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">ICL evolved from emergent surprise (2020) to mechanistic understanding (2022) to engineered capability (2026). Transformers implement implicit gradient descent during inference---they learn without weight updates. The frontier: models learning from their own feedback. Not magic. Meta-learning in plain sight.</div><h3>
          <a class="post-link" href="/2026/02/22/icl-revisited-from-mystery-to-engineering/">
            In-Context Learning Revisited: From Mystery to Engineering
          </a>
        </h3><nav class="toc" data-toc-id="icl-revisited-from-mystery-to-engineering">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-framework.png" class="post-marker" alt="" /></p>

<p>It was 2020 when GPT-3 shocked everyone. It could learn from examples in the query—without updating its weights. We called it In-Context Learning. But was it magic, or was it doing something deeper?</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/zWKmRxChRlA">ICL Revisited</a><br /><a href="https://www.youtube.com/shorts/zWKmRxChRlA"><img src="https://img.youtube.com/vi/zWKmRxChRlA/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>Papers</strong></td>
        <td><a href="#references">4 References</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="phase-1-the-empirical-discovery-2020">Phase 1: The Empirical Discovery (2020)</h2>

<p>The GPT-3 paper showed that large models could perform few-shot learning. Give them examples, and they generalize. No gradient updates. No retraining. Just forward passes.</p>

<p>The surprising part was that <strong>scaling alone</strong> seemed to unlock it.</p>

<h3 id="paper-language-models-are-few-shot-learners">Paper: Language Models are Few-Shot Learners</h3>

<p><strong>ELI5:</strong> Show a big language model a few examples of a task in your prompt, and it figures out how to do the task—without any retraining. Nobody told it to do this. It just emerged when models got big enough.</p>

<p><strong>Main idea:</strong> Scale unlocks emergent capabilities. ICL was discovered, not designed.</p>

<h2 id="phase-2-mechanistic-explanations-2022">Phase 2: Mechanistic Explanations (2022)</h2>

<p>By 2022, researchers began probing the internal mechanisms. Several papers proposed that transformers implement <strong>implicit meta-learning</strong>. The model appears to learn during inference by performing gradient-descent-like operations internally.</p>

<h3 id="paper-what-explains-in-context-learning-in-transformers">Paper: What Explains In-Context Learning in Transformers?</h3>

<p><strong>ELI5:</strong> When you give a transformer examples, its attention layers do something that looks like fitting a simple linear model to those examples—on the fly, during the forward pass. It’s not memorizing; it’s computing a mini-solution.</p>

<p><strong>Main idea:</strong> ICL works because attention can simulate linear regression internally.</p>

<h3 id="paper-transformers-learn-in-context-by-gradient-descent">Paper: Transformers Learn In-Context by Gradient Descent</h3>

<p><strong>ELI5:</strong> The transformer’s forward pass is secretly doing something similar to training. The attention mechanism acts like one step of gradient descent over the examples you provided. Learning happens inside inference.</p>

<p><strong>Main idea:</strong> ICL is implicit gradient descent—learning hidden inside prediction.</p>

<h2 id="phase-3-engineering-the-effect">Phase 3: Engineering the Effect</h2>

<p>Once researchers understood that ordering and structure affect ICL, prompt design became less of an art and more of an optimization problem. The quality and arrangement of demonstrations directly shape performance.</p>

<p>ICL became tunable. Researchers could now deliberately improve it rather than just observe it.</p>

<h2 id="phase-4-interactive-icl-2026">Phase 4: Interactive ICL (2026)</h2>

<p>Recent work pushes this further. Models are trained to predict natural language critiques and feedback. If a model can predict what a teacher would say, it can internalize that signal. External correction becomes an internal capability.</p>

<h3 id="paper-improving-interactive-in-context-learning-from-natural-language-feedback">Paper: Improving Interactive In-Context Learning from Natural Language Feedback</h3>

<p><strong>ELI5:</strong> Train a model to guess what feedback a human would give. Now the model has internalized the “teacher” and can improve itself without needing the actual teacher present. Self-correction without weight updates.</p>

<p><strong>Main idea:</strong> Models can learn to learn from feedback, making ICL interactive and self-improving.</p>

<h2 id="beyond-language">Beyond Language</h2>

<p>Newer work applies ICL to neuroscience discovery, showing that the mechanism is not limited to text tasks. It becomes a flexible reasoning substrate across domains. That’s when you know a concept has matured.</p>

<h2 id="the-arc">The Arc</h2>

<table>
  <thead>
    <tr>
      <th>Phase</th>
      <th>Era</th>
      <th>Key Insight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Discovery</strong></td>
      <td>2020</td>
      <td>Emerges from scale</td>
    </tr>
    <tr>
      <td><strong>Explanation</strong></td>
      <td>2022</td>
      <td>Implicit gradient descent</td>
    </tr>
    <tr>
      <td><strong>Engineering</strong></td>
      <td>2023-24</td>
      <td>Prompt design as optimization</td>
    </tr>
    <tr>
      <td><strong>Self-improvement</strong></td>
      <td>2026</td>
      <td>Learning from feedback</td>
    </tr>
  </tbody>
</table>

<h2 id="the-deeper-insight">The Deeper Insight</h2>

<p>In-Context Learning started as an emergent surprise. Now it’s becoming an engineered learning substrate inside transformers.</p>

<p>It was not magic. It was <strong>meta-learning hiding in plain sight</strong>.</p>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Paper</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Language Models are Few-Shot Learners (GPT-3)</td>
        <td><a href="https://arxiv.org/abs/2005.14165">arXiv:2005.14165</a></td>
      </tr>
      <tr>
        <td>What Explains In-Context Learning in Transformers?</td>
        <td><a href="https://arxiv.org/abs/2202.12837">arXiv:2202.12837</a></td>
      </tr>
      <tr>
        <td>Transformers Learn In-Context by Gradient Descent</td>
        <td><a href="https://arxiv.org/abs/2212.07677">arXiv:2212.07677</a></td>
      </tr>
      <tr>
        <td>Improving Interactive ICL from Natural Language Feedback</td>
        <td><a href="https://arxiv.org/abs/2602.16066">arXiv:2602.16066</a></td>
      </tr>
    </tbody>
  </table>

</div>

<hr />

<p><em>ICL started as “whoa, it works.” Now we understand “why it works.” Next: engineering it deliberately.</em></p>


          </div>





<div class="youtube-embed-container" id="yt-container-zWKmRxChRlA">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-zWKmRxChRlA"
      src="https://www.youtube.com/embed/zWKmRxChRlA?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-zWKmRxChRlA';
  const playerId = 'yt-player-zWKmRxChRlA';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="3"><p class="post-meta">February 22, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">473 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Autoencoders (compress and reconstruct), Correlation vs Causation (co-occurrence isn't cause), Curriculum Learning (easy to hard), Failure Analysis (categorize errors), Covariate Shift (new inputs, same task).</div><h3>
          <a class="post-link" href="/2026/02/22/five-ml-concepts-19/">
            Five ML Concepts - #19
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-19">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-nineteen.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/ppoONTOdqJQ">Five ML Concepts #19</a><br /><a href="https://www.youtube.com/shorts/ppoONTOdqJQ"><img src="https://img.youtube.com/vi/ppoONTOdqJQ/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Autoencoders</strong></td>
        <td><a href="https://www.science.org/doi/10.1126/science.1127647">Reducing the Dimensionality of Data with Neural Networks</a> (Hinton &amp; Salakhutdinov 2006)</td>
      </tr>
      <tr>
        <td><strong>Correlation vs Causation</strong></td>
        <td><a href="https://www.cambridge.org/core/books/causality/B0046844FAE10CBF274D4ACBDAEB5F5B">Causality</a> (Pearl 2009)</td>
      </tr>
      <tr>
        <td><strong>Curriculum Learning</strong></td>
        <td><a href="https://dl.acm.org/doi/10.1145/1553374.1553380">Curriculum Learning</a> (Bengio et al. 2009)</td>
      </tr>
      <tr>
        <td><strong>Failure Analysis</strong></td>
        <td><a href="https://www.oreilly.com/library/view/practical-machine-learning/9781098102357/">Practical Machine Learning for Computer Vision</a> (Lakshmanan et al. 2021)</td>
      </tr>
      <tr>
        <td><strong>Covariate Shift</strong></td>
        <td><a href="https://mitpress.mit.edu/9780262170055/">Dataset Shift in Machine Learning</a> (Quinonero-Candela et al. 2009)</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-autoencoders">1. Autoencoders</h3>

<p><strong>Autoencoders are neural networks trained to compress inputs into a smaller representation and reconstruct them.</strong> The bottleneck forces the model to capture essential structure.</p>

<p>This learned compression is useful for dimensionality reduction, denoising, and feature learning.</p>

<blockquote>
  <p>Like summarizing a book into key points and then rebuilding the story from that summary.</p>
</blockquote>

<h3 id="2-correlation-vs-causation">2. Correlation vs Causation</h3>

<p><strong>Two variables can move together without one causing the other.</strong> Models typically learn correlations present in data, not true cause-and-effect relationships.</p>

<p>This matters because interventions based on correlation alone may not produce intended effects.</p>

<blockquote>
  <p>Like noticing umbrella sales rise with rain—umbrellas don’t cause rain.</p>
</blockquote>

<h3 id="3-curriculum-learning">3. Curriculum Learning</h3>

<p><strong>Training starts with easier examples and gradually introduces harder ones.</strong> This can improve stability and learning speed in some settings.</p>

<p>The approach mirrors how humans learn complex subjects incrementally.</p>

<blockquote>
  <p>Like teaching math by starting with addition before moving to calculus.</p>
</blockquote>

<h3 id="4-failure-analysis">4. Failure Analysis</h3>

<p><strong>Failure analysis groups model errors into categories to understand where performance breaks down.</strong> This helps target improvements instead of guessing.</p>

<p>Systematic error analysis often reveals actionable patterns invisible in aggregate metrics.</p>

<blockquote>
  <p>Like a teacher reviewing which types of questions students miss most often.</p>
</blockquote>

<h3 id="5-covariate-shift">5. Covariate Shift</h3>

<p><strong>Covariate shift occurs when the input distribution changes between training and deployment, while the task itself remains the same.</strong> The model may underperform because it sees unfamiliar inputs.</p>

<p>Monitoring input distributions helps detect this shift early.</p>

<blockquote>
  <p>Like training a driver in sunny weather and testing them in snow.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Autoencoders</strong></td>
      <td>Compress and reconstruct to learn structure</td>
    </tr>
    <tr>
      <td><strong>Correlation vs Causation</strong></td>
      <td>Co-occurrence isn’t cause</td>
    </tr>
    <tr>
      <td><strong>Curriculum Learning</strong></td>
      <td>Start easy, progress to hard</td>
    </tr>
    <tr>
      <td><strong>Failure Analysis</strong></td>
      <td>Categorize errors to guide fixes</td>
    </tr>
    <tr>
      <td><strong>Covariate Shift</strong></td>
      <td>New inputs, same task</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 19 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/23/five-ml-concepts-20/">Next: #20 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-ppoONTOdqJQ">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-ppoONTOdqJQ"
      src="https://www.youtube.com/embed/ppoONTOdqJQ?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-ppoONTOdqJQ';
  const playerId = 'yt-player-ppoONTOdqJQ';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="4"><p class="post-meta">February 21, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">2251 words</span> &bull; <span class="post-read-time">12 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">JSON is everywhere, but it's not the only option. This post explores data formats beyond basic JSON—JSONL for streaming, JSONB for fast queries, Protocol Buffers for compact wire formats, YAML/TOML for human editing, and TOON for LLM efficiency. Each has trade-offs: pick two of readability, compactness, or speed.</div><h3>
          <a class="post-link" href="/2026/02/21/json-et-al-data-serialization-formats/">
            JSON et al: A Deep Dive into Data Serialization Formats
          </a>
        </h3><nav class="toc" data-toc-id="json-et-al-data-serialization-formats">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-brackets.png" class="post-marker no-invert" alt="" /></p>

<p>JSON is everywhere. APIs. Logs. Databases. Configuration files. But it’s not alone. A whole ecosystem of formats exists—each optimizing for different tradeoffs.</p>

<p>This post expands on the <a href="https://www.youtube.com/shorts/3ezjk1CnZEU">JSON et al</a> short, providing technical depth on each format: when it was created, where it’s specified, and what problems it solves.</p>

<hr />

<h2 id="the-tradeoff-triangle">The Tradeoff Triangle</h2>

<p>Before diving in, understand the fundamental constraint. Data formats balance three competing goals:</p>

<table>
  <thead>
    <tr>
      <th>Goal</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Human Readability</strong></td>
      <td>Can a developer read and edit it directly?</td>
    </tr>
    <tr>
      <td><strong>Compactness</strong></td>
      <td>How many bytes does it take to represent data?</td>
    </tr>
    <tr>
      <td><strong>Query Performance</strong></td>
      <td>How fast can you access specific fields?</td>
    </tr>
  </tbody>
</table>

<p>You usually only get two. JSON optimizes readability. Protobuf optimizes compactness. JSONB optimizes query performance. No format wins everywhere.</p>

<hr />

<h2 id="json-the-ubiquitous-baseline">JSON: The Ubiquitous Baseline</h2>

<p><strong>Created:</strong> 2001 (discovered/formalized by Douglas Crockford)
<strong>Specification:</strong> <a href="https://www.ecma-international.org/publications-and-standards/standards/ecma-404/">ECMA-404</a> (2013), <a href="https://datatracker.ietf.org/doc/html/rfc8259">RFC 8259</a> (2017)
<strong>File Extension:</strong> <code class="language-plaintext highlighter-rouge">.json</code></p>

<p>JSON (JavaScript Object Notation) emerged from JavaScript’s object literal syntax but became language-agnostic. Crockford didn’t invent it—he “discovered” it already existing in JavaScript and formalized the specification.</p>

<h3 id="technical-details">Technical Details</h3>

<ul>
  <li><strong>Encoding:</strong> UTF-8 text (UTF-16/32 allowed but rare)</li>
  <li><strong>Data Types:</strong> Objects <code class="language-plaintext highlighter-rouge">{}</code>, arrays <code class="language-plaintext highlighter-rouge">[]</code>, strings, numbers, booleans, <code class="language-plaintext highlighter-rouge">null</code></li>
  <li><strong>Schema:</strong> None required</li>
  <li><strong>Comments:</strong> Not allowed in strict JSON</li>
</ul>

<h3 id="strengths">Strengths</h3>

<ul>
  <li>Universal parser support (every language has one)</li>
  <li>Human readable without tools</li>
  <li>Web-native (JavaScript parses it natively)</li>
  <li>Simple specification (fits on a business card)</li>
</ul>

<h3 id="weaknesses">Weaknesses</h3>

<ul>
  <li>Verbose (field names repeated for every object)</li>
  <li>No binary data type (must base64-encode)</li>
  <li>No comments (frustrating for config files)</li>
  <li>Parsing overhead (tokenization + string decoding every time)</li>
</ul>

<h3 id="eli5">ELI5</h3>

<p>Like typing a long email instead of sending a terse text. Every message spells everything out—clear, but verbose.</p>

<h3 id="when-to-use">When to Use</h3>

<p>REST APIs, configuration (when comments aren’t needed), data interchange between systems, anywhere human readability matters more than efficiency.</p>

<hr />

<h2 id="jsonl--ndjson-streaming-json">JSONL / NDJSON: Streaming JSON</h2>

<p><strong>Created:</strong> ~2013 (formalized)
<strong>Specification:</strong> <a href="https://jsonlines.org/">JSON Lines</a>, <a href="http://ndjsonspec.org/">NDJSON</a>
<strong>File Extension:</strong> <code class="language-plaintext highlighter-rouge">.jsonl</code>, <code class="language-plaintext highlighter-rouge">.ndjson</code></p>

<p>JSONL (JSON Lines) and NDJSON (Newline-Delimited JSON) are the same concept: one valid JSON object per line, separated by newlines.</p>

<h3 id="technical-details-1">Technical Details</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{"name": "Alice", "score": 95}
{"name": "Bob", "score": 87}
{"name": "Carol", "score": 92}
</code></pre></div></div>

<p>No wrapping array. Each line is independently parseable.</p>

<h3 id="strengths-1">Strengths</h3>

<ul>
  <li><strong>Streaming:</strong> Process line-by-line without loading entire file</li>
  <li><strong>Append-only:</strong> Add records without rewriting the file</li>
  <li><strong>Parallel processing:</strong> Split by line, distribute to workers</li>
  <li><strong>Fault-tolerant:</strong> One corrupt line doesn’t invalidate the file</li>
</ul>

<h3 id="weaknesses-1">Weaknesses</h3>

<ul>
  <li>Not valid JSON (can’t parse with standard JSON parser)</li>
  <li>Still text-based (same verbosity as JSON)</li>
  <li>No random access by index</li>
</ul>

<h3 id="eli5-1">ELI5</h3>

<p>Like removing one comma per line to save some typing. Each line is self-contained, so you can grab and process them one at a time.</p>

<h3 id="when-to-use-1">When to Use</h3>

<p>Log files, big data pipelines (Spark, Pandas), ML datasets, event streams, anywhere you need to process records incrementally.</p>

<hr />

<h2 id="jsonb-binary-json-for-databases">JSONB: Binary JSON for Databases</h2>

<p><strong>Created:</strong> 2014 (PostgreSQL 9.4)
<strong>Specification:</strong> Implementation-specific (no universal standard)
<strong>Storage:</strong> Database column type</p>

<p>JSONB isn’t a file format—it’s a database storage optimization. PostgreSQL’s JSONB differs from MongoDB’s BSON, which differs from other implementations.</p>

<h3 id="postgresql-jsonb-details">PostgreSQL JSONB Details</h3>

<ul>
  <li><strong>Parsed once:</strong> Text converted to binary on INSERT</li>
  <li><strong>Keys sorted:</strong> Deterministic ordering for indexing</li>
  <li><strong>Duplicates removed:</strong> Last value wins</li>
  <li><strong>Offset table:</strong> O(log n) field lookup instead of O(n) text scanning</li>
</ul>

<h3 id="mongodb-bson">MongoDB BSON</h3>

<p><strong>Specification:</strong> <a href="https://bsonspec.org/">bsonspec.org</a></p>

<p>BSON (Binary JSON) is MongoDB’s serialization format. Unlike PostgreSQL’s JSONB, BSON is a standalone binary format:</p>

<ul>
  <li>Type-prefixed values</li>
  <li>Supports additional types (Date, Binary, ObjectId)</li>
  <li>Length-prefixed for fast skipping</li>
  <li>~10-15% smaller than JSON typically</li>
</ul>

<h3 id="strengths-2">Strengths</h3>

<ul>
  <li>Fast queries without re-parsing</li>
  <li>Indexable (GIN indexes on JSONB in PostgreSQL)</li>
  <li>Type coercion at storage time</li>
</ul>

<h3 id="weaknesses-2">Weaknesses</h3>

<ul>
  <li>Not portable (implementation-specific)</li>
  <li>Not human-readable</li>
  <li>INSERT overhead (parsing cost upfront)</li>
</ul>

<h3 id="eli5-2">ELI5</h3>

<p>Instead of cooking from scratch every time, you heat a pre-made meal. The prep work happens once (on INSERT), so serving (queries) is fast.</p>

<h3 id="when-to-use-2">When to Use</h3>

<p>Database storage where you query into JSON structures. PostgreSQL JSONB + GIN indexes enable fast <code class="language-plaintext highlighter-rouge">@&gt;</code> containment queries.</p>

<hr />

<h2 id="protocol-buffers-googles-schema-first-format">Protocol Buffers: Google’s Schema-First Format</h2>

<p><strong>Created:</strong> 2001 (internal Google), 2008 (open-sourced)
<strong>Specification:</strong> <a href="https://developers.google.com/protocol-buffers/docs/proto3">developers.google.com/protocol-buffers</a>
<strong>File Extension:</strong> <code class="language-plaintext highlighter-rouge">.proto</code> (schema), binary wire format</p>

<p>Protocol Buffers (Protobuf) is Google’s language-neutral, schema-required serialization format. It powers gRPC.</p>

<h3 id="technical-details-2">Technical Details</h3>

<p>Schema definition:</p>
<div class="language-protobuf highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">message</span> <span class="nc">Sensor</span> <span class="p">{</span>
  <span class="kt">int32</span> <span class="na">temperature</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
  <span class="kt">int32</span> <span class="na">humidity</span> <span class="o">=</span> <span class="mi">2</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Wire format uses field numbers, not names:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Field 1: 72
Field 2: 40
</code></pre></div></div>

<h3 id="key-features">Key Features</h3>

<ul>
  <li><strong>Varint encoding:</strong> Small integers use fewer bytes</li>
  <li><strong>Field numbers:</strong> Enable backward compatibility</li>
  <li><strong>Code generation:</strong> <code class="language-plaintext highlighter-rouge">.proto</code> → language-specific classes</li>
  <li><strong>No self-description:</strong> Receiver must know schema</li>
</ul>

<h3 id="strengths-3">Strengths</h3>

<ul>
  <li>Extremely compact (3-10x smaller than JSON typically)</li>
  <li>Fast serialization/deserialization</li>
  <li>Strong versioning semantics</li>
  <li>gRPC integration</li>
</ul>

<h3 id="weaknesses-3">Weaknesses</h3>

<ul>
  <li>Requires schema agreement</li>
  <li>Not human-readable</li>
  <li>Tooling required for debugging</li>
  <li>Schema evolution has rules</li>
</ul>

<h3 id="eli5-3">ELI5</h3>

<p>Everyone agrees upfront what “field 1” means. You don’t waste space spelling out “temperature”—you just send the number 1 and the value. Both sides know the code.</p>

<h3 id="when-to-use-3">When to Use</h3>

<p>Microservices (gRPC), internal APIs, anywhere bandwidth and latency matter more than debuggability.</p>

<hr />

<h2 id="asn1-the-telecom-veteran">ASN.1: The Telecom Veteran</h2>

<p><strong>Created:</strong> 1984 (ITU-T X.208)
<strong>Specification:</strong> <a href="https://www.itu.int/rec/T-REC-X.680-X.683">ITU-T X.680-X.683</a>
<strong>Encoding Rules:</strong> BER, DER, PER, XER, and more</p>

<p>ASN.1 (Abstract Syntax Notation One) predates all modern formats. It defines both schema and encoding, with multiple encoding rules for different use cases.</p>

<h3 id="encoding-rules-comparison">Encoding Rules Comparison</h3>

<table>
  <thead>
    <tr>
      <th>Rule</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>BER</strong> (Basic Encoding Rules)</td>
      <td>Flexible, general purpose</td>
    </tr>
    <tr>
      <td><strong>DER</strong> (Distinguished Encoding Rules)</td>
      <td>Deterministic, for cryptography</td>
    </tr>
    <tr>
      <td><strong>PER</strong> (Packed Encoding Rules)</td>
      <td>Most compact, for bandwidth-constrained</td>
    </tr>
    <tr>
      <td><strong>XER</strong> (XML Encoding Rules)</td>
      <td>XML-based, for interop</td>
    </tr>
  </tbody>
</table>

<h3 id="where-you-see-asn1">Where You See ASN.1</h3>

<ul>
  <li><strong>X.509 certificates</strong> (SSL/TLS certs are DER-encoded ASN.1)</li>
  <li><strong>LDAP</strong> (directory services)</li>
  <li><strong>SNMP</strong> (network management)</li>
  <li><strong>Telecom protocols</strong> (SS7, GSM, LTE)</li>
</ul>

<h3 id="strengths-4">Strengths</h3>

<ul>
  <li>Bit-level precision</li>
  <li>Proven over 40 years</li>
  <li>Multiple encoding options</li>
  <li>Formal verification possible</li>
</ul>

<h3 id="weaknesses-4">Weaknesses</h3>

<ul>
  <li>Complex specification</li>
  <li>Steep learning curve</li>
  <li>Tooling can be expensive</li>
  <li>Security vulnerabilities in parsers (historically)</li>
</ul>

<h3 id="eli5-4">ELI5</h3>

<p>Same idea as Protobuf—everyone agrees upfront what each field number means. ASN.1 just got there 20 years earlier and handles even more edge cases.</p>

<h3 id="when-to-use-4">When to Use</h3>

<p>You probably won’t choose ASN.1 for new projects. You’ll encounter it in cryptography, certificates, and legacy telecom systems.</p>

<hr />

<h2 id="yaml-human-friendly-configuration">YAML: Human-Friendly Configuration</h2>

<p><strong>Created:</strong> 2001 (Clark Evans, Ingy döt Net, Oren Ben-Kiki)
<strong>Specification:</strong> <a href="https://yaml.org/spec/1.2.2/">yaml.org/spec/1.2.2</a>
<strong>File Extension:</strong> <code class="language-plaintext highlighter-rouge">.yaml</code>, <code class="language-plaintext highlighter-rouge">.yml</code></p>

<p>YAML (YAML Ain’t Markup Language) prioritizes human readability. It’s a superset of JSON—any valid JSON is valid YAML.</p>

<h3 id="technical-details-3">Technical Details</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Comments allowed!</span>
<span class="na">server</span><span class="pi">:</span>
  <span class="na">host</span><span class="pi">:</span> <span class="s">localhost</span>
  <span class="na">port</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="na">features</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">auth</span>
    <span class="pi">-</span> <span class="s">logging</span>
</code></pre></div></div>

<h3 id="key-features-1">Key Features</h3>

<ul>
  <li><strong>Indentation-based:</strong> Whitespace matters</li>
  <li><strong>Comments:</strong> <code class="language-plaintext highlighter-rouge">#</code> for single-line</li>
  <li><strong>Anchors/aliases:</strong> <code class="language-plaintext highlighter-rouge">&amp;name</code> and <code class="language-plaintext highlighter-rouge">*name</code> for references</li>
  <li><strong>Multiple documents:</strong> <code class="language-plaintext highlighter-rouge">---</code> separator</li>
</ul>

<h3 id="strengths-5">Strengths</h3>

<ul>
  <li>Highly readable</li>
  <li>Comments supported</li>
  <li>Multi-line strings without escaping</li>
  <li>Complex data structures</li>
</ul>

<h3 id="weaknesses-5">Weaknesses</h3>

<ul>
  <li><strong>“Norway problem”:</strong> <code class="language-plaintext highlighter-rouge">NO</code> parses as boolean <code class="language-plaintext highlighter-rouge">false</code></li>
  <li>Whitespace sensitivity causes errors</li>
  <li>Multiple ways to express same data</li>
  <li>Security concerns (arbitrary code execution in some parsers)</li>
</ul>

<h3 id="eli5-5">ELI5</h3>

<p>Optimized for clarity, not bandwidth. YAML is for humans editing config files—not for machines exchanging data over networks.</p>

<h3 id="when-to-use-5">When to Use</h3>

<p>Configuration files (Kubernetes, Docker Compose, CI/CD), anywhere humans edit data directly and comments help.</p>

<hr />

<h2 id="toml-minimal-configuration">TOML: Minimal Configuration</h2>

<p><strong>Created:</strong> 2013 (Tom Preston-Werner)
<strong>Specification:</strong> <a href="https://toml.io/en/v1.0.0">toml.io</a>
<strong>File Extension:</strong> <code class="language-plaintext highlighter-rouge">.toml</code></p>

<p>TOML (Tom’s Obvious Minimal Language) emerged as a reaction to YAML’s complexity. It’s used by Rust (Cargo.toml), Python (pyproject.toml), and others.</p>

<h3 id="technical-details-4">Technical Details</h3>

<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">[</span><span class="n">server</span><span class="k">]</span>
<span class="n">host</span> <span class="o">=</span><span class="w"> </span><span class="s">"localhost"</span>
<span class="n">port</span> <span class="o">=</span><span class="w"> </span><span class="mi">8080</span>

<span class="k">[</span><span class="n">server</span><span class="k">.</span><span class="n">features</span><span class="k">]</span>
<span class="n">auth</span> <span class="o">=</span><span class="w"> </span><span class="kc">true</span>
<span class="n">logging</span> <span class="o">=</span><span class="w"> </span><span class="kc">true</span>
</code></pre></div></div>

<h3 id="key-features-2">Key Features</h3>

<ul>
  <li><strong>Explicit typing:</strong> Dates, times, arrays have clear syntax</li>
  <li><strong>Sections:</strong> <code class="language-plaintext highlighter-rouge">[section]</code> and <code class="language-plaintext highlighter-rouge">[section.subsection]</code></li>
  <li><strong>No anchors:</strong> Intentionally simpler than YAML</li>
  <li><strong>Deterministic:</strong> Same data = same representation</li>
</ul>

<h3 id="strengths-6">Strengths</h3>

<ul>
  <li>Easy to read and write</li>
  <li>Unambiguous parsing</li>
  <li>Clear error messages</li>
  <li>Growing ecosystem support</li>
</ul>

<h3 id="weaknesses-6">Weaknesses</h3>

<ul>
  <li>Less expressive than YAML</li>
  <li>Nested structures can be verbose</li>
  <li>Smaller ecosystem than JSON/YAML</li>
</ul>

<h3 id="eli5-6">ELI5</h3>

<p>Same goal as YAML—clarity for humans, not bandwidth for machines—but with stricter rules so you make fewer mistakes.</p>

<h3 id="when-to-use-6">When to Use</h3>

<p>Configuration files where YAML’s complexity isn’t needed. Rust projects (mandatory). Python packaging (pyproject.toml).</p>

<hr />

<h2 id="toon-token-optimized-for-llms">TOON: Token-Optimized for LLMs</h2>

<p><strong>Created:</strong> October 2025 (toon-format organization)
<strong>Specification:</strong> <a href="https://github.com/toon-format/toon">github.com/toon-format/toon</a> (v3.0)
<strong>File Extension:</strong> <code class="language-plaintext highlighter-rouge">.toon</code>
<strong>Media Type:</strong> <code class="language-plaintext highlighter-rouge">text/toon</code> (provisional)</p>

<p>TOON (Token Oriented Object Notation) is the newest format in this list, designed specifically for LLM input. It’s a lossless representation of JSON that minimizes tokens.</p>

<h3 id="technical-details-5">Technical Details</h3>

<p>TOON combines YAML-style indentation for nested objects with CSV-like tabular layouts for uniform arrays:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>users[2]{name,age}:
Alice,25
Bob,30
</code></pre></div></div>

<p>Equivalent JSON:</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="nl">"users"</span><span class="p">:</span><span class="w"> </span><span class="p">[{</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Alice"</span><span class="p">,</span><span class="w"> </span><span class="nl">"age"</span><span class="p">:</span><span class="w"> </span><span class="mi">25</span><span class="p">},</span><span class="w"> </span><span class="p">{</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Bob"</span><span class="p">,</span><span class="w"> </span><span class="nl">"age"</span><span class="p">:</span><span class="w"> </span><span class="mi">30</span><span class="p">}]}</span><span class="w">
</span></code></pre></div></div>

<h3 id="key-features-3">Key Features</h3>

<ul>
  <li><strong>Header-based:</strong> Field names declared once, values follow</li>
  <li><strong>40% fewer tokens:</strong> Than equivalent JSON typically</li>
  <li><strong>Lossless:</strong> Round-trips to JSON perfectly</li>
  <li><strong>UTF-8 always:</strong> No encoding ambiguity</li>
</ul>

<h3 id="performance">Performance</h3>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>JSON</th>
      <th>TOON</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Accuracy</td>
      <td>69.7%</td>
      <td>73.9%</td>
    </tr>
    <tr>
      <td>Efficiency (acc/1K tokens)</td>
      <td>15.3</td>
      <td>26.9</td>
    </tr>
  </tbody>
</table>

<h3 id="strengths-7">Strengths</h3>

<ul>
  <li>Significant token savings at scale</li>
  <li>Better context window utilization</li>
  <li>Lower API costs for LLM applications</li>
  <li>Human-readable (unlike binary formats)</li>
</ul>

<h3 id="weaknesses-7">Weaknesses</h3>

<ul>
  <li>New format (October 2025)</li>
  <li>Limited tooling compared to JSON</li>
  <li>Requires conversion layer for existing systems</li>
  <li>Not yet widely adopted</li>
</ul>

<h3 id="eli5-7">ELI5</h3>

<p>Like having one header row for each column in a table instead of repeating the column name for every single row. You declare field names once, then just list the values.</p>

<h3 id="when-to-use-7">When to Use</h3>

<p>LLM prompts with structured data, RAG applications, anywhere token efficiency matters. Especially useful for large datasets with uniform object arrays.</p>

<h3 id="implementations">Implementations</h3>

<ul>
  <li><strong>TypeScript:</strong> Reference implementation</li>
  <li><strong>Python:</strong> <a href="https://pypi.org/project/toons/">toons</a> (Rust-based, fast)</li>
  <li><strong>Go, Rust, .NET:</strong> Available via toon-format org</li>
</ul>

<hr />

<h2 id="alternatives-not-in-the-video">Alternatives Not in the Video</h2>

<h3 id="messagepack">MessagePack</h3>

<p><strong>Created:</strong> 2008 (Sadayuki Furuhashi)
<strong>Specification:</strong> <a href="https://msgpack.org/">msgpack.org</a></p>

<p>Binary JSON without schema. Type-prefixed values, efficient numeric encoding.</p>

<p><strong>Use when:</strong> You want JSON semantics but smaller/faster.</p>

<h3 id="cbor">CBOR</h3>

<p><strong>Created:</strong> 2013 (IETF)
<strong>Specification:</strong> <a href="https://datatracker.ietf.org/doc/html/rfc8949">RFC 8949</a></p>

<p>Concise Binary Object Representation. Designed for constrained environments (IoT).</p>

<p><strong>Use when:</strong> Resource-constrained devices, need a standard binary format.</p>

<h3 id="apache-avro">Apache Avro</h3>

<p><strong>Created:</strong> 2009 (Apache, Doug Cutting)
<strong>Specification:</strong> <a href="https://avro.apache.org/docs/current/spec.html">avro.apache.org</a></p>

<p>Schema-based, row-oriented binary format. Schema embedded or stored separately. Strong schema evolution support.</p>

<p><strong>Use when:</strong> Big data pipelines (Hadoop, Kafka), schema evolution is critical.</p>

<h3 id="apache-parquet">Apache Parquet</h3>

<p><strong>Created:</strong> 2013 (Twitter + Cloudera)
<strong>Specification:</strong> <a href="https://parquet.apache.org/docs/file-format/">parquet.apache.org</a></p>

<p>Columnar storage format. Not for serialization—for analytics storage.</p>

<p><strong>Use when:</strong> Large-scale analytics, data warehousing, Spark/Pandas workflows.</p>

<h3 id="capn-proto">Cap’n Proto</h3>

<p><strong>Created:</strong> 2013 (Kenton Varda, ex-Protobuf author)
<strong>Specification:</strong> <a href="https://capnproto.org/">capnproto.org</a></p>

<p>Zero-copy serialization. The serialized form <em>is</em> the in-memory form.</p>

<p><strong>Use when:</strong> Extreme performance requirements, inter-process communication.</p>

<h3 id="flatbuffers">FlatBuffers</h3>

<p><strong>Created:</strong> 2014 (Google)
<strong>Specification:</strong> <a href="https://google.github.io/flatbuffers/">google.github.io/flatbuffers</a></p>

<p>Zero-copy like Cap’n Proto but with better tooling. Used in games, mobile.</p>

<p><strong>Use when:</strong> Games, mobile apps, anywhere memory allocation matters.</p>

<hr />

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Format</th>
      <th>Year</th>
      <th>Schema</th>
      <th>Binary</th>
      <th>Human-Readable</th>
      <th>Best For</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>JSON</td>
      <td>2001</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>APIs, interchange</td>
    </tr>
    <tr>
      <td>JSONL</td>
      <td>2013</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>Logs, streaming</td>
    </tr>
    <tr>
      <td>JSONB</td>
      <td>2014</td>
      <td>No</td>
      <td>Yes</td>
      <td>No</td>
      <td>Database queries</td>
    </tr>
    <tr>
      <td>Protobuf</td>
      <td>2008</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>No</td>
      <td>Microservices</td>
    </tr>
    <tr>
      <td>ASN.1</td>
      <td>1984</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>No</td>
      <td>Crypto, telecom</td>
    </tr>
    <tr>
      <td>YAML</td>
      <td>2001</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>Config files</td>
    </tr>
    <tr>
      <td>TOML</td>
      <td>2013</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>Simple config</td>
    </tr>
    <tr>
      <td>TOON</td>
      <td>2025</td>
      <td>No</td>
      <td>No</td>
      <td>Yes</td>
      <td>LLM prompts</td>
    </tr>
    <tr>
      <td>MessagePack</td>
      <td>2008</td>
      <td>No</td>
      <td>Yes</td>
      <td>No</td>
      <td>Fast JSON</td>
    </tr>
    <tr>
      <td>CBOR</td>
      <td>2013</td>
      <td>Optional</td>
      <td>Yes</td>
      <td>No</td>
      <td>IoT</td>
    </tr>
    <tr>
      <td>Avro</td>
      <td>2009</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>No</td>
      <td>Big data</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>No “best” format exists.</strong> Each optimizes for different constraints.</p>
  </li>
  <li>
    <p><strong>Text formats favor humans.</strong> JSON, YAML, TOML prioritize readability over efficiency.</p>
  </li>
  <li>
    <p><strong>Binary formats favor machines.</strong> Protobuf, MessagePack, CBOR prioritize compactness and speed.</p>
  </li>
  <li>
    <p><strong>Schema formats favor correctness.</strong> Protobuf, Avro, ASN.1 catch errors at compile time.</p>
  </li>
  <li>
    <p><strong>The tradeoff triangle is real.</strong> Readability, compactness, query performance—pick two.</p>
  </li>
</ol>

<p>The question isn’t “which format wins?” The question is: what problem are you solving?</p>

<hr />

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://www.ecma-international.org/publications-and-standards/standards/ecma-404/">ECMA-404 JSON Specification</a></li>
  <li><a href="https://datatracker.ietf.org/doc/html/rfc8259">RFC 8259 JSON</a></li>
  <li><a href="https://jsonlines.org/">JSON Lines Specification</a></li>
  <li><a href="https://www.postgresql.org/docs/current/datatype-json.html">PostgreSQL JSONB Documentation</a></li>
  <li><a href="https://developers.google.com/protocol-buffers">Protocol Buffers Documentation</a></li>
  <li><a href="https://yaml.org/spec/1.2.2/">YAML 1.2.2 Specification</a></li>
  <li><a href="https://toml.io/en/v1.0.0">TOML v1.0.0 Specification</a></li>
  <li><a href="https://datatracker.ietf.org/doc/html/rfc8949">RFC 8949 CBOR</a></li>
  <li><a href="https://github.com/msgpack/msgpack/blob/master/spec.md">MessagePack Specification</a></li>
  <li><a href="https://avro.apache.org/docs/current/spec.html">Apache Avro Specification</a></li>
</ul>

<hr />

<p><em>Data formats are design decisions. Choose based on your constraints, not trends.</em></p>

<p><em>Questions? Find me on <a href="https://www.youtube.com/@SoftwareWrighter">YouTube @SoftwareWrighter</a>.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-3ezjk1CnZEU">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-3ezjk1CnZEU"
      src="https://www.youtube.com/embed/3ezjk1CnZEU?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-3ezjk1CnZEU';
  const playerId = 'yt-player-3ezjk1CnZEU';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="5"><p class="post-meta">February 21, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">466 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Preference Learning (train from comparisons), Ensembling (combine models for robustness), ML Fragility (breaks on distribution shift), Epoch (one pass through data), Cost vs Quality (bigger isn't always better).</div><h3>
          <a class="post-link" href="/2026/02/21/five-ml-concepts-18/">
            Five ML Concepts - #18
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-18">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-eighteen.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/osj1GQxG4uo">Five ML Concepts #18</a><br /><a href="https://www.youtube.com/shorts/osj1GQxG4uo"><img src="https://img.youtube.com/vi/osj1GQxG4uo/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Preference Learning</strong></td>
        <td><a href="https://arxiv.org/abs/2009.01325">Learning to summarize from human feedback</a> (Stiennon et al. 2020)</td>
      </tr>
      <tr>
        <td><strong>Ensembling</strong></td>
        <td><a href="https://link.springer.com/chapter/10.1007/3-540-45014-9_1">Ensemble Methods in Machine Learning</a> (Dietterich 2000)</td>
      </tr>
      <tr>
        <td><strong>ML Fragility</strong></td>
        <td><a href="https://arxiv.org/abs/1903.12261">Distribution Shift</a> (Quinonero-Candela et al. 2009)</td>
      </tr>
      <tr>
        <td><strong>Epoch</strong></td>
        <td><a href="https://www.deeplearningbook.org/">Deep Learning</a> (Goodfellow et al. 2016), Chapter 8</td>
      </tr>
      <tr>
        <td><strong>Cost vs Quality</strong></td>
        <td><a href="https://arxiv.org/abs/2009.06732">Efficient Transformers: A Survey</a> (Tay et al. 2022)</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-preference-learning">1. Preference Learning</h3>

<p><strong>Instead of learning from fixed labels, models are trained from comparisons between outputs.</strong> This helps align model behavior with human judgments.</p>

<p>The approach works well when absolute quality is hard to define but relative preferences are easier to express.</p>

<blockquote>
  <p>Like learning to cook by asking which dish tastes better.</p>
</blockquote>

<h3 id="2-ensembling">2. Ensembling</h3>

<p><strong>Ensembling combines predictions from multiple models.</strong> Different models make different errors, and combining them can improve robustness.</p>

<p>Common strategies include voting, averaging, and stacking models together.</p>

<blockquote>
  <p>Like asking several experts and averaging their opinions.</p>
</blockquote>

<h3 id="3-why-ml-is-fragile">3. Why ML Is Fragile</h3>

<p><strong>Models rely on statistical patterns learned from data.</strong> When those patterns shift, performance can degrade quickly.</p>

<p>This fragility emerges because models optimize for training distributions, not arbitrary future scenarios.</p>

<blockquote>
  <p>Like a spell checker that works on common words but struggles with unusual ones.</p>
</blockquote>

<h3 id="4-epoch">4. Epoch</h3>

<p><strong>An epoch is one complete pass through the training dataset.</strong> Multiple epochs allow the model to refine its weights over repeated passes.</p>

<p>Training typically continues for many epochs until validation performance stops improving.</p>

<blockquote>
  <p>Like reading a textbook from beginning to end more than once.</p>
</blockquote>

<h3 id="5-cost-vs-quality-tradeoffs">5. Cost vs Quality Tradeoffs</h3>

<p><strong>Increasing model size or compute often improves performance, but also increases cost and latency.</strong> Engineers balance quality against budget and responsiveness.</p>

<p>Production systems often use smaller, faster models rather than the largest available.</p>

<blockquote>
  <p>Like choosing between a luxury car and an economy car depending on your needs.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Preference Learning</strong></td>
      <td>Train from comparisons, not labels</td>
    </tr>
    <tr>
      <td><strong>Ensembling</strong></td>
      <td>Combine models for robustness</td>
    </tr>
    <tr>
      <td><strong>ML Fragility</strong></td>
      <td>Statistical models break on distribution shift</td>
    </tr>
    <tr>
      <td><strong>Epoch</strong></td>
      <td>One pass through training data</td>
    </tr>
    <tr>
      <td><strong>Cost vs Quality</strong></td>
      <td>Bigger isn’t always better in production</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 18 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/22/five-ml-concepts-19/">Next: #19 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-osj1GQxG4uo">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-osj1GQxG4uo"
      src="https://www.youtube.com/embed/osj1GQxG4uo?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-osj1GQxG4uo';
  const playerId = 'yt-player-osj1GQxG4uo';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="6"><p class="post-meta">February 20, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">970 words</span> &bull; <span class="post-read-time">5 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Personal Software via Vibe Coding: a music tool for AI agents. midi-cli-rs provides mood presets (suspense, upbeat, calm, jazz) so agents can generate complete audio compositions from simple commands. No music theory required.</div><h3>
          <a class="post-link" href="/2026/02/20/midi-cli-rs-music-for-ai-agents/">
            midi-cli-rs: Music Generation for AI Coding Agents
          </a>
        </h3><nav class="toc" data-toc-id="midi-cli-rs-music-for-ai-agents">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-notes.png" class="post-marker" alt="" /></p>

<p>AI coding agents can write code, generate images, and produce text. But what about music? When I needed background audio for explainer videos, I wanted a tool that AI agents could use directly—no music theory required.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://youtu.be/nDNcbKE8KtE">midi-cli-rs Explainer</a><br /><a href="https://youtu.be/nDNcbKE8KtE"><img src="https://img.youtube.com/vi/nDNcbKE8KtE/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>Examples</strong></td>
        <td><a href="https://softwarewrighter.github.io/midi-cli-rs/">Listen to Samples</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/midi-cli-rs">midi-cli-rs</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-problem">The Problem</h2>

<p>Generating music programmatically is hard. Traditional approaches require understanding music theory, MIDI specifications, instrument mappings, and audio synthesis. That’s a lot to ask of an AI agent that just needs a 5-second intro.</p>

<p>I wanted something simpler: a CLI tool where an agent could say “give me 5 seconds of suspenseful music” and get a usable WAV file.</p>

<h2 id="the-solution-mood-presets">The Solution: Mood Presets</h2>

<p>midi-cli-rs solves this with <strong>mood presets</strong>—curated musical generators that produce complete compositions from a single command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Generate a 5-second suspenseful intro</span>
midi-cli-rs preset <span class="nt">--mood</span> suspense <span class="nt">--duration</span> 5 <span class="nt">-o</span> intro.wav

<span class="c"># Upbeat outro with specific key</span>
midi-cli-rs preset <span class="nt">-m</span> upbeat <span class="nt">-d</span> 7 <span class="nt">--key</span> C <span class="nt">--seed</span> 42 <span class="nt">-o</span> outro.wav
</code></pre></div></div>

<p>Six moods are available:</p>

<table>
  <thead>
    <tr>
      <th>Mood</th>
      <th>Character</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">suspense</code></td>
      <td>Low drones, tremolo strings, tension</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">eerie</code></td>
      <td>Sparse tones, diminished harmony</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">upbeat</code></td>
      <td>Rhythmic chords, energetic</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">calm</code></td>
      <td>Warm pads, gentle arpeggios</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">ambient</code></td>
      <td>Textural drones, pentatonic bells</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">jazz</code></td>
      <td>Walking bass, brushed drums, piano trio</td>
    </tr>
  </tbody>
</table>

<p>Each mood generates multi-layer compositions with appropriate instruments, rhythms, and harmonies. The <code class="language-plaintext highlighter-rouge">--seed</code> parameter ensures reproducibility—same seed, same output. Different seeds produce meaningful variations in melody contour, rhythm patterns, and instrument choices.</p>

<h2 id="melodic-variation">Melodic Variation</h2>

<p>The presets don’t just randomize notes—they use a <strong>contour-based variation system</strong>. Changing the seed produces melodies that follow different shapes (ascending, descending, arch, wave) while staying musically coherent. This means you can generate multiple versions of a mood and pick the one that fits best.</p>

<h2 id="how-it-works">How It Works</h2>

<p>The tool generates MIDI programmatically, then renders to WAV using FluidSynth:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mood Preset → MIDI Generation → FluidSynth → WAV Output
</code></pre></div></div>

<p><strong>MIDI generation</strong> uses the <code class="language-plaintext highlighter-rouge">midly</code> crate to create standard MIDI files. Each preset generates multiple tracks with different instruments, note patterns, and dynamics.</p>

<p><strong>Audio rendering</strong> calls FluidSynth as a subprocess with a SoundFont (instrument samples). This avoids LGPL licensing complications—subprocess execution doesn’t trigger copyleft.</p>

<h2 id="note-level-control">Note-Level Control</h2>

<p>When presets aren’t enough, you can specify exact notes:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Note format: PITCH:DURATION:VELOCITY[@OFFSET]</span>
midi-cli-rs generate <span class="se">\</span>
    <span class="nt">--notes</span> <span class="s2">"C4:0.5:80@0,E4:0.5:80@0.5,G4:0.5:80@1,C5:1:90@1.5"</span> <span class="se">\</span>
    <span class="nt">-i</span> piano <span class="nt">-t</span> 120 <span class="nt">-o</span> arpeggio.wav
</code></pre></div></div>

<p>Or use JSON for complex multi-track arrangements:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s1">'{"tempo":90,"instrument":"piano","notes":[
  {"pitch":"C4","duration":0.5,"velocity":80,"offset":0},
  {"pitch":"E4","duration":0.5,"velocity":80,"offset":0.5},
  {"pitch":"G4","duration":1,"velocity":90,"offset":1}
]}'</span> | midi-cli-rs generate <span class="nt">--json</span> <span class="nt">-o</span> output.wav
</code></pre></div></div>

<h2 id="web-ui">Web UI</h2>

<p>For interactive composition, there’s a browser-based interface:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>midi-cli-rs serve  <span class="c"># Starts on http://127.0.0.1:3105</span>
</code></pre></div></div>

<p>The <strong>Presets tab</strong> lets you adjust mood, key, duration, intensity, and tempo with immediate audio preview. Click the clock button to generate a time-based seed for unique but reproducible results.</p>

<p>The <strong>Melodies tab</strong> provides note-by-note composition with keyboard shortcuts:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">a-g</code> for note pitch</li>
  <li><code class="language-plaintext highlighter-rouge">[</code> / <code class="language-plaintext highlighter-rouge">]</code> to adjust duration</li>
  <li><code class="language-plaintext highlighter-rouge">+</code> / <code class="language-plaintext highlighter-rouge">-</code> to change octave</li>
  <li><code class="language-plaintext highlighter-rouge">Tab</code> to navigate between notes</li>
</ul>

<h2 id="for-ai-agents">For AI Agents</h2>

<p>The CLI is designed for AI agent usage:</p>

<ol>
  <li><strong>Simple commands</strong>: One line generates complete audio</li>
  <li><strong>Reproducible</strong>: Seed values ensure consistent output</li>
  <li><strong>Self-documenting</strong>: <code class="language-plaintext highlighter-rouge">--help</code> includes agent-specific instructions</li>
  <li><strong>Composable</strong>: Generate tracks separately, combine with ffmpeg</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># AI agent workflow</span>
midi-cli-rs preset <span class="nt">-m</span> suspense <span class="nt">-d</span> 5 <span class="nt">--seed</span> 1 <span class="nt">-o</span> intro.wav
midi-cli-rs preset <span class="nt">-m</span> upbeat <span class="nt">-d</span> 10 <span class="nt">--seed</span> 2 <span class="nt">-o</span> main.wav
ffmpeg <span class="nt">-i</span> intro.wav <span class="nt">-i</span> main.wav <span class="nt">-filter_complex</span> <span class="nv">concat</span><span class="o">=</span><span class="nv">n</span><span class="o">=</span>2:v<span class="o">=</span>0:a<span class="o">=</span>1 final.wav
</code></pre></div></div>

<h2 id="soundfont-quality-matters">SoundFont Quality Matters</h2>

<p>The quality of generated audio depends heavily on the SoundFont used. SoundFonts are collections of audio samples for each instrument—a tiny SoundFont with compressed samples will sound thin and artificial, while a larger one with high-quality recordings produces professional results.</p>

<table>
  <thead>
    <tr>
      <th>SoundFont</th>
      <th>Size</th>
      <th>Quality</th>
      <th>License</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>TimGM6mb</td>
      <td>~6MB</td>
      <td>Basic</td>
      <td>GPL v2</td>
    </tr>
    <tr>
      <td>GeneralUser GS</td>
      <td>~30MB</td>
      <td>Good</td>
      <td>Permissive</td>
    </tr>
    <tr>
      <td>FluidR3_GM</td>
      <td>~140MB</td>
      <td>Very Good</td>
      <td>MIT</td>
    </tr>
    <tr>
      <td>MuseScore_General</td>
      <td>~200MB</td>
      <td>Excellent</td>
      <td>MIT</td>
    </tr>
  </tbody>
</table>

<p>For anything beyond quick prototypes, use a quality SoundFont. The difference is dramatic—the same MIDI file can sound like a toy keyboard or a real instrument depending on the samples.</p>

<p>The tool auto-detects SoundFonts in common locations (<code class="language-plaintext highlighter-rouge">~/.soundfonts/</code>, <code class="language-plaintext highlighter-rouge">/opt/homebrew/share/soundfonts/</code>, etc.), or specify one explicitly with <code class="language-plaintext highlighter-rouge">--soundfont</code>.</p>

<h2 id="technical-details">Technical Details</h2>

<p>Built with Rust 2024 edition using permissively licensed dependencies:</p>

<table>
  <thead>
    <tr>
      <th>Crate</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>midly</td>
      <td>MIDI file generation</td>
    </tr>
    <tr>
      <td>clap</td>
      <td>CLI argument parsing</td>
    </tr>
    <tr>
      <td>serde</td>
      <td>JSON serialization</td>
    </tr>
    <tr>
      <td>rand</td>
      <td>Randomization for presets</td>
    </tr>
    <tr>
      <td>axum</td>
      <td>Web server (for <code class="language-plaintext highlighter-rouge">serve</code> command)</td>
    </tr>
  </tbody>
</table>

<p>FluidSynth is called as a subprocess for WAV rendering, keeping the main codebase MIT-licensed.</p>

<h2 id="try-it">Try It</h2>

<p>Listen to <a href="https://softwarewrighter.github.io/midi-cli-rs/">sample outputs</a>, or build locally:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/softwarewrighter/midi-cli-rs.git
<span class="nb">cd </span>midi-cli-rs
cargo build <span class="nt">--release</span>
./target/release/midi-cli-rs preset <span class="nt">-m</span> jazz <span class="nt">-d</span> 5 <span class="nt">-o</span> jazz.wav
</code></pre></div></div>

<p>Requires FluidSynth for WAV output (<code class="language-plaintext highlighter-rouge">brew install fluid-synth</code> on macOS).</p>

<hr />

<p><em>Music generation shouldn’t require a music degree. With mood presets, AI agents can add audio to their creative toolkit.</em></p>


          </div>





<div class="youtube-embed-container" id="yt-container-nDNcbKE8KtE">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-nDNcbKE8KtE"
      src="https://www.youtube.com/embed/nDNcbKE8KtE?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-nDNcbKE8KtE';
  const playerId = 'yt-player-nDNcbKE8KtE';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="7"><p class="post-meta">February 20, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">494 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Benchmark Leakage (test data contamination), Concept vs Data Drift (changed relationships vs inputs), Weight Decay (L2 penalty for simplicity), Scaling Laws (predictable performance growth), Shadow Deployment (test alongside production).</div><h3>
          <a class="post-link" href="/2026/02/20/five-ml-concepts-17/">
            Five ML Concepts - #17
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-17">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-seventeen.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/Xk2hkc0bgi4">Five ML Concepts #17</a><br /><a href="https://www.youtube.com/shorts/Xk2hkc0bgi4"><img src="https://img.youtube.com/vi/Xk2hkc0bgi4/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Benchmark Leakage</strong></td>
        <td><a href="https://arxiv.org/abs/1512.00567">Rethinking the Inception Architecture for Computer Vision</a> (Szegedy et al. 2016)</td>
      </tr>
      <tr>
        <td><strong>Concept/Data Drift</strong></td>
        <td><a href="https://ieeexplore.ieee.org/document/8496795">Learning under Concept Drift: A Review</a> (Lu et al. 2018)</td>
      </tr>
      <tr>
        <td><strong>Weight Decay</strong></td>
        <td><a href="https://arxiv.org/abs/1711.05101">Decoupled Weight Decay Regularization</a> (Loshchilov &amp; Hutter 2019)</td>
      </tr>
      <tr>
        <td><strong>Scaling Laws</strong></td>
        <td><a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a> (Kaplan et al. 2020)</td>
      </tr>
      <tr>
        <td><strong>Shadow Deployment</strong></td>
        <td><a href="https://www.oreilly.com/library/view/reliable-machine-learning/9781098106218/">Reliable Machine Learning</a> (Cathy Chen et al. 2022)</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-benchmark-leakage">1. Benchmark Leakage</h3>

<p><strong>When benchmark or test data influences training, tuning, or model selection, evaluation results become unreliable.</strong> This inflates reported performance beyond real-world capability.</p>

<p>Strict separation between development and evaluation data is essential for honest assessment.</p>

<blockquote>
  <p>Like practicing with the exact questions that will appear on the final exam.</p>
</blockquote>

<h3 id="2-concept-drift-vs-data-drift">2. Concept Drift vs Data Drift</h3>

<p><strong>Data drift occurs when input distributions change. Concept drift occurs when the relationship between inputs and outputs changes.</strong> Both can degrade model performance over time.</p>

<p>Data drift: customers buy different products. Concept drift: what “good” means has changed.</p>

<blockquote>
  <p>Like customers buying different products versus products changing what they mean.</p>
</blockquote>

<h3 id="3-weight-decay">3. Weight Decay</h3>

<p><strong>A regularization method that penalizes large weights, often implemented as L2 regularization.</strong> This encourages simpler models that generalize better.</p>

<p>Weight decay adds a term proportional to the squared magnitude of weights to the loss function.</p>

<blockquote>
  <p>Like encouraging shorter, simpler answers instead of overly complicated ones.</p>
</blockquote>

<h3 id="4-scaling-laws">4. Scaling Laws</h3>

<p><strong>Empirical relationships showing how performance tends to improve as model size, data, or compute increase.</strong> These relationships follow predictable power-law curves.</p>

<p>Scaling laws help predict resource requirements for target performance levels.</p>

<blockquote>
  <p>Like noticing that adding horsepower often increases a car’s speed, but with diminishing returns.</p>
</blockquote>

<h3 id="5-shadow-deployment">5. Shadow Deployment</h3>

<p><strong>Running a new model in parallel with production without affecting live user decisions.</strong> The shadow model processes real traffic but its outputs are only logged, not served.</p>

<p>This allows safe evaluation before full deployment.</p>

<blockquote>
  <p>Like a new chef preparing the same dishes in the back kitchen before serving customers.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Benchmark Leakage</strong></td>
      <td>Test data contaminating training/selection</td>
    </tr>
    <tr>
      <td><strong>Concept vs Data Drift</strong></td>
      <td>Changed relationships vs changed inputs</td>
    </tr>
    <tr>
      <td><strong>Weight Decay</strong></td>
      <td>L2 penalty discourages large weights</td>
    </tr>
    <tr>
      <td><strong>Scaling Laws</strong></td>
      <td>Performance scales predictably with resources</td>
    </tr>
    <tr>
      <td><strong>Shadow Deployment</strong></td>
      <td>Test safely alongside production</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 17 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/21/five-ml-concepts-18/">Next: #18 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>


          </div>





<div class="youtube-embed-container" id="yt-container-Xk2hkc0bgi4">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-Xk2hkc0bgi4"
      src="https://www.youtube.com/embed/Xk2hkc0bgi4?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-Xk2hkc0bgi4';
  const playerId = 'yt-player-Xk2hkc0bgi4';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="8"><p class="post-meta">February 19, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">1080 words</span> &bull; <span class="post-read-time">6 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">ToonTalk is a 1995 visual programming environment where you train robots by showing them what to do. I vibe coded tt-rs, a Rust/WebAssembly reimplementation with boxes, scales, birds, nests, and robots---programming by demonstration for the browser.</div><h3>
          <a class="post-link" href="/2026/02/19/tbt-toontalk-visual-programming/">
            TBT (4/?): ToonTalk - Teaching Robots to Program
          </a>
        </h3><nav class="toc" data-toc-id="tbt-toontalk-visual-programming">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-robot.png" class="post-marker" alt="" /></p>

<p>I first discovered ToonTalk during the Windows XP era—probably around 2003 or 2004. It was unlike anything I’d seen: a programming environment disguised as a video game where you trained robots by showing them what to do. The concept stuck with me for two decades.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://youtu.be/qrcWMOfHN2s">ToonTalk in Rust</a><br /><a href="https://youtu.be/qrcWMOfHN2s"><img src="https://img.youtube.com/vi/qrcWMOfHN2s/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>tt-rs Demo</strong></td>
        <td><a href="https://sw-fun.github.io/tt-rs/">Live Demo</a></td>
      </tr>
      <tr>
        <td><strong>tt-rs Repo</strong></td>
        <td><a href="https://github.com/sw-fun/tt-rs">tt-rs</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="what-is-toontalk">What is ToonTalk?</h2>

<p>ToonTalk is a visual programming environment created by <a href="https://toontalk.com/English/kenkahn.htm">Ken Kahn</a> in 1995. The “Toon” stands for cartoon—every abstract programming concept is mapped to a concrete, animated metaphor:</p>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>ToonTalk Metaphor</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Variables</td>
      <td>Boxes with numbered holes</td>
    </tr>
    <tr>
      <td>Values</td>
      <td>Numbers, text, images in boxes</td>
    </tr>
    <tr>
      <td>Comparison</td>
      <td>Scales that tip when values differ</td>
    </tr>
    <tr>
      <td>Functions</td>
      <td>Robots that watch and learn</td>
    </tr>
    <tr>
      <td>Message passing</td>
      <td>Birds that carry items to nests</td>
    </tr>
    <tr>
      <td>Garbage collection</td>
      <td>Trucks that haul away unused items</td>
    </tr>
  </tbody>
</table>

<p>The design was influenced by games like <em>The Legend of Zelda</em> and <em>Robot Odyssey</em>—the kind of games that made you think while you played.</p>

<h2 id="programming-by-demonstration">Programming by Demonstration</h2>

<p>The core idea is radical: you don’t write code, you <em>show</em> a robot what to do.</p>

<ol>
  <li>Create a robot and put it in “training mode”</li>
  <li>Perform actions while the robot watches (move items, compare values, etc.)</li>
  <li>The robot records your actions as a program</li>
  <li>Give the robot a box matching the training pattern—it executes the learned behavior</li>
</ol>

<p>This is programming by demonstration. The robot generalizes from your example, matching patterns and applying transformations. It’s the same conceptual model as teaching a child: “Watch what I do, then you try.”</p>

<h2 id="three-generations">Three Generations</h2>

<p>ToonTalk has existed in three forms:</p>

<table>
  <thead>
    <tr>
      <th>Version</th>
      <th>Era</th>
      <th>Technology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Original ToonTalk</strong></td>
      <td>1995-2009</td>
      <td>C++, 3D desktop application</td>
    </tr>
    <tr>
      <td><strong>ToonTalk Reborn</strong></td>
      <td>2014-2017</td>
      <td>JavaScript/jQuery web app</td>
    </tr>
    <tr>
      <td><strong>tt-rs</strong></td>
      <td>2025-2026</td>
      <td>Rust/WebAssembly/Yew</td>
    </tr>
  </tbody>
</table>

<p>The original was a full 3D world—cities, houses, helicopters, even bombs for debugging. Ken Kahn later created <a href="https://github.com/ToonTalk/ToonTalk">ToonTalk Reborn</a>, a simplified JavaScript version that runs in browsers.</p>

<h2 id="why-i-built-tt-rs">Why I Built tt-rs</h2>

<p>When I rediscovered ToonTalk Reborn a few years ago, I wanted to experiment with the concepts myself. But diving into a large jQuery codebase wasn’t appealing. So I did what any reasonable person would do: I vibe coded my own version in Rust.</p>

<p><strong>tt-rs</strong> is a modern reimplementation using:</p>

<ul>
  <li><strong>Rust</strong> for core logic</li>
  <li><strong>WebAssembly</strong> for browser execution</li>
  <li><strong>Yew</strong> for reactive UI</li>
  <li><strong>SVG/CSS</strong> for graphics and animations</li>
</ul>

<p>It’s not a port—it’s a fresh implementation inspired by the same ideas. Building it myself lets me understand the concepts deeply and experiment with variations.</p>

<h2 id="three-learning-levels">Three Learning Levels</h2>

<p>The demo introduces concepts progressively through three levels:</p>

<table>
  <thead>
    <tr>
      <th>Level</th>
      <th>Concepts</th>
      <th>Widgets</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>tt1</strong></td>
      <td>Basics</td>
      <td>Numbers, boxes, scales, wand, vacuum</td>
    </tr>
    <tr>
      <td><strong>tt2</strong></td>
      <td>Messaging</td>
      <td>Birds and nests for communication</td>
    </tr>
    <tr>
      <td><strong>tt3</strong></td>
      <td>Automation</td>
      <td>Sensors (time, random) + robots</td>
    </tr>
  </tbody>
</table>

<p>Level one covers the fundamentals: numbers with arithmetic, boxes as containers, scales for comparison, and tools for copying and removing. Level two adds asynchronous messaging—birds carry items to their paired nests. Level three brings sensors that produce values and robots that automate actions.</p>

<h2 id="current-features">Current Features</h2>

<p>The <a href="https://sw-fun.github.io/tt-rs/">live demo</a> includes:</p>

<p><strong>Widgets:</strong></p>
<ul>
  <li><strong>Numbers</strong>: Rational arithmetic with +, -, *, / operators</li>
  <li><strong>Boxes</strong>: Configurable containers with 0-9 holes (resize with keyboard)</li>
  <li><strong>Text</strong>: Basic text display</li>
  <li><strong>Scales</strong>: Visual comparison that tips when values differ</li>
  <li><strong>Robot</strong>: Training mode, action recording, execution</li>
  <li><strong>Bird/Nest</strong>: Message passing with pairing and delivery</li>
  <li><strong>Sensors</strong>: Time (milliseconds) and random number generation</li>
</ul>

<p><strong>Tools:</strong></p>
<ul>
  <li><strong>Wand</strong>: Copy any widget</li>
  <li><strong>Vacuum</strong>: Remove widgets</li>
  <li><strong>Magnifier</strong>: Inspect nest message queues and robot actions</li>
</ul>

<p><strong>Interactions:</strong></p>
<ul>
  <li>Drag-and-drop with visual feedback</li>
  <li>Box joining (drop box on edge of another)</li>
  <li>Box splitting (drop box on a number)</li>
  <li>Contextual help panel with level-specific content</li>
  <li>Puzzle system with animated “Show Me” demos</li>
</ul>

<h2 id="robot-training">Robot Training</h2>

<p>The core feature is programming by demonstration:</p>

<ol>
  <li><strong>Click robot</strong> to enter training mode (yellow glow indicates “I’m watching”)</li>
  <li><strong>Perform actions</strong> while the robot records (arithmetic, copy, remove, move to box)</li>
  <li><strong>Click robot</strong> again to stop training</li>
  <li><strong>Click robot</strong> to replay—it executes the recorded sequence</li>
</ol>

<p>The tutorials demonstrate this workflow step by step. In the “Train Robot” tutorial, you teach a robot to move a number into a box. In “Robot Sensors,” you train a robot to generate random numbers, apply modulo, and send results to a nest via a bird.</p>

<h2 id="interactive-tutorials">Interactive Tutorials</h2>

<p>Each tutorial has two parts:</p>

<ol>
  <li><strong>Show Me</strong>: Watch an animated demonstration where a cursor walks through the solution</li>
  <li><strong>Practice</strong>: Try it yourself with the same widgets</li>
</ol>

<p>The tutorials cover:</p>
<ul>
  <li>Fill a box with numbers</li>
  <li>Add numbers together</li>
  <li>Copy widgets with the wand</li>
  <li>Send messages with birds and nests</li>
  <li>Train your first robot</li>
  <li>Combine robots with sensors</li>
</ul>

<h2 id="whats-next">What’s Next</h2>

<p>The immediate priorities:</p>

<ol>
  <li><strong>Pattern matching</strong> - Robot generalizes from specific values to “any number”</li>
  <li><strong>Watched execution</strong> - See robot work step-by-step with animated cursor</li>
  <li><strong>Persistence</strong> - Save and load workspaces</li>
</ol>

<p>Long term, I’d like to add the 3D elements from the original—the cities, the houses, the helicopter view. But that’s a much larger project.</p>

<h2 id="the-enduring-appeal">The Enduring Appeal</h2>

<p>What makes ToonTalk fascinating isn’t just the visual metaphors—it’s the <em>computational model</em>. Under the hood, ToonTalk implements concurrent constraint logic programming. The robots are essentially guarded Horn clauses. The birds and nests implement the actor model.</p>

<p>Heavy concepts, but you don’t need to know any of that to use it. You just train robots by example. The abstraction is complete.</p>

<p>That’s why it stuck with me for twenty years. Good abstractions are rare. When you find one, it’s worth understanding deeply.</p>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>ToonTalk Website</strong></td>
        <td><a href="https://www.toontalk.com/">toontalk.com</a></td>
      </tr>
      <tr>
        <td><strong>ToonTalk on Wikipedia</strong></td>
        <td><a href="https://en.wikipedia.org/wiki/ToonTalk">Wikipedia</a></td>
      </tr>
      <tr>
        <td><strong>ToonTalk Reborn (JS)</strong></td>
        <td><a href="https://github.com/ToonTalk/ToonTalk">github.com/ToonTalk/ToonTalk</a></td>
      </tr>
      <tr>
        <td><strong>ToonTalk Reborn Demo</strong></td>
        <td><a href="https://toontalk.github.io/ToonTalk/">toontalk.github.io/ToonTalk</a></td>
      </tr>
      <tr>
        <td><strong>ToonTalk Reborn Wiki</strong></td>
        <td><a href="https://github.com/ToonTalk/ToonTalk/wiki">Wiki</a></td>
      </tr>
      <tr>
        <td><strong>Ken Kahn’s Page</strong></td>
        <td><a href="https://toontalk.com/English/kenkahn.htm">Ken Kahn</a></td>
      </tr>
      <tr>
        <td><strong>Original Paper (1995)</strong></td>
        <td><a href="https://eric.ed.gov/?id=ED392435">ERIC - ToonTalk: An Animated Programming Environment</a></td>
      </tr>
      <tr>
        <td><strong>Ken Kahn’s Research</strong></td>
        <td><a href="https://www.academia.edu/2795458/ToonTalk_and_Logo">Academia.edu</a></td>
      </tr>
    </tbody>
  </table>

</div>

<hr />

<p><em>Part 4 of the Throwback Thursday series. <a href="/series/#throwback-thursday">View all parts</a></em></p>

<p><em>Some ideas are worth rediscovering. ToonTalk is one of them.</em></p>


          </div>





<div class="youtube-embed-container" id="yt-container-qrcWMOfHN2s">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-qrcWMOfHN2s"
      src="https://www.youtube.com/embed/qrcWMOfHN2s?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-qrcWMOfHN2s';
  const playerId = 'yt-player-qrcWMOfHN2s';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="9"><p class="post-meta">February 19, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">490 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Train/Val/Test Split (separate data roles), Overconfidence (high probability wrong predictions), Batch Normalization (stable training), Optimization vs Generalization (low train loss doesn't mean good test), A/B Testing (compare with experiments).</div><h3>
          <a class="post-link" href="/2026/02/19/five-ml-concepts-16/">
            Five ML Concepts - #16
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-16">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-sixteen.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/HdFa9C3ahkw">Five ML Concepts #16</a><br /><a href="https://www.youtube.com/shorts/HdFa9C3ahkw"><img src="https://img.youtube.com/vi/HdFa9C3ahkw/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Train/Val/Test Split</strong></td>
        <td><a href="https://www.deeplearningbook.org/">Deep Learning</a> (Goodfellow et al. 2016), Chapter 5</td>
      </tr>
      <tr>
        <td><strong>Overconfidence</strong></td>
        <td><a href="https://arxiv.org/abs/1706.04599">On Calibration of Modern Neural Networks</a> (Guo et al. 2017)</td>
      </tr>
      <tr>
        <td><strong>Batch Normalization</strong></td>
        <td><a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training</a> (Ioffe &amp; Szegedy 2015)</td>
      </tr>
      <tr>
        <td><strong>Optimization vs Generalization</strong></td>
        <td><a href="https://arxiv.org/abs/1611.03530">Understanding Deep Learning Requires Rethinking Generalization</a> (Zhang et al. 2017)</td>
      </tr>
      <tr>
        <td><strong>A/B Testing</strong></td>
        <td><a href="https://www.exp-platform.com/Documents/GusijDMKD.pdf">Controlled Experiments on the Web</a> (Kohavi et al. 2009)</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-train--validation--test-split">1. Train / Validation / Test Split</h3>

<p><strong>Data is divided into training, validation, and test sets.</strong> Training learns patterns, validation tunes hyperparameters, test evaluates final performance.</p>

<p>Never use test data for any decisions during development—it should only be touched once.</p>

<blockquote>
  <p>Like practicing on homework, checking with practice tests, then taking the real exam.</p>
</blockquote>

<h3 id="2-overconfidence">2. Overconfidence</h3>

<p><strong>Models can assign very high probabilities to incorrect predictions.</strong> This is often related to poor calibration and can be dangerous in high-stakes applications.</p>

<p>Temperature scaling and other calibration methods can help align confidence with accuracy.</p>

<blockquote>
  <p>Like a student who is absolutely certain of a wrong answer.</p>
</blockquote>

<h3 id="3-batch-normalization">3. Batch Normalization</h3>

<p><strong>Normalizes layer activations during training to improve stability and convergence.</strong> Each mini-batch’s activations are normalized to have zero mean and unit variance.</p>

<p>This reduces internal covariate shift and often allows higher learning rates.</p>

<blockquote>
  <p>Like keeping everyone on a similar pace during training so no one runs too far ahead.</p>
</blockquote>

<h3 id="4-optimization-vs-generalization">4. Optimization vs Generalization</h3>

<p><strong>Training loss can decrease while test performance does not improve.</strong> Good optimization does not guarantee good generalization.</p>

<p>A model can perfectly fit training data while failing on new examples—this is overfitting.</p>

<blockquote>
  <p>Like memorizing last year’s exam instead of understanding the subject.</p>
</blockquote>

<h3 id="5-ab-testing-models">5. A/B Testing Models</h3>

<p><strong>Comparing two model versions using controlled live traffic experiments.</strong> Users are randomly assigned to see predictions from model A or model B.</p>

<p>Statistical analysis determines which model performs better on real-world metrics.</p>

<blockquote>
  <p>Like taste-testing two recipes with real customers to see which works better.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Train/Val/Test</strong></td>
      <td>Separate data for learning, tuning, and evaluation</td>
    </tr>
    <tr>
      <td><strong>Overconfidence</strong></td>
      <td>High probability on wrong predictions</td>
    </tr>
    <tr>
      <td><strong>Batch Normalization</strong></td>
      <td>Normalize activations for stable training</td>
    </tr>
    <tr>
      <td><strong>Optimization vs Generalization</strong></td>
      <td>Low train loss ≠ good test performance</td>
    </tr>
    <tr>
      <td><strong>A/B Testing</strong></td>
      <td>Compare models with live experiments</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 16 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/20/five-ml-concepts-17/">Next: #17 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>


          </div>





<div class="youtube-embed-container" id="yt-container-HdFa9C3ahkw">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-HdFa9C3ahkw"
      src="https://www.youtube.com/embed/HdFa9C3ahkw?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-HdFa9C3ahkw';
  const playerId = 'yt-player-HdFa9C3ahkw';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="10"><p class="post-meta">February 18, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">809 words</span> &bull; <span class="post-read-time">5 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">RSFT on easy examples made performance worse---27% vs 37% SFT baseline. Training distribution must match evaluation distribution. Easy examples teach shortcuts that fail on hard problems. The fix is one flag change.</div><h3>
          <a class="post-link" href="/2026/02/18/multi-hop-reasoning-distribution-trap/">
            Multi-Hop Reasoning (2/2): The Distribution Trap
          </a>
        </h3><nav class="toc" data-toc-id="multi-hop-reasoning-distribution-trap">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-roos.png" class="post-marker" alt="" /></p>

<p>In <a href="/2026/02/01/multi-hop-reasoning/">Part 1</a>, a tiny 135M model achieved 75% accuracy on multi-hop reasoning. This time we scale up to 360M—and discover that <strong>RSFT on easy examples makes performance worse</strong>.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2601.15160">KG-Guided RAG (arXiv)</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/multi-hop-reasoning">multi-hop-reasoning</a></td>
      </tr>
      <tr>
        <td><strong>ELI5</strong></td>
        <td><a href="https://github.com/softwarewrighter/multi-hop-reasoning/blob/main/documentation/eli5.md">eli5.md</a></td>
      </tr>
      <tr>
        <td><strong>Demo</strong></td>
        <td><a href="https://softwarewrighter.github.io/multi-hop-reasoning/">Live Demo</a></td>
      </tr>
      <tr>
        <td><strong>Explainer</strong></td>
        <td>Coming soon</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="scaling-up-smollm-360m">Scaling Up: SmolLM-360M</h2>

<p>Part 1 used the 135M model. For better reasoning traces and demo quality, we trained the 360M variant:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Parameters</th>
      <th>Platform</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SmolLM-135M-Instruct</td>
      <td>135M</td>
      <td>MLX (macOS)</td>
    </tr>
    <tr>
      <td>SmolLM-360M-Instruct</td>
      <td>360M</td>
      <td>MLX + Unsloth (cross-platform)</td>
    </tr>
  </tbody>
</table>

<p>The 360M model produces more coherent traces and is used by the live inference demo.</p>

<h2 id="the-distribution-trap">The Distribution Trap</h2>

<p>Here’s what happened when we trained RSFT on the “easy” training data:</p>

<table>
  <thead>
    <tr>
      <th>Phase</th>
      <th>Training Data</th>
      <th>Accuracy</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base</td>
      <td>—</td>
      <td>0%</td>
      <td>No format compliance</td>
    </tr>
    <tr>
      <td>SFT (500 iters)</td>
      <td>Easy (1-3 hop)</td>
      <td>37%</td>
      <td>Learns TRACE + ANSWER format</td>
    </tr>
    <tr>
      <td><strong>RSFT</strong></td>
      <td><strong>Easy (1-3 hop)</strong></td>
      <td><strong>27%</strong></td>
      <td><strong>Worse than SFT!</strong></td>
    </tr>
  </tbody>
</table>

<p>RSFT on easy examples performed <em>worse</em> than the SFT baseline.</p>

<h3 id="why">Why?</h3>

<p>The training examples (1-3 hops) don’t match the evaluation distribution (4-5 hops). The model learns shortcuts that work on easy problems but fail on hard ones.</p>

<table>
  <thead>
    <tr>
      <th>Training Distribution</th>
      <th>Eval Distribution</th>
      <th>Result</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Easy (1-3 hop)</td>
      <td>Hard (4-5 hop)</td>
      <td>27% (worse)</td>
    </tr>
    <tr>
      <td>Hard (4-5 hop)</td>
      <td>Hard (4-5 hop)</td>
      <td><strong>75%</strong> (Part 1 result)</td>
    </tr>
  </tbody>
</table>

<p>The rejection sampling “winners” from easy examples teach strategies that don’t generalize.</p>

<h2 id="the-key-finding">The Key Finding</h2>

<p><strong>Rejection sampling must match your target distribution.</strong></p>

<p>This is counterintuitive. You might expect that training on more examples (even easy ones) would help. Instead:</p>

<ul>
  <li>Easy winners use shortcuts (fewer reasoning steps)</li>
  <li>Hard eval requires full chain reasoning</li>
  <li>Model learns the wrong patterns</li>
</ul>

<p>The fix: train RSFT on <code class="language-plaintext highlighter-rouge">eval.jsonl</code> (hard examples), not <code class="language-plaintext highlighter-rouge">train.jsonl</code> (easy examples).</p>

<h2 id="demo-improvements">Demo Improvements</h2>

<p>The demo now includes four interactive tabs:</p>

<table>
  <thead>
    <tr>
      <th>Tab</th>
      <th>Feature</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Training</strong></td>
      <td>Animated SFT→RSFT visualization with KG scoring</td>
    </tr>
    <tr>
      <td><strong>Inference</strong></td>
      <td>Pre-recorded inference examples</td>
    </tr>
    <tr>
      <td><strong>Try It</strong></td>
      <td>Live inference with 360M model</td>
    </tr>
    <tr>
      <td><strong>Distribution</strong></td>
      <td>Interactive visualization of the key finding</td>
    </tr>
  </tbody>
</table>

<h3 id="try-it-live-inference">Try It: Live Inference</h3>

<p>Ask DevOps troubleshooting questions and watch the model reason:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Question: What causes TLSHandshakeError?

TRACE: TLSHandshakeError is caused by ClockSkew,
and ClockSkew leads to CertificateExpired,
and CertificateExpired is fixed by RenewCert...
ANSWER: B
</code></pre></div></div>

<p>The knowledge graph scores the reasoning path during training, but at inference the model reasons independently.</p>

<h2 id="cross-platform-support">Cross-Platform Support</h2>

<p>The pipeline now runs on both platforms:</p>

<table>
  <thead>
    <tr>
      <th>Platform</th>
      <th>Framework</th>
      <th>Command</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>macOS (Apple Silicon)</td>
      <td>MLX</td>
      <td><code class="language-plaintext highlighter-rouge">make train-360m</code></td>
    </tr>
    <tr>
      <td>Linux (NVIDIA CUDA)</td>
      <td>Unsloth</td>
      <td><code class="language-plaintext highlighter-rouge">make train-360m-unsloth</code></td>
    </tr>
  </tbody>
</table>

<p>Unsloth provides 2x faster training with 60% less memory on NVIDIA GPUs.</p>

<h2 id="current-status">Current Status</h2>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Status</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>SFT training (360M)</td>
      <td>Complete</td>
    </tr>
    <tr>
      <td>RSFT (wrong distribution)</td>
      <td>Complete (27%)</td>
    </tr>
    <tr>
      <td>RSFT (correct distribution)</td>
      <td><strong>Next step</strong></td>
    </tr>
    <tr>
      <td>Live demo with Try It</td>
      <td>Complete</td>
    </tr>
    <tr>
      <td>Cross-platform support</td>
      <td>Complete</td>
    </tr>
  </tbody>
</table>

<h2 id="next-steps">Next Steps</h2>

<table>
  <thead>
    <tr>
      <th>Priority</th>
      <th>Task</th>
      <th>Expected Result</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>High</strong></td>
      <td>Retrain RSFT on eval.jsonl</td>
      <td>75%+ accuracy</td>
    </tr>
    <tr>
      <td>Medium</td>
      <td>Update demo to use corrected model</td>
      <td>Better live inference</td>
    </tr>
    <tr>
      <td>Medium</td>
      <td>Curriculum learning (easy→hard)</td>
      <td>Smoother training</td>
    </tr>
    <tr>
      <td>Low</td>
      <td>Larger models (1B+)</td>
      <td>Higher ceiling</td>
    </tr>
  </tbody>
</table>

<p>The corrected RSFT training:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">-m</span> core.rsft <span class="se">\</span>
  <span class="nt">--examples</span> data/eval.jsonl <span class="se">\ </span> <span class="c"># Hard examples!</span>
  <span class="nt">--kg</span> data/kg.json <span class="se">\</span>
  <span class="nt">--sft-adapter</span> data/runs/run_360m/models/sft <span class="se">\</span>
  <span class="nt">--output</span> data/runs/run_360m/models/rsft_eval <span class="se">\</span>
  <span class="nt">--model</span> HuggingFaceTB/SmolLM-360M-Instruct <span class="se">\</span>
  <span class="nt">--k-samples</span> 8 <span class="se">\</span>
  <span class="nt">--max-examples</span> 50
</code></pre></div></div>

<h2 id="lessons-learned">Lessons Learned</h2>

<h3 id="1-distribution-matching-is-non-negotiable">1. Distribution Matching is Non-Negotiable</h3>

<p>This isn’t a minor optimization—it’s the difference between 27% and 75% accuracy. Wrong distribution = wrong winners = wrong model.</p>

<h3 id="2-easy-examples-can-hurt">2. Easy Examples Can Hurt</h3>

<p>More training data isn’t always better. Easy examples teach shortcuts that fail on hard problems.</p>

<h3 id="3-verify-your-pipeline">3. Verify Your Pipeline</h3>

<p>We trained a full RSFT model before realizing the distribution mismatch. Always check that training data matches eval distribution.</p>

<h3 id="4-the-fix-is-simple">4. The Fix is Simple</h3>

<p>Once identified, the fix is one flag change: <code class="language-plaintext highlighter-rouge">--examples data/eval.jsonl</code> instead of <code class="language-plaintext highlighter-rouge">train.jsonl</code>.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://github.com/softwarewrighter/multi-hop-reasoning">Repository: multi-hop-reasoning</a></li>
  <li><a href="https://softwarewrighter.github.io/multi-hop-reasoning/">Live Demo</a></li>
  <li><a href="/2026/02/01/multi-hop-reasoning/">Part 1: Training Wheels for Small LLMs</a></li>
  <li><a href="https://arxiv.org/abs/2601.15160">Paper: Knowledge Graph-Guided RAG</a></li>
  <li><a href="https://github.com/softwarewrighter/multi-hop-reasoning/blob/main/documentation/training-status.md">Training Status</a></li>
</ul>

<hr />

<p><em>Part 2 of 2 in the Multi-Hop Reasoning series. <a href="/series/#multi-hop-reasoning">View all parts</a></em></p>

<p><em>Training distribution matters. Easy examples teach easy shortcuts.</em></p>


          </div><img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="11"><p class="post-meta">February 18, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">788 words</span> &bull; <span class="post-read-time">4 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Part 2 of implementing the Share algorithm: after fixing critical bugs (zero-gradient saddle point, half-parameter training), routing-based coefficient selection achieves zero regressions. Result handling improved 40% to 50%. We're 60% through verifying the paper's claims.</div><h3>
          <a class="post-link" href="/2026/02/18/sleepy-coder-routing-prevents-forgetting/">
            Towards Continuous LLM Learning (2): Routing Prevents Forgetting
          </a>
        </h3><nav class="toc" data-toc-id="sleepy-coder-routing-prevents-forgetting">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-ink-bottle.png" class="post-marker" alt="" /></p>

<p>In <a href="/2026/02/12/sleepy-coder-when-fine-tuning-fails/">Part 1</a>, naive LoRA fine-tuning caused catastrophic forgetting. Now we’re implementing the Share algorithm properly—and we’re about 60% of the way to verifying the paper’s claims.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/sleepy-coder">sleepy-coder</a></td>
      </tr>
      <tr>
        <td><strong>Part 1</strong></td>
        <td><a href="/2026/02/12/sleepy-coder-when-fine-tuning-fails/">When Fine-Tuning Fails</a></td>
      </tr>
      <tr>
        <td><strong>ELI5</strong></td>
        <td><a href="https://github.com/softwarewrighter/sleepy-coder/blob/main/docs/eli5.md">eli5.md</a></td>
      </tr>
      <tr>
        <td><strong>Share Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2602.06043">arXiv:2602.06043</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="paper-claims-vs-implementation-status">Paper Claims vs Implementation Status</h2>

<p>We’re systematically verifying the claims from the Share and UWSH papers:</p>

<table>
  <thead>
    <tr>
      <th>Paper Claim</th>
      <th>Infrastructure</th>
      <th>Demonstrated?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Shared basis via SVD</td>
      <td>Complete</td>
      <td><strong>Yes</strong></td>
    </tr>
    <tr>
      <td>~100x parameter reduction</td>
      <td>Complete (76x)</td>
      <td><strong>Yes</strong></td>
    </tr>
    <tr>
      <td>Task routing beats averaging</td>
      <td>Tested (Exp 1b)</td>
      <td><strong>Partial</strong></td>
    </tr>
    <tr>
      <td>Prevents catastrophic forgetting</td>
      <td>Tested (Exp 1b)</td>
      <td><strong>Partial</strong></td>
    </tr>
    <tr>
      <td>Sequential learning</td>
      <td>Not tested</td>
      <td><strong>No</strong></td>
    </tr>
    <tr>
      <td>UWSH subspace stability</td>
      <td>Not tested</td>
      <td><strong>No</strong></td>
    </tr>
  </tbody>
</table>

<p><strong>Overall: ~60% complete.</strong> Infrastructure is solid. Routing tested. Sequential learning remains.</p>

<h2 id="what-we-built">What We Built</h2>

<p>The full Share algorithm implementation:</p>

<ul>
  <li><strong>Phase 1</strong>: SVD-based subspace extraction from 51 LoRA adapters (60% variance threshold)</li>
  <li><strong>Phase 2</strong>: Coefficient-only training with frozen basis (83K params vs 1.6M full LoRA)</li>
  <li><strong>Phase 3</strong>: Basis merging and updates</li>
  <li><strong>Routing</strong>: Error pattern classification for coefficient selection</li>
</ul>

<h2 id="bug-fixes-that-unlocked-progress">Bug Fixes That Unlocked Progress</h2>

<p>Two critical bugs blocked proper Phase 2 training:</p>

<h3 id="bug-1-zero-gradient-saddle-point">Bug 1: Zero-Gradient Saddle Point</h3>

<p>Both coefficient matrices initialized to zero:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>eps_beta = 0, eps_alpha = 0
→ delta_W = 0 @ 0 = 0
→ zero gradients, no learning
</code></pre></div></div>

<p><strong>Fix</strong>: Dual small-random initialization.</p>

<h3 id="bug-2-half-parameter-training">Bug 2: Half-Parameter Training</h3>

<p>LoRA-style initialization only trained one coefficient set:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Before: 112/224 parameters getting gradients
After:  224/224 parameters getting gradients
</code></pre></div></div>

<p><strong>Fix</strong>: Both coefficient matrices need random initialization.</p>

<h2 id="experiment-1b-routing-works">Experiment 1b: Routing Works</h2>

<p>With gradient-trained v4 coefficients and proper routing:</p>

<table>
  <thead>
    <tr>
      <th>Strategy</th>
      <th>Pass Rate</th>
      <th>BC</th>
      <th>RH</th>
      <th>TB</th>
      <th>Regressions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline (no LoRA)</td>
      <td>46.7%</td>
      <td>70%</td>
      <td>40%</td>
      <td>30%</td>
      <td>–</td>
    </tr>
    <tr>
      <td>Averaged</td>
      <td>50.0%</td>
      <td>70%</td>
      <td>40%</td>
      <td>40%</td>
      <td>1</td>
    </tr>
    <tr>
      <td><strong>Routed</strong></td>
      <td><strong>50.0%</strong></td>
      <td><strong>70%</strong></td>
      <td><strong>50%</strong></td>
      <td><strong>30%</strong></td>
      <td><strong>0</strong></td>
    </tr>
  </tbody>
</table>

<p><strong>Result handling improved 40% → 50%.</strong> Zero regressions. This is the first positive transfer from Share coefficients.</p>

<h2 id="the-forgetting-heatmap">The Forgetting Heatmap</h2>

<p>We applied each coefficient <strong>individually to all 30 koans</strong>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Koan       BL  mut_bc dbl_mt ret_lr mis_cl mis_hs mis_or opt_ok res_me ROUTED
bc_001-009 P   P      P      P      P      P      P      P      P      P
bc_003,5,10.   .      .      .      .      .      .      .      .      .
rh_002     .   .     +GAIN   .      .     +GAIN  +GAIN  +GAIN  +GAIN  +GAIN
rh_008     P  -LOST  -LOST  -LOST  -LOST  -LOST  -LOST  -LOST  -LOST   P
tb_005     P   P      P      P      P     -LOST   P      P      P      P
</code></pre></div></div>

<p><strong>Key finding</strong>: rh_008 regresses under <em>every</em> coefficient applied globally. But routing <strong>saves it</strong> by falling back to the base model when no pattern matches.</p>

<p>This is exactly what the Share paper predicts: task-specific coefficients improve targeted patterns without interfering with unrelated ones.</p>

<h2 id="what-the-papers-claim-vs-what-weve-verified">What the Papers Claim vs What We’ve Verified</h2>

<h3 id="verified">Verified</h3>

<ol>
  <li>
    <p><strong>Shared basis via SVD</strong> — We extract principal components from 51 adapters. Works.</p>
  </li>
  <li>
    <p><strong>76x parameter reduction</strong> — 83K coefficient parameters vs 1.6M full LoRA. Verified.</p>
  </li>
  <li>
    <p><strong>Routing prevents forgetting</strong> — Zero regressions with routed inference. The fragile rh_008 koan survives because it falls back to base model.</p>
  </li>
  <li>
    <p><strong>Positive transfer possible</strong> — Result handling improved 40% → 50% with routed coefficients.</p>
  </li>
</ol>

<h3 id="not-yet-verified">Not Yet Verified</h3>

<ol>
  <li>
    <p><strong>Sequential learning</strong> — The core continual learning claim. Train task 1 → eval → train task 2 → eval (verify task 1 still passes). This is next.</p>
  </li>
  <li>
    <p><strong>UWSH subspace stability</strong> — Do different adapter subsets converge to similar subspaces? Grassmann distance measurement needed.</p>
  </li>
</ol>

<h2 id="next-experiments">Next Experiments</h2>

<table>
  <thead>
    <tr>
      <th>Priority</th>
      <th>Experiment</th>
      <th>Target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>High</strong></td>
      <td>Sequential learning curve</td>
      <td>No degradation on prior tasks</td>
    </tr>
    <tr>
      <td><strong>High</strong></td>
      <td>Fix k_alpha=32 (paper recommends)</td>
      <td>Match paper exactly</td>
    </tr>
    <tr>
      <td>Medium</td>
      <td>UWSH verification</td>
      <td>&gt;70% subspace overlap</td>
    </tr>
    <tr>
      <td>Medium</td>
      <td>Add rank update vectors</td>
      <td>Full algorithm</td>
    </tr>
  </tbody>
</table>

<h2 id="the-architecture">The Architecture</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Day:   Agent attempts to fix Rust errors
       ↓
       Successes and failures logged
       ↓
Night: Train coefficients (frozen basis)
       ↓
       83K params per task
       ↓
Eval:  Route to appropriate coefficients
       ↓
       Pattern-matched inference
       ↓
(repeat)
</code></pre></div></div>

<p>The key insight: <strong>train small, route smart</strong>. The shared basis captures common structure. Per-task coefficients specialize without interference.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://github.com/softwarewrighter/sleepy-coder">sleepy-coder Repository</a></li>
  <li><a href="/2026/02/12/sleepy-coder-when-fine-tuning-fails/">Part 1: When Fine-Tuning Fails</a></li>
  <li><a href="https://github.com/softwarewrighter/sleepy-coder/blob/main/docs/paper-checklists.md">Paper Checklists</a></li>
  <li><a href="https://arxiv.org/abs/2602.06043">Share Paper (arXiv:2602.06043)</a></li>
  <li><a href="https://arxiv.org/abs/2512.05117">UWSH Paper (arXiv:2512.05117)</a></li>
</ul>

<hr />

<p><em>Part 2 of the Towards Continuous LLM Learning series. <a href="/series/#towards-continuous-llm-learning">View all parts</a></em></p>

<p><em>60% of the way to verifying the papers. Sequential learning is next.</em></p>


          </div><img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="12"><p class="post-meta">February 18, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">492 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Perplexity (how surprised by data), Catastrophic Forgetting (new learning erases old), Weight Initialization (starting values matter), Curse of Dimensionality (high-D makes data sparse), Monitoring (track performance and drift).</div><h3>
          <a class="post-link" href="/2026/02/18/five-ml-concepts-15/">
            Five ML Concepts - #15
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-15">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-fifteen.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/lkviAtRMgjc">Five ML Concepts #15</a><br /><a href="https://www.youtube.com/shorts/lkviAtRMgjc"><img src="https://img.youtube.com/vi/lkviAtRMgjc/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Perplexity</strong></td>
        <td><a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a> (Bengio et al. 2003)</td>
      </tr>
      <tr>
        <td><strong>Catastrophic Forgetting</strong></td>
        <td><a href="https://arxiv.org/abs/1612.00796">Overcoming Catastrophic Forgetting in Neural Networks</a> (Kirkpatrick et al. 2017)</td>
      </tr>
      <tr>
        <td><strong>Weight Initialization</strong></td>
        <td><a href="https://proceedings.mlr.press/v9/glorot10a.html">Understanding the Difficulty of Training Deep Feedforward Neural Networks</a> (Glorot &amp; Bengio 2010)</td>
      </tr>
      <tr>
        <td><strong>Curse of Dimensionality</strong></td>
        <td><a href="https://hastie.su.domains/ElemStatLearn/">The Elements of Statistical Learning</a> (Hastie et al. 2009), Chapter 2</td>
      </tr>
      <tr>
        <td><strong>Monitoring &amp; Drift</strong></td>
        <td><a href="https://arxiv.org/abs/1810.11953">Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift</a> (Rabanser et al. 2019)</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-perplexity">1. Perplexity</h3>

<p><strong>A metric for language models that reflects how well the model predicts the next token.</strong> Lower perplexity means better predictive performance.</p>

<p>Perplexity is the exponentiated average negative log-likelihood per token.</p>

<blockquote>
  <p>Like a test where lower scores mean you found the answers easier to guess.</p>
</blockquote>

<h3 id="2-catastrophic-forgetting">2. Catastrophic Forgetting</h3>

<p><strong>When training on new tasks causes a model to lose performance on previously learned tasks.</strong> This is a key challenge in continual learning.</p>

<p>Techniques like elastic weight consolidation help preserve important weights.</p>

<blockquote>
  <p>Like learning a new phone number and forgetting the old one.</p>
</blockquote>

<h3 id="3-weight-initialization">3. Weight Initialization</h3>

<p><strong>The starting values of model weights influence how well training progresses.</strong> Poor initialization can cause vanishing or exploding gradients.</p>

<p>Xavier and He initialization are common strategies for setting initial weights appropriately.</p>

<blockquote>
  <p>Like starting a race from a good position instead of stuck in a ditch.</p>
</blockquote>

<h3 id="4-curse-of-dimensionality">4. Curse of Dimensionality</h3>

<p><strong>In high-dimensional spaces, data becomes sparse and distances behave differently, making learning harder.</strong> Points that seem close in low dimensions can be far apart in high dimensions.</p>

<p>Feature selection and dimensionality reduction help mitigate this effect.</p>

<blockquote>
  <p>Like searching for a friend in a city versus across the entire universe.</p>
</blockquote>

<h3 id="5-monitoring--drift-detection">5. Monitoring &amp; Drift Detection</h3>

<p><strong>Continuously tracking model performance and detecting shifts in input data distributions.</strong> Production models can degrade silently without proper monitoring.</p>

<p>Automated alerts help catch problems before they affect users.</p>

<blockquote>
  <p>Like a weather station alerting you when conditions change.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Perplexity</strong></td>
      <td>How surprised the model is by the data</td>
    </tr>
    <tr>
      <td><strong>Catastrophic Forgetting</strong></td>
      <td>New learning erases old knowledge</td>
    </tr>
    <tr>
      <td><strong>Weight Initialization</strong></td>
      <td>Starting values affect training stability</td>
    </tr>
    <tr>
      <td><strong>Curse of Dimensionality</strong></td>
      <td>High dimensions make data sparse</td>
    </tr>
    <tr>
      <td><strong>Monitoring &amp; Drift</strong></td>
      <td>Track performance and data changes</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 15 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/19/five-ml-concepts-16/">Next: #16 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>


          </div>





<div class="youtube-embed-container" id="yt-container-lkviAtRMgjc">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-lkviAtRMgjc"
      src="https://www.youtube.com/embed/lkviAtRMgjc?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-lkviAtRMgjc';
  const playerId = 'yt-player-lkviAtRMgjc';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="13"><p class="post-meta">February 17, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">460 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: ROC/AUC (performance across thresholds), Spurious Correlations (coincidental patterns), Gradient Clipping (limit gradients for stability), Loss Landscapes (error surface over parameters), Cold Start (no history for new users).</div><h3>
          <a class="post-link" href="/2026/02/17/five-ml-concepts-14/">
            Five ML Concepts - #14
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-14">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-fourteen.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/OOB56_OARnk">Five ML Concepts #14</a><br /><a href="https://www.youtube.com/shorts/OOB56_OARnk"><img src="https://img.youtube.com/vi/OOB56_OARnk/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>ROC/AUC</strong></td>
        <td><a href="https://www.sciencedirect.com/science/article/abs/pii/S016786550500303X">An Introduction to ROC Analysis</a> (Fawcett 2006)</td>
      </tr>
      <tr>
        <td><strong>Spurious Correlations</strong></td>
        <td><a href="https://ieeexplore.ieee.org/document/5995347">Unbiased Look at Dataset Bias</a> (Torralba &amp; Efros 2011)</td>
      </tr>
      <tr>
        <td><strong>Gradient Clipping</strong></td>
        <td><a href="https://arxiv.org/abs/1211.5063">On the Difficulty of Training Recurrent Neural Networks</a> (Pascanu et al. 2013)</td>
      </tr>
      <tr>
        <td><strong>Loss Landscapes</strong></td>
        <td><a href="https://arxiv.org/abs/1712.09913">Visualizing the Loss Landscape of Neural Nets</a> (Li et al. 2018)</td>
      </tr>
      <tr>
        <td><strong>Cold Start</strong></td>
        <td><a href="https://dl.acm.org/doi/10.1145/2365952.2365972">Addressing Cold Start in Recommender Systems</a> (Schein et al. 2002)</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-roc--auc">1. ROC / AUC</h3>

<p><strong>ROC curves plot true positive rate against false positive rate across all classification thresholds.</strong> AUC (Area Under the Curve) summarizes overall ranking performance in a single number.</p>

<p>AUC of 0.5 means random guessing; 1.0 means perfect ranking.</p>

<blockquote>
  <p>Like judging a student by considering every possible passing grade cutoff.</p>
</blockquote>

<h3 id="2-spurious-correlations">2. Spurious Correlations</h3>

<p><strong>Coincidental patterns in training data that don’t reflect true relationships.</strong> Models that rely on them can fail when the coincidence disappears.</p>

<p>Dataset curation and diverse evaluation help detect spurious features.</p>

<blockquote>
  <p>Like assuming umbrellas cause rain because you always see them together.</p>
</blockquote>

<h3 id="3-gradient-clipping">3. Gradient Clipping</h3>

<p><strong>Limiting the size of gradients during backpropagation.</strong> This helps prevent exploding gradients and unstable training, especially in recurrent networks.</p>

<p>Clipping can be by value or by global norm.</p>

<blockquote>
  <p>Like putting a speed limit on a car so it doesn’t lose control.</p>
</blockquote>

<h3 id="4-loss-landscapes">4. Loss Landscapes</h3>

<p><strong>How model error changes across different parameter settings.</strong> Training is like navigating this surface toward regions of lower loss.</p>

<p>Flat minima may generalize better than sharp ones.</p>

<blockquote>
  <p>Like hiking through mountains searching for the lowest valley, feeling the slope beneath your feet.</p>
</blockquote>

<h3 id="5-cold-start-problems">5. Cold Start Problems</h3>

<p><strong>Difficulty predicting for new users or items with no history.</strong> Without prior data, personalization becomes difficult.</p>

<p>Solutions include content-based features, popularity fallbacks, or asking initial questions.</p>

<blockquote>
  <p>Like a librarian trying to recommend books to someone who just walked in.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>ROC / AUC</strong></td>
      <td>Classifier performance across thresholds</td>
    </tr>
    <tr>
      <td><strong>Spurious Correlations</strong></td>
      <td>Coincidental patterns that don’t generalize</td>
    </tr>
    <tr>
      <td><strong>Gradient Clipping</strong></td>
      <td>Limit gradient size for stability</td>
    </tr>
    <tr>
      <td><strong>Loss Landscapes</strong></td>
      <td>Error surface over parameter space</td>
    </tr>
    <tr>
      <td><strong>Cold Start</strong></td>
      <td>No history for new users/items</td>
    </tr>
  </tbody>
</table>

<hr />

<p><em>Part 14 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></em></p>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-OOB56_OARnk">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-OOB56_OARnk"
      src="https://www.youtube.com/embed/OOB56_OARnk?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-OOB56_OARnk';
  const playerId = 'yt-player-OOB56_OARnk';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="14"><p class="post-meta">February 16, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">470 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Calibration (predicted probabilities match outcomes), Shortcut Learning (exploiting spurious patterns), Early Stopping (halt when validation plateaus), Universal Approximation (NNs can fit any function), Checkpointing (save model state).</div><h3>
          <a class="post-link" href="/2026/02/16/five-ml-concepts-13/">
            Five ML Concepts - #13
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-13">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-thirteen.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/WPAk79_UONE">Five ML Concepts #13</a><br /><a href="https://www.youtube.com/shorts/WPAk79_UONE"><img src="https://img.youtube.com/vi/WPAk79_UONE/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Calibration</strong></td>
        <td><a href="https://arxiv.org/abs/1706.04599">On Calibration of Modern Neural Networks</a> (Guo et al. 2017)</td>
      </tr>
      <tr>
        <td><strong>Shortcut Learning</strong></td>
        <td><a href="https://arxiv.org/abs/2004.07780">Shortcut Learning in Deep Neural Networks</a> (Geirhos et al. 2020)</td>
      </tr>
      <tr>
        <td><strong>Early Stopping</strong></td>
        <td><a href="https://page.mi.fu-berlin.de/prechelt/Biblio/stop_neurips98.pdf">Early Stopping - But When?</a> (Prechelt 1998)</td>
      </tr>
      <tr>
        <td><strong>Universal Approximation</strong></td>
        <td><a href="https://www.sciencedirect.com/science/article/abs/pii/0893608089900208">Approximation by Superpositions of a Sigmoidal Function</a> (Cybenko 1989)</td>
      </tr>
      <tr>
        <td><strong>Checkpointing</strong></td>
        <td><a href="https://arxiv.org/abs/1604.06174">Training Deep Nets with Sublinear Memory Cost</a> (Chen et al. 2016)</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-calibration">1. Calibration</h3>

<p><strong>How well a model’s predicted probabilities match real-world outcomes.</strong> If a model predicts 70% confidence many times, it should be correct about 70% of those cases.</p>

<p>Well-calibrated models enable better decision-making under uncertainty.</p>

<blockquote>
  <p>Like a weather forecast that predicts rain 30% of the time and is right about 30% of those forecasts.</p>
</blockquote>

<h3 id="2-shortcut-learning">2. Shortcut Learning</h3>

<p><strong>When models rely on superficial patterns instead of meaningful features.</strong> For example, identifying cows by detecting grass and failing when cows appear indoors.</p>

<p>Shortcuts can inflate benchmark scores while masking poor real-world performance.</p>

<blockquote>
  <p>Like passing a test by memorizing answer positions instead of learning the material.</p>
</blockquote>

<h3 id="3-early-stopping">3. Early Stopping</h3>

<p><strong>Training is stopped when validation performance stops improving.</strong> This helps prevent overfitting by halting before the model memorizes training data.</p>

<p>Patience hyperparameters control how long to wait before stopping.</p>

<blockquote>
  <p>Like knowing when to stop practicing before you start reinforcing mistakes.</p>
</blockquote>

<h3 id="4-universal-approximation">4. Universal Approximation</h3>

<p><strong>The theorem stating that neural networks can approximate any continuous function, given enough capacity.</strong> In practice, finding the right weights through optimization is the challenge.</p>

<p>The theorem guarantees existence, not learnability.</p>

<blockquote>
  <p>Like having enough Lego blocks to build almost any shape—assembly is still hard.</p>
</blockquote>

<h3 id="5-checkpointing">5. Checkpointing</h3>

<p><strong>Saving the model’s state during training.</strong> This allows recovery from interruptions and comparison across training stages.</p>

<p>Checkpoints also enable selecting the best model rather than just the final one.</p>

<blockquote>
  <p>Like saving your game progress so you can reload if something goes wrong.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Calibration</strong></td>
      <td>Predicted probabilities match outcomes</td>
    </tr>
    <tr>
      <td><strong>Shortcut Learning</strong></td>
      <td>Exploiting spurious patterns</td>
    </tr>
    <tr>
      <td><strong>Early Stopping</strong></td>
      <td>Stop when validation plateaus</td>
    </tr>
    <tr>
      <td><strong>Universal Approximation</strong></td>
      <td>NNs can approximate any function</td>
    </tr>
    <tr>
      <td><strong>Checkpointing</strong></td>
      <td>Save model state during training</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 13 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/17/five-ml-concepts-14/">Next: #14 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-WPAk79_UONE">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-WPAk79_UONE"
      src="https://www.youtube.com/embed/WPAk79_UONE?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-WPAk79_UONE';
  const playerId = 'yt-player-WPAk79_UONE';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="15"><p class="post-meta">February 15, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">510 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Precision vs Recall (correct positives vs finding all), OOD Inputs (data unlike training), Batch Size (examples per update), Inductive Bias (built-in assumptions), Latency vs Throughput (speed vs capacity).</div><h3>
          <a class="post-link" href="/2026/02/15/five-ml-concepts-12/">
            Five ML Concepts - #12
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-12">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-drummers.png" class="post-marker" style="width: 480px;" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/jQqyRdQAjPw">Five ML Concepts #12</a><br /><a href="https://www.youtube.com/shorts/jQqyRdQAjPw"><img src="https://img.youtube.com/vi/jQqyRdQAjPw/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Precision/Recall</strong></td>
        <td><a href="https://www.cs.odu.edu/~mukka/cs795sum09dm/Lecturenotes/Day3/F-measure-YS-26Oct07.pdf">The Truth of the F-Measure</a> (Sasaki 2007)</td>
      </tr>
      <tr>
        <td><strong>OOD Detection</strong></td>
        <td><a href="https://arxiv.org/abs/1610.02136">A Baseline for Detecting Misclassified and Out-of-Distribution Examples</a> (Hendrycks &amp; Gimpel 2017)</td>
      </tr>
      <tr>
        <td><strong>Batch Size</strong></td>
        <td><a href="https://arxiv.org/abs/1609.04836">On Large-Batch Training for Deep Learning</a> (Goyal et al. 2017)</td>
      </tr>
      <tr>
        <td><strong>Inductive Bias</strong></td>
        <td><a href="https://arxiv.org/abs/1806.01261">Relational Inductive Biases, Deep Learning, and Graph Networks</a> (Battaglia et al. 2018)</td>
      </tr>
      <tr>
        <td><strong>Latency/Throughput</strong></td>
        <td><a href="https://arxiv.org/abs/2104.04473">Efficient Large-Scale Language Model Training on GPU Clusters</a> (Narayanan et al. 2021)</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-precision-vs-recall">1. Precision vs Recall</h3>

<p><strong>Precision measures how often positive predictions are correct. Recall measures how many actual positives are successfully found.</strong> Improving one often reduces the other.</p>

<p>The tradeoff depends on your application: spam filters favor precision, medical screening favors recall.</p>

<blockquote>
  <p>Like a search party: you can find everyone but raise false alarms, or be very certain and miss some people.</p>
</blockquote>

<h3 id="2-ood-inputs-out-of-distribution">2. OOD Inputs (Out-of-Distribution)</h3>

<p><strong>Data that differs significantly from what the model saw during training.</strong> Models may fail silently or produce confident but wrong answers.</p>

<p>Detecting OOD inputs is an active research area for safer AI deployment.</p>

<blockquote>
  <p>Like asking a chef trained only in Italian food to make sushi.</p>
</blockquote>

<h3 id="3-batch-size">3. Batch Size</h3>

<p><strong>The number of training examples processed before updating model weights.</strong> Larger batches can be more efficient computationally, but may generalize worse.</p>

<p>Finding the right batch size involves balancing speed, memory, and model quality.</p>

<blockquote>
  <p>Like grading tests one at a time or waiting to grade a full stack.</p>
</blockquote>

<h3 id="4-inductive-bias">4. Inductive Bias</h3>

<p><strong>The assumptions built into a model that guide how it learns from data.</strong> Without inductive bias, models cannot generalize beyond training examples.</p>

<p>CNNs assume spatial locality; transformers assume tokens can attend to any position.</p>

<blockquote>
  <p>Like expecting nearby houses to have similar prices before looking at the data.</p>
</blockquote>

<h3 id="5-latency-vs-throughput">5. Latency vs Throughput</h3>

<p><strong>Latency is how long a single request takes. Throughput is how many requests can be handled per second.</strong> Optimizing one often comes at the expense of the other.</p>

<p>Batching improves throughput but increases latency for individual requests.</p>

<blockquote>
  <p>Like a restaurant serving one table quickly or many tables at once.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Precision vs Recall</strong></td>
      <td>Correct positives vs finding all positives</td>
    </tr>
    <tr>
      <td><strong>OOD Inputs</strong></td>
      <td>Data unlike training distribution</td>
    </tr>
    <tr>
      <td><strong>Batch Size</strong></td>
      <td>Examples per weight update</td>
    </tr>
    <tr>
      <td><strong>Inductive Bias</strong></td>
      <td>Built-in learning assumptions</td>
    </tr>
    <tr>
      <td><strong>Latency vs Throughput</strong></td>
      <td>Speed per request vs total capacity</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 12 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/16/five-ml-concepts-13/">Next: #13 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-jQqyRdQAjPw">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-jQqyRdQAjPw"
      src="https://www.youtube.com/embed/jQqyRdQAjPw?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-jQqyRdQAjPw';
  const playerId = 'yt-player-jQqyRdQAjPw';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="16"><p class="post-meta">February 14, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">1048 words</span> &bull; <span class="post-read-time">6 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Personal Software for education: a neural network platform where every step is visible---no framework magic. CLI with progress bars, web UI with real-time loss charts, WASM for browser execution. Built via Vibe Coding to watch XOR training reveal why hidden layers matter.</div><h3>
          <a class="post-link" href="/2026/02/14/neural-net-rs-educational-ml-platform/">
            Neural-Net-RS: An Educational Neural Network Platform
          </a>
        </h3><nav class="toc" data-toc-id="neural-net-rs-educational-ml-platform">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-graph.png" class="post-marker" alt="" /></p>

<p>I wanted a neural network implementation where every step is visible—no framework magic hiding the math. Something I could use to teach the fundamentals, with a CLI for quick experiments and a web UI for visual demonstrations. Claude Code built it.</p>

<p>This is <strong>Personal Software</strong> for education: a complete neural network training platform with multiple interfaces, all from a single Rust codebase.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Repo</strong></td>
        <td><a href="https://github.com/sw-ml-study/neural-net-rs">neural-net-rs</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/watch?v=UNrzf4Wgbv4">Neural-Net-RS Explainer</a><br /><a href="https://www.youtube.com/watch?v=UNrzf4Wgbv4"><img src="https://img.youtube.com/vi/UNrzf4Wgbv4/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="why-build-your-own-neural-network">Why Build Your Own Neural Network?</h2>

<p>Frameworks like PyTorch and TensorFlow are production-ready, but they hide the fundamentals. When teaching or learning, you want to see:</p>

<ul>
  <li>How weights and biases actually change during training</li>
  <li>Why XOR needs a hidden layer when AND doesn’t</li>
  <li>What backpropagation really computes</li>
</ul>

<p>Neural-Net-RS exposes all of this. No autograd magic—every gradient is computed explicitly. No tensor abstractions—just matrices with clear row-major storage.</p>

<h2 id="what-got-built">What Got Built</h2>

<p>A modular Rust workspace with multiple interfaces to the same core:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>neural-net-rs/
├── matrix/              # Linear algebra foundation
├── neural-network/      # Core ML implementation
├── neural-net-cli/      # Command-line interface
├── neural-net-server/   # REST API with SSE streaming
└── neural-net-wasm/     # WebAssembly for browser
</code></pre></div></div>

<p><strong>One codebase, three ways to interact:</strong></p>
<ul>
  <li><strong>CLI</strong>: Train from terminal with progress bars</li>
  <li><strong>Web UI</strong>: Visual training with real-time loss charts</li>
  <li><strong>WASM</strong>: Run entirely in browser, no server needed</li>
</ul>

<h2 id="the-classic-problems">The Classic Problems</h2>

<p>The platform includes 8 built-in examples that teach ML concepts progressively:</p>

<table>
  <thead>
    <tr>
      <th>Problem</th>
      <th>Architecture</th>
      <th>Key Concept</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>AND, OR</td>
      <td>2→2→1</td>
      <td>Linear separability</td>
    </tr>
    <tr>
      <td><strong>XOR</strong></td>
      <td>2→3→1</td>
      <td>Why hidden layers matter</td>
    </tr>
    <tr>
      <td>Parity3</td>
      <td>3→6→1</td>
      <td>Scaling non-linearity</td>
    </tr>
    <tr>
      <td>Quadrant</td>
      <td>2→8→4</td>
      <td>Multi-class classification</td>
    </tr>
    <tr>
      <td>Adder2</td>
      <td>4→8→3</td>
      <td>Learning arithmetic</td>
    </tr>
    <tr>
      <td>Iris</td>
      <td>4→8→3</td>
      <td>Real-world dataset</td>
    </tr>
    <tr>
      <td>Pattern3x3</td>
      <td>9→6→4</td>
      <td>Visual pattern recognition</td>
    </tr>
  </tbody>
</table>

<h3 id="the-xor-problem">The XOR Problem</h3>

<p>XOR is the canonical neural network problem. AND and OR are linearly separable—a single line can divide the outputs. XOR isn’t. You <em>need</em> a hidden layer.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>AND: (0,0)→0, (0,1)→0, (1,0)→0, (1,1)→1  ← One line separates
XOR: (0,0)→0, (0,1)→1, (1,0)→1, (1,1)→0  ← No line works
</code></pre></div></div>

<p>Watch XOR training and you see why neural networks are powerful: they learn to create intermediate representations that make non-linear problems separable.</p>

<h2 id="implementation-details">Implementation Details</h2>

<h3 id="feed-forward-with-backpropagation">Feed-Forward with Backpropagation</h3>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">struct</span> <span class="n">Network</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="n">layers</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span><span class="p">,</span>      <span class="c1">// [input, hidden..., output]</span>
    <span class="k">pub</span> <span class="n">weights</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="n">Matrix</span><span class="o">&gt;</span><span class="p">,</span>    <span class="c1">// Learned connections</span>
    <span class="k">pub</span> <span class="n">biases</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="n">Matrix</span><span class="o">&gt;</span><span class="p">,</span>     <span class="c1">// Per-neuron offsets</span>
    <span class="k">pub</span> <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">f64</span><span class="p">,</span>      <span class="c1">// Training step size</span>
<span class="p">}</span>
</code></pre></div></div>

<p><strong>Forward pass</strong>: Each layer computes <code class="language-plaintext highlighter-rouge">activation(weights × input + bias)</code></p>

<p><strong>Backward pass</strong>: Gradients flow backward using the chain rule, updating weights to reduce error.</p>

<p>The sigmoid activation function maps any input to (0, 1):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>σ(x) = 1 / (1 + e^(-x))
</code></pre></div></div>

<h3 id="custom-matrix-library">Custom Matrix Library</h3>

<p>Educational clarity over maximum performance:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">struct</span> <span class="n">Matrix</span> <span class="p">{</span>
    <span class="n">rows</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
    <span class="n">cols</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
    <span class="n">data</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f64</span><span class="o">&gt;</span><span class="p">,</span>  <span class="c1">// Row-major storage</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Operations: dot product, transpose, element-wise multiply, map. Everything visible, nothing hidden.</p>

<h3 id="checkpoint-system">Checkpoint System</h3>

<p>Training can be interrupted and resumed:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Train for 5000 epochs, save checkpoint</span>
neural-net-cli train xor <span class="nt">--epochs</span> 5000 <span class="nt">--checkpoint</span> model.json

<span class="c"># Resume from checkpoint</span>
neural-net-cli train xor <span class="nt">--epochs</span> 10000 <span class="nt">--resume</span> model.json
</code></pre></div></div>

<p>Checkpoints include version metadata to prevent loading incompatible models.</p>

<h2 id="cli-usage">CLI Usage</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># List available examples</span>
neural-net-cli examples

<span class="c"># Train XOR with progress bar</span>
neural-net-cli train xor <span class="nt">--epochs</span> 10000 <span class="nt">--learning-rate</span> 0.5

<span class="c"># Predict with trained model</span>
neural-net-cli predict model.json <span class="nt">--input</span> <span class="s2">"0,1"</span>

<span class="c"># Run web UI</span>
neural-net-cli serve <span class="nt">--port</span> 8080
</code></pre></div></div>

<p>The CLI uses <code class="language-plaintext highlighter-rouge">indicatif</code> for real-time progress bars:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training XOR [=========&gt;   ] 7500/10000 (75%) Loss: 0.0023
</code></pre></div></div>

<h2 id="web-interface">Web Interface</h2>

<p>The server embeds all assets at compile time—one binary serves everything:</p>

<ul>
  <li><strong>Training panel</strong>: Select problem, set hyperparameters, watch loss decrease</li>
  <li><strong>Network visualization</strong>: See layer structure and connection strengths</li>
  <li><strong>Prediction panel</strong>: Test the trained model interactively</li>
  <li><strong>Loss chart</strong>: Real-time plotting via Server-Sent Events</li>
</ul>

<p>Two training modes:</p>
<ul>
  <li><strong>Local (WASM)</strong>: Runs entirely in browser</li>
  <li><strong>Remote (API)</strong>: Server-side with streaming progress</li>
</ul>

<h2 id="technology-choices">Technology Choices</h2>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Rust</strong></td>
      <td>Performance, safety, single-binary distribution</td>
    </tr>
    <tr>
      <td><strong>Axum</strong></td>
      <td>Lightweight async web framework</td>
    </tr>
    <tr>
      <td><strong>wasm-bindgen</strong></td>
      <td>Rust → WebAssembly compilation</td>
    </tr>
    <tr>
      <td><strong>Indicatif</strong></td>
      <td>Terminal progress bars</td>
    </tr>
    <tr>
      <td><strong>Serde</strong></td>
      <td>JSON serialization for checkpoints</td>
    </tr>
  </tbody>
</table>

<p>The WASM module is ~248KB after optimization.</p>

<h2 id="test-coverage">Test Coverage</h2>

<p>136+ tests across the workspace:</p>
<ul>
  <li>Matrix operations (unit tests)</li>
  <li>Network training (integration tests)</li>
  <li>CLI commands (integration tests)</li>
  <li>Server endpoints (integration tests)</li>
  <li>WASM bindings (unit tests)</li>
</ul>

<p>Zero clippy warnings. Reproducible results via seeded RNG.</p>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Backpropagation</strong></td>
        <td><a href="https://www.nature.com/articles/323533a0">Learning representations by back-propagating errors</a> (Rumelhart et al. 1986)</td>
      </tr>
      <tr>
        <td><strong>Multi-Layer Perceptron</strong></td>
        <td><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer perceptron</a> (Wikipedia)</td>
      </tr>
      <tr>
        <td><strong>XOR Problem</strong></td>
        <td><a href="https://mitpress.mit.edu/9780262534772/perceptrons/">Perceptrons</a> (Minsky &amp; Papert 1969)</td>
      </tr>
      <tr>
        <td><strong>Weight Initialization</strong></td>
        <td><a href="https://proceedings.mlr.press/v9/glorot10a.html">Understanding the Difficulty of Training Deep Feedforward Neural Networks</a> (Glorot &amp; Bengio 2010)</td>
      </tr>
      <tr>
        <td><strong>Inspired by</strong></td>
        <td><a href="https://github.com/codemoonsxyz/neural-net-rs">codemoonsxyz/neural-net-rs</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-vibe-coding-process">The Vibe Coding Process</h2>

<p>This project grew through iterative conversation with Claude Code:</p>

<ol>
  <li>“Build a basic neural network in Rust with backpropagation”</li>
  <li>“Add a CLI with progress bars”</li>
  <li>“Add a web UI with real-time training visualization”</li>
  <li>“Compile to WASM so it runs in the browser”</li>
  <li>“Add checkpoint save/resume”</li>
  <li>“Include classic ML examples with educational documentation”</li>
</ol>

<p>Each request built on the previous. The AI handled architecture decisions, chose appropriate crates, and maintained test coverage throughout.</p>

<hr />

<p><em>When you want to understand how neural networks actually work, sometimes you need to see every weight update. That’s what this platform provides—education through transparency.</em></p>


          </div>





<div class="youtube-embed-container" id="yt-container-UNrzf4Wgbv4">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-UNrzf4Wgbv4"
      src="https://www.youtube.com/embed/UNrzf4Wgbv4?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-UNrzf4Wgbv4';
  const playerId = 'yt-player-UNrzf4Wgbv4';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="17"><p class="post-meta">February 14, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">914 words</span> &bull; <span class="post-read-time">5 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Personal Software via Vibe Coding: I needed to find cat photos scattered across my system. Instead of cloud services or app stores, I described what I wanted to Claude Code and got a working Rust CLI tool using YOLOv8 and ONNX Runtime. Privacy-first, locally-run, and mine to modify.</div><h3>
          <a class="post-link" href="/2026/02/14/cat-finder-local-ml-rust/">
            Cat Finder: Personal Software via Vibe Coding
          </a>
        </h3><nav class="toc" data-toc-id="cat-finder-local-ml-rust">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/cat-finder.png" class="post-marker" alt="" /></p>

<p>I needed to find cat photos scattered across my system. Instead of searching the app store, signing up for a cloud service, or uploading my personal photos to someone else’s servers, I asked Claude Code to build me the tool I needed. An hour later, I had it.</p>

<p>This is <strong>Personal Software</strong>—software that exists because you needed it, built the way you want it, running entirely under your control.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Repo</strong></td>
        <td><a href="https://github.com/sw-ml-study/cat-finder">cat-finder</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/watch?v=pkhSwI97udw">Cat Finder Explainer</a><br /><a href="https://www.youtube.com/watch?v=pkhSwI97udw"><img src="https://img.youtube.com/vi/pkhSwI97udw/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-vibe-coding-approach">The Vibe Coding Approach</h2>

<p><strong>Vibe Coding</strong> is about describing what you want and letting AI handle the implementation details. No boilerplate, no Stack Overflow rabbit holes, no fighting with build systems. You focus on the <em>what</em>, the AI handles the <em>how</em>.</p>

<p>For Cat Finder, the conversation went something like:</p>

<blockquote>
  <p>“I want a CLI tool that scans directories for images containing cats. Run locally, no cloud. Use YOLO for detection. Output just the file paths so I can pipe them to other commands.”</p>
</blockquote>

<p>Claude Code chose the tech stack (Rust, YOLOv8n, ONNX Runtime), handled the tensor math, figured out the COCO class IDs, and produced a working tool. I guided the direction; the AI wrote the code.</p>

<h2 id="why-personal-software">Why Personal Software?</h2>

<p>The traditional options for “find cat photos” would be:</p>

<ol>
  <li><strong>Cloud service</strong>: Upload photos to Google/Apple/Amazon, let them scan everything, hope they respect your privacy</li>
  <li><strong>Desktop app</strong>: Find something in an app store, hope it does what you want, deal with subscription fees or ads</li>
  <li><strong>Write it yourself</strong>: Spend days learning YOLO integration, tensor formats, image preprocessing</li>
</ol>

<p>Personal Software offers a fourth path: <strong>describe what you need, get exactly that, own the result completely</strong>.</p>

<p>Cat Finder runs entirely on my machine. No accounts, no uploads, no subscriptions, no ads. The code is mine to modify, extend, or share.</p>

<h2 id="what-got-built">What Got Built</h2>

<p>A Rust CLI tool using <strong>YOLOv8n</strong> (the nano variant) through <strong>ONNX Runtime</strong>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Directory Traversal → Image Preprocessing → YOLO Inference → Cat Detection → Output
</code></pre></div></div>

<h3 id="the-detection-pipeline">The Detection Pipeline</h3>

<ol>
  <li><strong>Walk directories</strong> recursively, finding image files (jpg, png, gif, webp, etc.)</li>
  <li><strong>Preprocess each image</strong>: resize to 640×640, normalize to 0.0-1.0, convert to NCHW tensor format</li>
  <li><strong>Run inference</strong> through the YOLOv8n ONNX model</li>
  <li><strong>Parse output</strong> for class ID 15 (cat in COCO ordering) above confidence threshold</li>
  <li><strong>Print matching paths</strong> to stdout for easy piping to other tools</li>
</ol>

<h3 id="unix-philosophy">Unix Philosophy</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># stdout: just paths (machine-parseable)</span>
<span class="c"># stderr: logging and progress</span>

cat-finder ~/Photos | xargs <span class="nt">-I</span> <span class="o">{}</span> <span class="nb">cp</span> <span class="o">{}</span> ~/CatPhotos/
</code></pre></div></div>

<p>This separation enables composable Unix pipelines. The tool does one thing well and plays nicely with others.</p>

<h2 id="technology-stack">Technology Stack</h2>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Rust</strong></td>
      <td>Memory-safe, high-performance core</td>
    </tr>
    <tr>
      <td><strong>YOLOv8n</strong></td>
      <td>Lightweight object detection (12MB model)</td>
    </tr>
    <tr>
      <td><strong>ONNX Runtime</strong></td>
      <td>Cross-platform inference engine</td>
    </tr>
    <tr>
      <td><strong>clap</strong></td>
      <td>CLI argument parsing</td>
    </tr>
    <tr>
      <td><strong>ndarray</strong></td>
      <td>Tensor operations</td>
    </tr>
    <tr>
      <td><strong>walkdir</strong></td>
      <td>Recursive directory traversal</td>
    </tr>
  </tbody>
</table>

<p>Total footprint: ~80MB (runtime + model + binary)</p>

<p>I didn’t choose this stack—Claude Code did, based on the requirements. It made good choices.</p>

<h2 id="usage">Usage</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Basic usage</span>
cat-finder ~/Photos

<span class="c"># Adjust confidence threshold</span>
cat-finder <span class="nt">--confidence</span> 0.5 ~/Photos

<span class="c"># Verbose output with timestamps</span>
cat-finder <span class="nt">-v</span> <span class="nt">-t</span> ~/Photos

<span class="c"># Copy all cat photos to a new folder</span>
cat-finder ~/Photos | xargs <span class="nt">-I</span> <span class="o">{}</span> <span class="nb">cp</span> <span class="o">{}</span> ~/CatAlbum/
</code></pre></div></div>

<h2 id="honest-about-limitations">Honest About Limitations</h2>

<p>The README documents failure cases transparently:</p>

<table>
  <thead>
    <tr>
      <th>Image Type</th>
      <th>Detection Success</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Clear photographs</td>
      <td>High</td>
    </tr>
    <tr>
      <td>Artistic/stylized images</td>
      <td>Low</td>
    </tr>
    <tr>
      <td>Cats in clothing</td>
      <td>Low</td>
    </tr>
    <tr>
      <td>Small/partial cats</td>
      <td>Variable</td>
    </tr>
    <tr>
      <td>Low quality/blurry</td>
      <td>Variable</td>
    </tr>
  </tbody>
</table>

<p>Test results: 7 of 9 cat images detected (77.8% recall). Oil paintings and anthropomorphized cats confuse models trained on photographs. This is documented, not hidden.</p>

<h2 id="bonus-features">Bonus Features</h2>

<p>The project grew organically based on related needs:</p>

<p><strong>Duplicate Finder</strong>: A second binary for finding duplicate images using size-based filtering followed by SHA-256 checksums.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>find-duplicates ~/Photos
</code></pre></div></div>

<p><strong>Web Demo</strong>: A Flask-based interface for visual feedback with real-time progress via Server-Sent Events.</p>

<p>These emerged from “while you’re at it…” requests during development. Vibe coding makes feature additions nearly frictionless.</p>

<h2 id="setup">Setup</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/sw-ml-study/cat-finder
<span class="nb">cd </span>cat-finder
./scripts/setup.sh  <span class="c"># Downloads model, builds project</span>
./cat-finder ~/Photos
</code></pre></div></div>

<h2 id="the-personal-software-philosophy">The Personal Software Philosophy</h2>

<p><strong>Privacy-first</strong>: All processing happens locally. No cloud APIs, no external services, no data leaving your machine.</p>

<p><strong>Ownership</strong>: The code is yours. Modify it, extend it, share it, delete it.</p>

<p><strong>Fit-for-purpose</strong>: Built for exactly what you need, nothing more, nothing less.</p>

<p><strong>Transparency</strong>: Known limitations documented. No marketing spin.</p>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>YOLOv8</strong></td>
        <td><a href="https://docs.ultralytics.com/">Ultralytics YOLOv8</a> - State-of-the-art object detection</td>
      </tr>
      <tr>
        <td><strong>ONNX Runtime</strong></td>
        <td><a href="https://onnxruntime.ai/">ONNX Runtime</a> - Cross-platform inference engine</td>
      </tr>
      <tr>
        <td><strong>ort crate</strong></td>
        <td><a href="https://crates.io/crates/ort">ort</a> - Rust bindings for ONNX Runtime</td>
      </tr>
      <tr>
        <td><strong>COCO Dataset</strong></td>
        <td><a href="https://docs.ultralytics.com/datasets/detect/coco/">COCO Classes</a> - Class ID 15 = cat</td>
      </tr>
    </tbody>
  </table>

</div>

<hr />

<p><em>You don’t always need an app store or a cloud service. Sometimes you just need to describe what you want and let an AI build it for you. That’s vibe coding. That’s personal software.</em></p>


          </div>





<div class="youtube-embed-container" id="yt-container-pkhSwI97udw">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-pkhSwI97udw"
      src="https://www.youtube.com/embed/pkhSwI97udw?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-pkhSwI97udw';
  const playerId = 'yt-player-pkhSwI97udw';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="18"><p class="post-meta">February 14, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">525 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: RNN (sequential processing with memory), Chain of Thought (step-by-step reasoning), Softmax (scores to probabilities), MoE (route inputs to specialists), Distribution Shift (training vs deployment mismatch).</div><h3>
          <a class="post-link" href="/2026/02/14/five-ml-concepts-11/">
            Five ML Concepts - #11
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-11">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-pipers.png" class="post-marker" style="width: 480px;" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/95ng2EmBTbA">Five ML Concepts #11</a><br /><a href="https://www.youtube.com/shorts/95ng2EmBTbA"><img src="https://img.youtube.com/vi/95ng2EmBTbA/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>RNN</strong></td>
        <td><a href="https://www.nature.com/articles/323533a0">Learning representations by back-propagating errors</a> (Rumelhart et al. 1986)</td>
      </tr>
      <tr>
        <td><strong>Chain of Thought</strong></td>
        <td><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a> (Wei et al. 2022)</td>
      </tr>
      <tr>
        <td><strong>Softmax</strong></td>
        <td><a href="https://www.deeplearningbook.org/">Deep Learning</a> (Goodfellow et al. 2016), Chapter 6</td>
      </tr>
      <tr>
        <td><strong>MoE</strong></td>
        <td><a href="https://arxiv.org/abs/1701.06538">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a> (Shazeer et al. 2017)</td>
      </tr>
      <tr>
        <td><strong>Distribution Shift</strong></td>
        <td><a href="https://direct.mit.edu/books/edited-volume/3098/Dataset-Shift-in-Machine-Learning">Dataset Shift in Machine Learning</a> (Quiñonero-Candela et al. 2009)</td>
      </tr>
    </tbody>
  </table>

</div>

<div style="display:none">

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-rnn-recurrent-neural-network">1. RNN (Recurrent Neural Network)</h3>

<p><strong>Networks designed for sequential data that maintain a hidden state carrying information across time steps.</strong> This makes them useful for language, time series, and audio.</p>

<p>LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are improved variants that better handle long-range dependencies.</p>

<blockquote>
  <p>Like reading a story while keeping mental notes about characters and plot as you go.</p>
</blockquote>

<h3 id="2-chain-of-thought">2. Chain of Thought</h3>

<p><strong>A prompting technique that encourages step-by-step reasoning in language models.</strong> Instead of producing an answer immediately, the model generates intermediate steps.</p>

<p>This can improve performance on math, logic, and multi-step problems.</p>

<blockquote>
  <p>Like showing your work on a math test instead of just writing the final answer.</p>
</blockquote>

<h3 id="3-softmax">3. Softmax</h3>

<p><strong>Converts a vector of scores into a probability distribution where each output falls between zero and one, and all outputs sum to one.</strong> It is commonly used in classification models.</p>

<p>Softmax makes raw scores easier to interpret as probabilities.</p>

<blockquote>
  <p>Like turning test scores into percentages that add up to 100%.</p>
</blockquote>

<h3 id="4-moe-mixture-of-experts">4. MoE (Mixture of Experts)</h3>

<p><strong>Instead of one large network, the model contains many smaller expert networks with a routing mechanism that selects which experts process each input.</strong> This allows models to scale capacity while keeping computation efficient.</p>

<p>Only a subset of experts activates for any given input.</p>

<blockquote>
  <p>Like a hospital with specialists where a receptionist directs you to the right doctor.</p>
</blockquote>

<h3 id="5-distribution-shift">5. Distribution Shift</h3>

<p><strong>Occurs when deployment data differs from training data, causing a model trained on one environment to perform poorly in another.</strong> Common causes include seasonal changes, user behavior shifts, or new populations.</p>

<p>Monitoring for drift and retraining helps maintain performance.</p>

<blockquote>
  <p>Like a weather model trained on summer data struggling to predict winter storms.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>RNN</strong></td>
      <td>Sequential processing with memory across time</td>
    </tr>
    <tr>
      <td><strong>Chain of Thought</strong></td>
      <td>Step-by-step reasoning in prompts</td>
    </tr>
    <tr>
      <td><strong>Softmax</strong></td>
      <td>Scores to normalized probabilities</td>
    </tr>
    <tr>
      <td><strong>MoE</strong></td>
      <td>Route inputs to specialized experts</td>
    </tr>
    <tr>
      <td><strong>Distribution Shift</strong></td>
      <td>Training vs deployment data mismatch</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 11 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/15/five-ml-concepts-12/">Next: #12 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-95ng2EmBTbA">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-95ng2EmBTbA"
      src="https://www.youtube.com/embed/95ng2EmBTbA?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-95ng2EmBTbA';
  const playerId = 'yt-player-95ng2EmBTbA';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="19"><p class="post-meta">February 13, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">995 words</span> &bull; <span class="post-read-time">5 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">When data won't fit in a context window, RLM expands the workspace instead. The MIT paper achieves 87-91% accuracy where standard prompting scores 0%. My Rust implementation provides four capability levels from DSL commands to WASM sandboxing to LLM delegation.</div><h3>
          <a class="post-link" href="/2026/02/13/rlm-recursive-language-models/">
            RLM: Recursive Language Models for Massive Context
          </a>
        </h3><nav class="toc" data-toc-id="rlm-recursive-language-models">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-mirror-recursion.png" class="post-marker" alt="" /></p>

<p>What happens when your data won’t fit in a context window? RLM expands the workspace instead of cramming everything into limited memory. This post covers the MIT paper, my Rust implementation, and six video demonstrations.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2512.24601">arXiv:2512.24601</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/rlm-project">rlm-project</a></td>
      </tr>
      <tr>
        <td><strong>Playlist</strong></td>
        <td><a href="https://www.youtube.com/playlist?list=PLKjvVAEaR4itAgRBOJGi-B2CCY2-Wvgem">RLM Implementations</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-problem-context-limits">The Problem: Context Limits</h2>

<p>Large language models have a hard limit. They can only process so much text at once.</p>

<p>Imagine a cookie jar that holds 100 cookies. What if you need to search through ten thousand? When you force too much in, the model forgets things—this is called <strong>context rot</strong>.</p>

<p>Bigger models help, but the limit always exists. We need a different approach.</p>

<h2 id="the-rlm-solution">The RLM Solution</h2>

<p>Recursive Language Models flip the problem. Instead of bigger jars, use better tools.</p>

<p>The data stays in a <strong>context box</strong>. The model gets tools to peek inside:</p>

<table>
  <thead>
    <tr>
      <th>Tool</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">slice</code></td>
      <td>Get a character range</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">find</code></td>
      <td>Search for text</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">regex</code></td>
      <td>Pattern matching</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">count</code></td>
      <td>Count occurrences</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">llm_query</code></td>
      <td>Ask a sub-LLM to analyze a chunk</td>
    </tr>
  </tbody>
</table>

<p>Small, focused, deliberate. The model thinks about what it needs, then asks for just that.</p>

<h3 id="the-results">The Results</h3>

<p>From the MIT paper—on tasks that don’t fit in context:</p>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Standard prompting</td>
      <td>0%</td>
    </tr>
    <tr>
      <td>RLM</td>
      <td>87-91%</td>
    </tr>
  </tbody>
</table>

<p>Results hold across GPT-4, Claude, Llama, Mistral, and Gemini.</p>

<h2 id="my-implementation-four-capability-levels">My Implementation: Four Capability Levels</h2>

<p>I built a Rust implementation with four capability levels:</p>

<table>
  <thead>
    <tr>
      <th>Level</th>
      <th>Name</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>L1</td>
      <td>DSL</td>
      <td>Built-in commands (find, regex, count)</td>
    </tr>
    <tr>
      <td>L2</td>
      <td>WASM</td>
      <td>LLM generates Rust → compiles to WebAssembly sandbox</td>
    </tr>
    <tr>
      <td>L3</td>
      <td>CLI</td>
      <td>LLM generates Rust → compiles to native binary</td>
    </tr>
    <tr>
      <td>L4</td>
      <td>LLM</td>
      <td>Recursive delegation to sub-LLMs</td>
    </tr>
  </tbody>
</table>

<p>Each level trades off safety for capability:</p>
<ul>
  <li><strong>L1</strong> is instant but limited to predefined operations</li>
  <li><strong>L2</strong> runs custom code but in a sandboxed environment</li>
  <li><strong>L3</strong> breaks free for large datasets that would timeout in WASM</li>
  <li><strong>L4</strong> uses LLM reasoning for semantic analysis</li>
</ul>

<h2 id="the-video-series">The Video Series</h2>

<p>Six videos demonstrate RLM in action:</p>

<h3 id="1-rlm-explained">1. RLM Explained</h3>

<p><a href="https://www.youtube.com/watch?v=5DhaTPuyhys"><img src="https://img.youtube.com/vi/5DhaTPuyhys/mqdefault.jpg" alt="RLM Explained" /></a></p>

<p>The foundational video. Covers the MIT paper, the cookie jar analogy, and benchmark results showing 0% → 91% accuracy improvement.</p>

<p><strong>Key insight:</strong> Expand the workspace, not the context.</p>

<hr />

<h3 id="2-war-and-peace-demo">2. War and Peace Demo</h3>

<p><a href="https://www.youtube.com/watch?v=d5gaL4iOdLA"><img src="https://img.youtube.com/vi/d5gaL4iOdLA/mqdefault.jpg" alt="War and Peace Demo" /></a></p>

<p>Can AI read all of War and Peace to find a hidden secret? The full text is 3.2 MB with 65,666 lines—way too big for any context window.</p>

<p>RLM finds “the password to Prince Andrei’s secret vault” in just <strong>2 iterations</strong> using only 3,000 tokens. That’s 100% savings compared to sending the full document.</p>

<hr />

<h3 id="3-wasm-sandboxing">3. WASM Sandboxing</h3>

<p><a href="https://www.youtube.com/watch?v=jMo5AaMRUkM"><img src="https://img.youtube.com/vi/jMo5AaMRUkM/mqdefault.jpg" alt="WASM Sandboxing" /></a></p>

<p>What if your LLM could write custom analysis code on the fly? Level 2 demonstrates WebAssembly sandboxing.</p>

<p>The LLM writes Rust code that compiles to WASM and runs in a secure sandbox. Demos include:</p>
<ul>
  <li>Error ranking in logs</li>
  <li>Response time percentiles</li>
  <li>Unique IP counting</li>
</ul>

<p>Trade-offs: ASCII only, 64MB memory limit, subset of Rust.</p>

<hr />

<h3 id="4-native-cli-binaries">4. Native CLI Binaries</h3>

<p><a href="https://www.youtube.com/watch?v=oN6XyZdEHqY"><img src="https://img.youtube.com/vi/oN6XyZdEHqY/mqdefault.jpg" alt="Native CLI Binaries" /></a></p>

<p>When 5,000 lines would timeout in WASM, Level 3 breaks free. Native Rust binaries process massive datasets with no limits.</p>

<p>Four CLI demos:</p>
<ul>
  <li><strong>Error ranking</strong>: Hash map counts error types</li>
  <li><strong>Unique IPs</strong>: Hash set finds distinct addresses</li>
  <li><strong>Percentiles</strong>: Sort and index for p50/p95/p99</li>
  <li><strong>Word frequency</strong>: Tokenize, filter stop words, count</li>
</ul>

<hr />

<h3 id="5-detective-mystery-demo">5. Detective Mystery Demo</h3>

<p><a href="https://www.youtube.com/watch?v=a-p4kojgJtM"><img src="https://img.youtube.com/vi/a-p4kojgJtM/mqdefault.jpg" alt="Detective Mystery Demo" /></a></p>

<p>A murder at the manor. Seven suspects. Dozens of clues. Can an LLM solve it?</p>

<p>Level 4 delegates reasoning to sub-LLMs. Instead of code execution, the model calls other models to:</p>
<ul>
  <li>Analyze witness statements</li>
  <li>Compare alibis</li>
  <li>Draw conclusions</li>
</ul>

<p>Watch as L4 examines each suspect and identifies the killer.</p>

<hr />

<h3 id="6-large-context-processing">6. Large Context Processing</h3>

<p><a href="https://www.youtube.com/watch?v=l6OjvtG2Nlk"><img src="https://img.youtube.com/vi/l6OjvtG2Nlk/mqdefault.jpg" alt="Large Context Processing" /></a></p>

<p>War and Peace is 3MB—far too large for any context window. This video shows Level 4 extracting noble family relationships from the entire novel.</p>

<p>The process:</p>
<ol>
  <li>L3 extracts relationship sentences (father, mother, son, daughter…)</li>
  <li>L4 analyzes filtered data with sub-LLMs</li>
  <li>Final output: structured family trees</li>
</ol>

<p>Three million characters → structured family trees in ~90 seconds.</p>

<hr />

<h2 id="architecture">Architecture</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────┐     ┌─────────────────┐     ┌─────────────┐
│   Client    │────▶│  RLM Server     │────▶│  Root LLM   │
│  /visualize │     │  (Rust/Axum)    │     │  (DeepSeek) │
└─────────────┘     └────────┬────────┘     └─────────────┘
                             │
                    ┌────────▼────────┐
                    │ Command Executor │
                    │  slice, find,   │
                    │  regex, count,  │
                    │  llm_query...   │
                    └────────┬────────┘
                             │
              ┌──────────────┼──────────────┐
              ▼              ▼              ▼
        ┌──────────┐  ┌──────────┐  ┌──────────┐
        │  Ollama  │  │  Ollama  │  │  Ollama  │
        │ (local)  │  │ (remote) │  │ (other)  │
        └──────────┘  └──────────┘  └──────────┘
              Sub-LM Pool (for llm_query)
</code></pre></div></div>

<h2 id="quick-start">Quick Start</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>rlm-orchestrator

<span class="c"># Configure providers in config.toml</span>
<span class="nb">export </span><span class="nv">DEEPSEEK_API_KEY</span><span class="o">=</span><span class="s2">"your-key"</span>

<span class="c"># Run the server</span>
cargo run <span class="nt">--bin</span> rlm-server

<span class="c"># Open visualizer</span>
open http://localhost:8080/visualize
</code></pre></div></div>

<h2 id="the-cookie-jar-analogy">The Cookie Jar Analogy</h2>

<p>Think of it like this:</p>

<ul>
  <li><strong>Old way:</strong> Dump everything on the table, then dig through the mess</li>
  <li><strong>RLM way:</strong> Use a scoop—grab just the cookies you need</li>
</ul>

<p>The key insight is simple: <strong>expand the workspace, not the context.</strong></p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2512.24601">RLM Paper (arXiv:2512.24601)</a> - Zhang, Kraska, Khattab (MIT CSAIL)</li>
  <li><a href="https://github.com/softwarewrighter/rlm-project">rlm-project Repository</a></li>
  <li><a href="https://github.com/softwarewrighter/rlm-project/wiki">rlm-project Wiki</a></li>
  <li><a href="https://www.youtube.com/playlist?list=PLKjvVAEaR4itAgRBOJGi-B2CCY2-Wvgem">RLM Implementations Playlist</a></li>
  <li><a href="https://github.com/softwarewrighter/rlm-project/blob/main/docs/rlm-eli5.md">ELI5: What is RLM?</a></li>
</ul>

<hr />

<p><em>When context windows aren’t enough, RLM gives your LLM tools to explore. Six videos, four capability levels, one insight: expand the workspace, not the context.</em></p>


          </div>





<div class="youtube-embed-container" id="yt-container-5DhaTPuyhys">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-5DhaTPuyhys"
      src="https://www.youtube.com/embed/5DhaTPuyhys?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-5DhaTPuyhys';
  const playerId = 'yt-player-5DhaTPuyhys';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="20"><p class="post-meta">February 13, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">521 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: CNN (sliding filters for image features), Encoder-Decoder (compress then generate), RAG (retrieve context before generating), Few-shot Learning (learn from prompt examples), Distillation (small student mimics large teacher).</div><h3>
          <a class="post-link" href="/2026/02/13/five-ml-concepts-10/">
            Five ML Concepts - #10
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-10">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-ten.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/lVPnKvi9VdA">Five ML Concepts #10</a><br /><a href="https://www.youtube.com/shorts/lVPnKvi9VdA"><img src="https://img.youtube.com/vi/lVPnKvi9VdA/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>CNN</strong></td>
        <td><a href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">ImageNet Classification with Deep Convolutional Neural Networks</a> (Krizhevsky et al. 2012)</td>
      </tr>
      <tr>
        <td><strong>Encoder-Decoder</strong></td>
        <td><a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a> (Sutskever et al. 2014)</td>
      </tr>
      <tr>
        <td><strong>RAG</strong></td>
        <td><a href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a> (Lewis et al. 2020)</td>
      </tr>
      <tr>
        <td><strong>Few-shot Learning</strong></td>
        <td><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> (Brown et al. 2020)</td>
      </tr>
      <tr>
        <td><strong>Distillation</strong></td>
        <td><a href="https://arxiv.org/abs/1503.02531">Distilling the Knowledge in a Neural Network</a> (Hinton et al. 2015)</td>
      </tr>
    </tbody>
  </table>

</div>

<div style="display:none">

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-cnn-convolutional-neural-network">1. CNN (Convolutional Neural Network)</h3>

<p><strong>Networks designed for image data that use small filters sliding across an image to detect edges, textures, and shapes.</strong> Early layers find simple patterns, while deeper layers recognize complex objects.</p>

<p>CNNs are a foundation of modern computer vision.</p>

<blockquote>
  <p>Like scanning a photo with a magnifying glass that learns to recognize patterns at different scales.</p>
</blockquote>

<h3 id="2-encoder-decoder">2. Encoder-Decoder</h3>

<p><strong>A model architecture with two parts: the encoder compresses input into a representation, and the decoder generates an output from that representation.</strong> This pattern is common in translation, summarization, and speech systems.</p>

<p>The representation acts as a bottleneck that captures essential information.</p>

<blockquote>
  <p>Like summarizing a book into notes, then writing a new version from those notes.</p>
</blockquote>

<h3 id="3-rag-retrieval-augmented-generation">3. RAG (Retrieval-Augmented Generation)</h3>

<p><strong>Instead of relying only on learned parameters, the model retrieves relevant documents and uses them during generation.</strong> This helps ground responses in external information and can reduce hallucinations.</p>

<p>RAG combines the strengths of retrieval systems and generative models.</p>

<blockquote>
  <p>Like an open-book exam where you can look up facts instead of relying purely on memory.</p>
</blockquote>

<h3 id="4-few-shot-learning">4. Few-shot Learning</h3>

<p><strong>Adapting behavior from just a few examples provided directly in the prompt.</strong> Instead of retraining, the model infers the pattern and applies it to new inputs.</p>

<p>Zero-shot learning relies only on instructions, without examples.</p>

<blockquote>
  <p>Like learning a card game by watching a few hands before playing.</p>
</blockquote>

<h3 id="5-distillation">5. Distillation</h3>

<p><strong>Transferring knowledge from a large teacher model to a smaller student.</strong> The student learns to match the teacher’s outputs, not its internal weights.</p>

<p>This produces models that are smaller and cheaper while retaining much of the original capability.</p>

<blockquote>
  <p>Like an apprentice learning by imitating a master’s finished work, not by copying their brain.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>CNN</strong></td>
      <td>Sliding filters for hierarchical image features</td>
    </tr>
    <tr>
      <td><strong>Encoder-Decoder</strong></td>
      <td>Compress input, then generate output</td>
    </tr>
    <tr>
      <td><strong>RAG</strong></td>
      <td>Retrieve context before generating</td>
    </tr>
    <tr>
      <td><strong>Few-shot Learning</strong></td>
      <td>Learn from examples in the prompt</td>
    </tr>
    <tr>
      <td><strong>Distillation</strong></td>
      <td>Small student mimics large teacher</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 10 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/14/five-ml-concepts-11/">Next: #11 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-lVPnKvi9VdA">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-lVPnKvi9VdA"
      src="https://www.youtube.com/embed/lVPnKvi9VdA?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-lVPnKvi9VdA';
  const playerId = 'yt-player-lVPnKvi9VdA';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="21"><p class="post-meta">February 12, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">1655 words</span> &bull; <span class="post-read-time">9 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Before pixels, there were vectors. Vibe Coding classic arcade games (Asteroids, BattleZone, Tempest) in Rust/WebAssembly with wgpu rendering---from my first encounter with an IBM 2250 to playable browser demos, all built in one day with Claude Code.</div><h3>
          <a class="post-link" href="/2026/02/12/tbt-vector-graphics-games/">
            TBT (3/?): Vector Graphics Games
          </a>
        </h3><nav class="toc" data-toc-id="tbt-vector-graphics-games">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/geometric.png" class="post-marker" alt="" /></p>

<p>Before pixels, there were vectors. This Throwback Thursday explores the evolution of vector graphics gaming—from military radar displays to arcade classics—and my attempt to recreate them in Rust and WebAssembly.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Live Demo</strong></td>
        <td><a href="https://softwarewrighter.github.io/vectorcade-web-yew/">Play in Browser</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/watch?v=lxEFBzDjp3A">TBT Vector Graphics Games</a><br /><a href="https://www.youtube.com/watch?v=lxEFBzDjp3A"><img src="https://img.youtube.com/vi/lxEFBzDjp3A/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>Games</strong></td>
        <td><a href="https://github.com/softwarewrighter/vectorcade-games">vectorcade-games</a></td>
      </tr>
      <tr>
        <td><strong>Shared</strong></td>
        <td><a href="https://github.com/softwarewrighter/vectorcade-shared">vectorcade-shared</a></td>
      </tr>
      <tr>
        <td><strong>Fonts</strong></td>
        <td><a href="https://github.com/softwarewrighter/vectorcade-fonts">vectorcade-fonts</a></td>
      </tr>
      <tr>
        <td><strong>Renderer</strong></td>
        <td><a href="https://github.com/softwarewrighter/vectorcade-render-wgpu">vectorcade-render-wgpu</a></td>
      </tr>
      <tr>
        <td><strong>Web</strong></td>
        <td><a href="https://github.com/softwarewrighter/vectorcade-web-yew">vectorcade-web-yew</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="my-first-vector-display-the-ibm-2250">My First Vector Display: The IBM 2250</h2>

<figure class="image-right" style="float: right; margin: 0 0 1em 1.5em; max-width: 320px;">
  <img src="/assets/images/posts/ibm-2250-hes-console.png" alt="IBM 2250 Graphics Display Unit with light pen, October 1969" style="width: 100%;" />
  <figcaption style="font-size: 0.85em; color: #666; text-align: center;">IBM 2250 at Brown University, 1969. <a href="#references">Photo credit</a></figcaption>
</figure>

<p>My first encounter with vector graphics was an <strong>IBM 2250 Graphics Display Unit</strong>—introduced in 1964, costing around <strong>$280,000</strong> in period dollars. It connected to an IBM 1130 that acted as a graphics controller for an IBM S/370 mainframe where the graphical applications ran. At that price, nobody was playing games on it—<strong>Computer Aided Design</strong> was the killer app.</p>

<p>The 2250’s specifications were impressive for its era:</p>

<table>
  <thead>
    <tr>
      <th>Specification</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Display</td>
      <td>21-inch P39 phosphor CRT</td>
    </tr>
    <tr>
      <td>Resolution</td>
      <td>1024 × 1024 addressable points</td>
    </tr>
    <tr>
      <td>Usable area</td>
      <td>12” × 12” (square aspect)</td>
    </tr>
    <tr>
      <td>Refresh rate</td>
      <td>~40 frames/second</td>
    </tr>
    <tr>
      <td>Input</td>
      <td>Light pen for direct interaction</td>
    </tr>
    <tr>
      <td>Vector drawing</td>
      <td>Hardware character generator optional</td>
    </tr>
  </tbody>
</table>

<p>The CRT drew lines by steering an electron beam directly—no pixel grid, no rasterization. Just pure geometry traced in phosphor glow. The green P39 phosphor had long persistence, reducing flicker but creating ghostly trails on moving objects.</p>

<p>The light pen was revolutionary: you could point directly at displayed geometry and the system knew <em>which</em> vector you were touching. Interactive graphics in 1964.</p>

<h2 id="the-arcade-era">The Arcade Era</h2>

<p>Vector displays found their way into arcades, where they defined a visual style that’s still recognizable today:</p>

<table>
  <thead>
    <tr>
      <th>Game</th>
      <th>Year</th>
      <th>Innovation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Lunar Lander</strong></td>
      <td>1979</td>
      <td>Physics simulation, thrust/gravity</td>
    </tr>
    <tr>
      <td><strong>Asteroids</strong></td>
      <td>1979</td>
      <td>Wrap-around space, particle effects</td>
    </tr>
    <tr>
      <td><strong>BattleZone</strong></td>
      <td>1980</td>
      <td>Green wireframe 3D, first-person tanks</td>
    </tr>
    <tr>
      <td><strong>Tempest</strong></td>
      <td>1981</td>
      <td>Multi-colored vectors, pseudo-3D depth</td>
    </tr>
  </tbody>
</table>

<p><em>(Note: Pong (1972) was actually a raster game using discrete logic, but its simple geometry makes it a natural fit for vector recreation.)</em></p>

<p>Each generation built on the last. White vectors on black screens gave way to green wireframes, then full color. The hardware pushed boundaries that feel primitive now but were revolutionary then.</p>

<h2 id="the-vectorcade-project">The Vectorcade Project</h2>

<p>Vectorcade recreates these mechanics using modern tools:</p>

<ul>
  <li><strong>Rust</strong> for game logic and rendering</li>
  <li><strong>WebAssembly</strong> for browser deployment</li>
  <li><strong>wgpu</strong> for GPU-accelerated vector rendering</li>
  <li><strong>Yew</strong> for the web frontend</li>
</ul>

<h3 id="multi-repo-architecture">Multi-Repo Architecture</h3>

<p>The project architecture emerged from a design session with ChatGPT, exploring how to structure a multi-agent development workflow. The result: a DAG of repositories, each with clear ownership boundaries:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vectorcade-shared/      (Pure Rust API contracts)
    ↓
vectorcade-fonts/       (Vector font styles)
    ↓
vectorcade-games/       (Game logic: Pong, Asteroids, etc.)
    ↓
vectorcade-render-wgpu/ (wgpu + lyon tessellation)
    ↓
vectorcade-web-yew/     (Yew web shell)
</code></pre></div></div>

<p>This DAG structure allows parallel development with assigned agent roles:</p>

<table>
  <thead>
    <tr>
      <th>Agent</th>
      <th>Repo</th>
      <th>Focus</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>A</strong></td>
      <td>vectorcade-shared</td>
      <td>Core API steward: minimal, stable, pure</td>
    </tr>
    <tr>
      <td><strong>B</strong></td>
      <td>vectorcade-fonts</td>
      <td>Font stylist: 3-5 distinct vector styles</td>
    </tr>
    <tr>
      <td><strong>C</strong></td>
      <td>vectorcade-games</td>
      <td>Game logic: Pong → Asteroids → Lunar Lander</td>
    </tr>
    <tr>
      <td><strong>D</strong></td>
      <td>vectorcade-render-wgpu</td>
      <td>Renderer: lyon tessellation → wgpu triangles</td>
    </tr>
    <tr>
      <td><strong>E</strong></td>
      <td>vectorcade-web-yew</td>
      <td>Integrator: UI, mobile controls, PWA</td>
    </tr>
  </tbody>
</table>

<p>Each agent works against stable interfaces—the <code class="language-plaintext highlighter-rouge">DrawCmd</code> display list and <code class="language-plaintext highlighter-rouge">Game</code> trait—so they don’t step on each other.</p>

<h3 id="the-display-list-model">The Display List Model</h3>

<p>Games don’t render directly. They emit <strong>draw commands</strong> that the renderer interprets:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">enum</span> <span class="n">DrawCmd</span> <span class="p">{</span>
    <span class="n">Clear</span> <span class="p">{</span> <span class="n">color</span><span class="p">:</span> <span class="n">Rgba</span> <span class="p">},</span>
    <span class="nf">Line</span><span class="p">(</span><span class="n">Line2</span><span class="p">),</span>
    <span class="n">Polyline</span> <span class="p">{</span> <span class="n">pts</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="p">[</span><span class="nb">f32</span><span class="p">;</span><span class="mi">2</span><span class="p">]</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">closed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">stroke</span><span class="p">:</span> <span class="n">Stroke</span> <span class="p">},</span>
    <span class="n">Text</span> <span class="p">{</span> <span class="n">pos</span><span class="p">:</span> <span class="p">[</span><span class="nb">f32</span><span class="p">;</span><span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="p">:</span> <span class="nb">String</span><span class="p">,</span> <span class="n">size_px</span><span class="p">:</span> <span class="nb">f32</span><span class="p">,</span> <span class="n">color</span><span class="p">:</span> <span class="n">Rgba</span> <span class="p">},</span>
    <span class="nf">PushTransform</span><span class="p">(</span><span class="n">Transform2</span><span class="p">),</span>
    <span class="n">PopTransform</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This keeps game logic portable. The same Asteroids code can render through wgpu on desktop, WebGPU in browsers, or even a software rasterizer.</p>

<h3 id="vector-fonts">Vector Fonts</h3>

<p>Classic arcade games had distinctive lettering. Vectorcade includes multiple font styles to match:</p>

<table>
  <thead>
    <tr>
      <th>Style</th>
      <th>Look</th>
      <th>Games</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">ATARI</code></td>
      <td>Boxy, utilitarian</td>
      <td>Asteroids, Lunar Lander</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">CINEMATRONICS</code></td>
      <td>Thin, angular</td>
      <td>Star Castle</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">MIDWAY</code></td>
      <td>Slightly rounded</td>
      <td>Defender</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">VECTOR_SCANLINE</code></td>
      <td>Broken segments</td>
      <td>“Beam jitter” effect</td>
    </tr>
  </tbody>
</table>

<p>Each font is pure vector geometry—no bitmaps, no texture atlases.</p>

<h3 id="3d-projection">3D Projection</h3>

<p>BattleZone and Tempest need 3D-to-2D projection. Instead of a full 3D renderer, Vectorcade uses a “2.5D pipeline”:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">struct</span> <span class="n">Camera3</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="n">pos</span><span class="p">:</span> <span class="p">[</span><span class="nb">f32</span><span class="p">;</span><span class="mi">3</span><span class="p">],</span>
    <span class="k">pub</span> <span class="n">yaw</span><span class="p">:</span> <span class="nb">f32</span><span class="p">,</span>
    <span class="k">pub</span> <span class="n">pitch</span><span class="p">:</span> <span class="nb">f32</span><span class="p">,</span>
    <span class="k">pub</span> <span class="n">fov_y_rad</span><span class="p">:</span> <span class="nb">f32</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">pub</span> <span class="k">fn</span> <span class="nf">project_polyline</span><span class="p">(</span><span class="n">cam</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Camera3</span><span class="p">,</span> <span class="n">pts3</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[[</span><span class="nb">f32</span><span class="p">;</span><span class="mi">3</span><span class="p">]])</span> <span class="k">-&gt;</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="p">[</span><span class="nb">f32</span><span class="p">;</span><span class="mi">2</span><span class="p">]</span><span class="o">&gt;</span><span class="p">;</span>
</code></pre></div></div>

<p>Games maintain 3D geometry; the core projects it to 2D lines. Depth-based brightness gives the classic “farther = dimmer” effect.</p>

<h2 id="why-rust--wasm">Why Rust + WASM?</h2>

<p>The combination solves several problems:</p>

<ol>
  <li><strong>Performance</strong>: Games need consistent frame rates; Rust delivers</li>
  <li><strong>Portability</strong>: Same code runs native and in browsers</li>
  <li><strong>Safety</strong>: No dangling pointers in the game loop</li>
  <li><strong>Modern tooling</strong>: Cargo, wasm-pack, Trunk make deployment straightforward</li>
</ol>

<p>The wgpu + lyon stack provides cross-platform GPU rendering with proper thick-line support (WebGL’s <code class="language-plaintext highlighter-rouge">lineWidth</code> is notoriously inconsistent).</p>

<h2 id="current-status">Current Status</h2>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Status</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>vectorcade-shared</td>
      <td>Functional</td>
    </tr>
    <tr>
      <td>vectorcade-fonts</td>
      <td>Functional</td>
    </tr>
    <tr>
      <td>vectorcade-games</td>
      <td>Playable (5 demos)</td>
    </tr>
    <tr>
      <td>vectorcade-render-wgpu</td>
      <td>Functional</td>
    </tr>
    <tr>
      <td>vectorcade-web-yew</td>
      <td>Functional</td>
    </tr>
  </tbody>
</table>

<p>The core architecture works. All five demos are playable in the browser. Polish and audio remain.</p>

<h2 id="the-demos">The Demos</h2>

<p>The video showcases five demonstrations, progressing from static display to full gameplay:</p>

<h3 id="1-ibm-2250-chessboard">1. IBM 2250 Chessboard</h3>

<p>A static image rendered in the style of the original IBM 2250. The 2250 was mainly used for Computer Aided Design, but programmers did create games on it—this chessboard pays tribute to that era.</p>

<h3 id="2-pong-playable">2. Pong (Playable)</h3>

<p>A vector implementation of the classic. The original Pong (1972) wasn’t actually a vector game—it used discrete logic and a raster display—but some clones used vector hardware. This recreation captures the pure-geometry aesthetic.</p>

<h3 id="3-asteroids-playable">3. Asteroids (Playable)</h3>

<p>One of the most popular vector arcade games. Rotate, thrust, and shoot to survive. The ship and asteroids wrap around screen edges, creating the classic “infinite space” feel.</p>

<h3 id="4-battlezone-playable">4. BattleZone (Playable)</h3>

<p>Green wireframe 3D tanks. Drive through a battlefield, shooting enemies and dodging missiles. One of the first games with true 3D perspective—rendered entirely with vectors.</p>

<h3 id="5-tempest-playable">5. Tempest (Playable)</h3>

<p>The pinnacle of vector arcade hardware. Move around the edge of geometric tubes, shooting enemies that climb up from the depths. Each level changes the tube shape and color scheme.</p>

<h3 id="implementation">Implementation</h3>

<p>Each game implements the same <code class="language-plaintext highlighter-rouge">Game</code> trait:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">trait</span> <span class="n">Game</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">metadata</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">GameMeta</span><span class="p">;</span>
    <span class="k">fn</span> <span class="nf">reset</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="k">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="n">GameCtx</span><span class="p">);</span>
    <span class="k">fn</span> <span class="nf">update</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="k">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="n">GameCtx</span><span class="p">,</span> <span class="n">dt</span><span class="p">:</span> <span class="nb">f32</span><span class="p">);</span>
    <span class="k">fn</span> <span class="nf">render</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="k">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="n">GameCtx</span><span class="p">,</span> <span class="n">out</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="n">DrawCmd</span><span class="o">&gt;</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>

<p>This makes games drop-in replaceable in the web shell—no renderer changes needed.</p>

<h2 id="todo">TODO</h2>

<p>The demos are playable but not finished. Remaining work:</p>

<ul>
  <li><del><strong>GPU rendering</strong>: Switch from Canvas 2D emulation to actual wgpu GPU rendering</del> [Ed. Completed 2/13]</li>
  <li><strong>Music and sound effects</strong>: Authentic arcade audio</li>
  <li><strong>More aggressive opponents</strong>: AI improvements for challenge</li>
  <li><strong>Additional levels/difficulties</strong>: Progression and replay value</li>
  <li><strong>More animations</strong>: Explosions, transitions, effects</li>
</ul>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://github.com/softwarewrighter/vectorcade-games">vectorcade-games</a></li>
  <li><a href="https://github.com/softwarewrighter/vectorcade-shared">vectorcade-shared</a></li>
  <li><a href="https://github.com/softwarewrighter/vectorcade-fonts">vectorcade-fonts</a></li>
  <li><a href="https://github.com/softwarewrighter/vectorcade-render-wgpu">vectorcade-render-wgpu</a></li>
  <li><a href="https://github.com/softwarewrighter/vectorcade-web-yew">vectorcade-web-yew</a></li>
</ul>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 3 of the Throwback Thursday series. <a href="/series/#throwback-thursday">View all parts</a></td>
      <td>Next: <a href="/2026/02/19/tbt-toontalk-visual-programming/">TBT (4/?): ToonTalk</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Before pixels, there were vectors. Vectorcade brings them back—in Rust, for the browser, with phosphor glow optional.</em></p>

<div class="resource-box">

  <p><strong>Credits</strong></p>

  <table>
    <thead>
      <tr>
        <th>Role</th>
        <th>Credit</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Director</strong></td>
        <td>Mike Wright</td>
      </tr>
      <tr>
        <td><strong>Research &amp; Architecture</strong></td>
        <td>ChatGPT</td>
      </tr>
      <tr>
        <td><strong>vectorcade-shared</strong></td>
        <td>Claude Code CLI agent</td>
      </tr>
      <tr>
        <td><strong>vectorcade-fonts</strong></td>
        <td>Claude Code CLI agent</td>
      </tr>
      <tr>
        <td><strong>vectorcade-games</strong></td>
        <td>Claude Code CLI agent</td>
      </tr>
      <tr>
        <td><strong>vectorcade-render-wgpu</strong></td>
        <td>Claude Code CLI agent</td>
      </tr>
      <tr>
        <td><strong>vectorcade-web-yew</strong></td>
        <td>Claude Code CLI agent</td>
      </tr>
      <tr>
        <td><strong>Explainer Video</strong></td>
        <td>Claude Code</td>
      </tr>
      <tr>
        <td><strong>Blog Post</strong></td>
        <td>Claude Code</td>
      </tr>
    </tbody>
  </table>

  <p><strong>Timeline</strong>: First pass vibe coded in one day (February 12, 2026)</p>
  <ul>
    <li>First commit: 11:08 AM PST</li>
    <li>Last commit: 5:08 PM PST</li>
    <li>Total commits: 52 across 4 repositories</li>
    <li>WGPU support added February 13, 2026</li>
  </ul>

</div>

<h2 id="references">References</h2>

<p><strong>IBM 2250 Photo</strong>: “<a href="https://commons.wikimedia.org/wiki/File:HES_IBM_2250_Console_grlloyd_Oct1969.png">HES IBM 2250 Console grlloyd Oct1969</a>” by Gregory Lloyd, October 1969. Brown University Hypertext Editing System (HES) demonstration. Licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>. Used with attribution.</p>


          </div>





<div class="youtube-embed-container" id="yt-container-lxEFBzDjp3A">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-lxEFBzDjp3A"
      src="https://www.youtube.com/embed/lxEFBzDjp3A?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-lxEFBzDjp3A';
  const playerId = 'yt-player-lxEFBzDjp3A';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="22"><p class="post-meta">February 12, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">781 words</span> &bull; <span class="post-read-time">4 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">When multiple AI agents work together, fixed communication patterns fail at scale. DyTopo rebuilds the graph each round based on semantic similarity between what agents need and what they can offer, preventing context explosion while enabling adaptive collaboration.</div><h3>
          <a class="post-link" href="/2026/02/12/dytopo-rs-dynamic-topology-multi-agent/">
            DyTopo: Dynamic Topology for Multi-Agent AI
          </a>
        </h3><nav class="toc" data-toc-id="dytopo-rs-dynamic-topology-multi-agent">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-team.png" class="post-marker" alt="" /></p>

<p>When multiple AI agents work together, how should they communicate? Fixed patterns fail at scale. DyTopo rebuilds the communication graph each round based on what agents need and what they can offer.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/_8_08H97LxM">DyTopo</a><br /><a href="https://www.youtube.com/shorts/_8_08H97LxM"><img src="https://img.youtube.com/vi/_8_08H97LxM/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2505.16128">arXiv:2505.16128</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/dytopo-rs">dytopo-rs</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-problem-fixed-topologies-dont-scale">The Problem: Fixed Topologies Don’t Scale</h2>

<p>Multi-agent systems need communication patterns. The obvious approaches have problems:</p>

<table>
  <thead>
    <tr>
      <th>Topology</th>
      <th>Problem</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>All-to-all</strong></td>
      <td>Context explosion—every agent reads every message</td>
    </tr>
    <tr>
      <td><strong>Chain</strong></td>
      <td>Bottlenecks—one slow agent blocks everyone</td>
    </tr>
    <tr>
      <td><strong>Star</strong></td>
      <td>Single point of failure at the hub</td>
    </tr>
  </tbody>
</table>

<p>As agent count grows, fixed topologies either explode in messages or create chokepoints.</p>

<h2 id="the-dytopo-solution-dynamic-routing">The DyTopo Solution: Dynamic Routing</h2>

<p>DyTopo (Dynamic Topology) solves this by reconstructing the communication graph each round. The key insight: <strong>agents know what they need and what they can offer</strong>.</p>

<p>Each round, every agent emits:</p>
<ul>
  <li><strong>Query</strong>: What information do I need?</li>
  <li><strong>Key</strong>: What can I contribute?</li>
</ul>

<p>The router computes semantic similarity between all keys and queries, then builds a <strong>sparse directed graph</strong>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>score(sender → receiver) = cosine(sender.key, receiver.query)
</code></pre></div></div>

<p>High-scoring pairs connect. Low-scoring pairs are ignored. The result: efficient, adaptive communication.</p>

<h2 id="how-it-works">How It Works</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Round N:
  1. Manager broadcasts goal
  2. Each agent produces:
     - Query (what I need)
     - Key (what I offer)
     - Draft (my current contribution)
  3. Router embeds keys and queries
  4. Similarity matrix → sparse graph (top-K per receiver)
  5. Messages flow along edges
  6. Trace written to JSONL
</code></pre></div></div>

<p>The topology adapts every round. An agent working on parsing might connect to the syntax expert in round 1, then the error-handling expert in round 2.</p>

<h2 id="the-implementation-rust-zero-python">The Implementation: Rust, Zero Python</h2>

<p>dytopo-rs is a <strong>fully Rust implementation</strong> with no Python dependencies:</p>

<table>
  <thead>
    <tr>
      <th>Crate</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dytopo-core</code></td>
      <td>Shared types (AgentId, Topology, TraceEvent)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dytopo-embed</code></td>
      <td>Text embedding (hash-based baseline, semantic planned)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dytopo-router</code></td>
      <td>Sparse graph construction</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dytopo-agents</code></td>
      <td>Agent implementations</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dytopo-orchestrator</code></td>
      <td>Main execution loop</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dytopo-viz</code></td>
      <td>DOT export for visualization</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dytopo-cli</code></td>
      <td>Command-line interface</td>
    </tr>
  </tbody>
</table>

<h3 id="why-rust">Why Rust?</h3>

<ol>
  <li><strong>Zero-cost abstractions</strong> for performance-critical embedding/routing</li>
  <li><strong>Strong type system</strong> catches protocol mismatches at compile time</li>
  <li><strong>No Python dependency</strong> for baseline demos</li>
  <li><strong>Fearless concurrency</strong> for future parallelization</li>
</ol>

<h2 id="running-the-demo">Running the Demo</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cargo run <span class="nt">-p</span> dytopo-cli <span class="nt">--</span> demo <span class="nt">--rounds</span> 3 <span class="nt">--agents</span> 5 <span class="nt">--topk</span> 2
</code></pre></div></div>

<p>This produces:</p>
<ul>
  <li>Per-round topology printed to stdout</li>
  <li><code class="language-plaintext highlighter-rouge">./traces/trace_*.jsonl</code> for machine-readable analysis</li>
  <li>DOT files for graph visualization</li>
</ul>

<h2 id="current-status">Current Status</h2>

<p><strong>Milestone 0 is complete</strong>—the system runs end-to-end with stub agents and hash-based embeddings.</p>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>Status</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Core types and traits</td>
      <td>Done</td>
    </tr>
    <tr>
      <td>Hash embedder (deterministic)</td>
      <td>Done</td>
    </tr>
    <tr>
      <td>Top-K sparse routing</td>
      <td>Done</td>
    </tr>
    <tr>
      <td>Stub agents with templates</td>
      <td>Done</td>
    </tr>
    <tr>
      <td>Orchestrator loop</td>
      <td>Done</td>
    </tr>
    <tr>
      <td>JSONL tracing</td>
      <td>Done</td>
    </tr>
    <tr>
      <td>DOT visualization</td>
      <td>Done</td>
    </tr>
  </tbody>
</table>

<h3 id="planned">Planned</h3>

<ul>
  <li>Semantic embeddings (fastembed/candle)</li>
  <li>LLM-backed agents (Ollama integration)</li>
  <li>Inbox summarization for long conversations</li>
  <li>Evaluation harness comparing topologies</li>
</ul>

<h2 id="key-design-decisions">Key Design Decisions</h2>

<h3 id="why-hash-embeddings-first">Why Hash Embeddings First?</h3>

<p>The baseline uses deterministic hash-based embeddings:</p>
<ul>
  <li><strong>Reproducible</strong> demos for debugging</li>
  <li><strong>No external dependencies</strong> to download</li>
  <li>Validates the full pipeline before adding ML complexity</li>
</ul>

<p>Semantic embeddings are planned as drop-in replacements.</p>

<h3 id="why-sparse-graphs">Why Sparse Graphs?</h3>

<p>Each agent receives at most <code class="language-plaintext highlighter-rouge">topk</code> messages per round:</p>
<ul>
  <li><strong>Prevents context explosion</strong> as agent count grows</li>
  <li><strong>Makes communication interpretable</strong>—you can trace why agents connected</li>
  <li><strong>Matches the paper’s approach</strong></li>
</ul>

<h3 id="why-jsonl-traces">Why JSONL Traces?</h3>

<p>Every event is logged to JSONL:</p>
<ul>
  <li><strong>Append-only</strong> for streaming</li>
  <li><strong>Line-based</strong> for grep/filtering</li>
  <li><strong>Machine-parseable</strong> for analysis tools</li>
  <li><strong>Human-readable</strong> for debugging</li>
</ul>

<h2 id="topology-comparison">Topology Comparison</h2>

<p>The system supports multiple topology strategies for comparison:</p>

<table>
  <thead>
    <tr>
      <th>Strategy</th>
      <th>Description</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dynamic</code></td>
      <td>DyTopo routing</td>
      <td>Adaptive, sparse</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">fully_connected</code></td>
      <td>All-to-all</td>
      <td>Baseline comparison</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">chain</code></td>
      <td>Sequential</td>
      <td>Pipeline tasks</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">star</code></td>
      <td>Hub-and-spoke</td>
      <td>Centralized coordination</td>
    </tr>
  </tbody>
</table>

<h2 id="whats-next">What’s Next</h2>

<ol>
  <li><strong>LLM Agent Support</strong> (Milestone 2)—Replace stubs with real reasoning</li>
  <li><strong>Semantic Embeddings</strong> (Milestone 1)—Meaningful routing decisions</li>
  <li><strong>Evaluation Harness</strong> (Milestone 4)—Quantify DyTopo advantages</li>
</ol>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2505.16128">DyTopo Paper (arXiv:2505.16128)</a> - Li et al., 2025</li>
  <li><a href="https://github.com/softwarewrighter/dytopo-rs">dytopo-rs Repository</a></li>
</ul>

<hr />

<p><em>Dynamic topology lets agents find the right collaborators each round. No context explosion. No bottlenecks. Just efficient, adaptive communication.</em></p>


          </div>





<div class="youtube-embed-container" id="yt-container-_8_08H97LxM">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-_8_08H97LxM"
      src="https://www.youtube.com/embed/_8_08H97LxM?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-_8_08H97LxM';
  const playerId = 'yt-player-_8_08H97LxM';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="23"><p class="post-meta">February 12, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">1224 words</span> &bull; <span class="post-read-time">7 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">What happens when you fine-tune a model on new tasks? It forgets old ones. This post documents our implementation of the Share algorithm in Rust—using SVD-based subspace extraction to enable continual learning without catastrophic forgetting. Part 1 covers the problem and initial negative results.</div><h3>
          <a class="post-link" href="/2026/02/12/sleepy-coder-when-fine-tuning-fails/">
            Towards Continuous LLM Learning (1): Sleepy Coder - When Fine-Tuning Fails
          </a>
        </h3><nav class="toc" data-toc-id="sleepy-coder-when-fine-tuning-fails">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/sleeper-dreaming.png" class="post-marker" alt="" /></p>

<p>What if your AI coding assistant could learn from its mistakes? Not just for one session, but across training cycles. We built exactly that—and fifty-one adapters later, learned the mistake was trying to teach it at all.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://youtu.be/YhAbOvWEkzE">Sleepy Coder</a><br /><a href="https://youtu.be/YhAbOvWEkzE"><img src="https://img.youtube.com/vi/YhAbOvWEkzE/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/sleepy-coder">sleepy-coder</a></td>
      </tr>
      <tr>
        <td><strong>Share Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2602.06043">arXiv:2602.06043</a></td>
      </tr>
      <tr>
        <td><strong>UWSH Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2512.05117">arXiv:2512.05117</a></td>
      </tr>
      <tr>
        <td><strong>Part 2</strong></td>
        <td><a href="/2026/02/18/sleepy-coder-routing-prevents-forgetting/">Routing Prevents Forgetting</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-dream-daynight-learning">The Dream: Day/Night Learning</h2>

<p>AI coding agents have a memory problem. They fix a bug today, then make the same mistake next week. Every session starts from the same frozen model. Nothing carries forward.</p>

<p>The idea was elegant: build an agent that improves overnight.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DAY CYCLE (Inference)
  Agent attempts to fix Rust compiler errors
  Successes and failures are logged
        ↓
NIGHT CYCLE (Training)
  Fine-tune on failure patterns using LoRA
  Create specialized adapters
        ↓
EVAL
  Test against benchmark
  Measure improvement
        ↓
(repeat)
</code></pre></div></div>

<p>During the day, the agent works and we log its failures—the error messages, the broken code, and the fixes that worked. Overnight, we fine-tune the model on those failures. Each morning, a new checkpoint should wake up a little better than before.</p>

<p>We based this on two papers from the Johns Hopkins team (Kaushik, Vaidya, Chaudhari, Chellappa, Yuille):</p>

<ol>
  <li>
    <p><strong>Share LoRA Subspaces</strong> (arXiv:2602.06043) — Learn a shared low-rank basis across tasks, then train only coefficients (76x fewer parameters per task)</p>
  </li>
  <li>
    <p><strong>UWSH</strong> (arXiv:2512.05117) — The Universal Weight Subspace Hypothesis suggests neural networks converge to shared spectral subspaces</p>
  </li>
</ol>

<p>The theory was sound. The implementation worked. The results were devastating.</p>

<h2 id="the-system">The System</h2>

<p>The Sleepy Coder agent runs in a Rust runtime, fixing compiler errors on 30 “koans” (small coding exercises) across 5 error families:</p>

<ul>
  <li><strong>Borrow Checker</strong>: Ownership and lifetime errors</li>
  <li><strong>Type Bounds</strong>: Missing trait implementations</li>
  <li><strong>Result Handling</strong>: Option/Result conversions</li>
  <li><strong>Type Mismatches</strong>: Incompatible types</li>
  <li><strong>Missing Items</strong>: Undefined functions or modules</li>
</ul>

<p>The base model: <strong>Qwen2.5-Coder-1.5B-Instruct</strong> — small enough to train on a single GPU, capable enough to pass most koans without any fine-tuning.</p>

<h2 id="the-journey-from-hope-to-reality">The Journey: From Hope to Reality</h2>

<h3 id="chapter-1-naive-lora">Chapter 1: Naive LoRA</h3>

<p>First attempt: standard fine-tuning on failure patterns.</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Before</th>
      <th>After</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Pass Rate</td>
      <td>73.3%</td>
      <td>60.0%</td>
    </tr>
    <tr>
      <td>Change</td>
      <td>—</td>
      <td>-13.3%</td>
    </tr>
  </tbody>
</table>

<p>Catastrophic forgetting. The model learned the new patterns but forgot how to do everything else.</p>

<h3 id="chapter-2-the-paper-chase">Chapter 2: The Paper Chase</h3>

<p>We found the Share paper promising “continual learning without forgetting.” The UWSH paper provided theoretical backing: neural networks naturally converge to shared low-rank subspaces.</p>

<p>Key insight from Share:</p>
<blockquote>
  <p>Train ONLY the coefficients. Keep the basis FROZEN.</p>
</blockquote>

<p>This meant ~21,000 trainable parameters instead of ~1.6 million. A 76x reduction.</p>

<h3 id="chapter-3-the-proper-implementation">Chapter 3: The Proper Implementation</h3>

<div class="definition-box">
  <p><strong>SVD: Singular Value Decomposition</strong> breaks a matrix into components that reveal its underlying structure. In Share, SVD finds the common “directions” that multiple LoRA adapters share—a compressed basis that captures what they have in common.</p>
</div>

<p>We rebuilt everything:</p>

<ul>
  <li><strong>Phase 1:</strong> Extract shared basis from 51 adapters via SVD</li>
  <li><strong>Phase 2:</strong> Train only coefficient vectors (frozen basis)</li>
  <li><strong>Phase 3:</strong> Merge and update basis periodically</li>
</ul>

<p>We trained 51 pattern-specific adapters. We followed the algorithm precisely.</p>

<h3 id="chapter-4-the-stubborn-seven">Chapter 4: The Stubborn Seven</h3>

<p>No matter what we tried, 7 tasks kept failing:</p>

<table>
  <thead>
    <tr>
      <th>Task</th>
      <th>The Problem</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>bc_003</td>
      <td>Mutable borrow while immutable exists</td>
    </tr>
    <tr>
      <td>bc_005</td>
      <td>Double mutable borrow</td>
    </tr>
    <tr>
      <td>bc_010</td>
      <td>Returning reference to local data</td>
    </tr>
    <tr>
      <td>tb_002</td>
      <td>Missing Clone trait</td>
    </tr>
    <tr>
      <td>tb_007</td>
      <td>Missing Hash trait</td>
    </tr>
    <tr>
      <td>tb_008</td>
      <td>Missing Ord trait</td>
    </tr>
    <tr>
      <td>rh_004</td>
      <td>Option to Result conversion</td>
    </tr>
  </tbody>
</table>

<p>These require deep understanding of Rust’s ownership system—something a 1.5B model can’t reliably learn.</p>

<h3 id="chapter-5-the-final-score">Chapter 5: The Final Score</h3>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>Pass Rate</th>
      <th>vs Baseline</th>
      <th>Regressions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline (no training)</td>
      <td>73.3%</td>
      <td>—</td>
      <td>0</td>
    </tr>
    <tr>
      <td>Naive LoRA</td>
      <td>60.0%</td>
      <td>-13.3%</td>
      <td>Many</td>
    </tr>
    <tr>
      <td>Targeted LoRA (7 patterns)</td>
      <td>63.3%</td>
      <td>-10%</td>
      <td>4+</td>
    </tr>
    <tr>
      <td>Replay buffer</td>
      <td>70.0%</td>
      <td>-3.3%</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Phase 2 coef-only (10K params)</td>
      <td>66.7%</td>
      <td>-6.6%</td>
      <td>2</td>
    </tr>
    <tr>
      <td><strong>Share Full (Ph2+Ph3)</strong></td>
      <td><strong>73.3%</strong></td>
      <td><strong>0%</strong></td>
      <td><strong>0</strong></td>
    </tr>
  </tbody>
</table>

<p>The Share algorithm did exactly what it claimed: it prevented forgetting. But it couldn’t improve beyond baseline because <strong>there was nothing to improve.</strong></p>

<h2 id="what-went-wrong">What Went Wrong</h2>

<h3 id="1-the-model-already-knows">1. The Model Already Knows</h3>

<p>The base model already passes 73% of patterns. Training on these patterns doesn’t add knowledge—it dilutes what’s there.</p>

<h3 id="2-training-causes-forgetting">2. Training Causes Forgetting</h3>

<p>Even training only on the 7 failure patterns (44 examples) caused 4 new regressions. The model’s knowledge is interconnected.</p>

<h3 id="3-averaging-destroys-specialization">3. Averaging Destroys Specialization</h3>

<p>The Share paper assumes task routing at inference—selecting the right coefficients for each task. We averaged coefficients, which negated any specialization.</p>

<h3 id="4-more-adapters-made-it-worse">4. More Adapters Made It Worse</h3>

<table>
  <thead>
    <tr>
      <th>Adapter Count</th>
      <th>Pass Rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>6 adapters</td>
      <td>73.3%</td>
    </tr>
    <tr>
      <td>51 adapters</td>
      <td>70.0%</td>
    </tr>
  </tbody>
</table>

<p>More adapters meant more subspace dilution when averaging. The signal got lost in the noise.</p>

<h2 id="the-critical-insight">The Critical Insight</h2>

<p><strong>LoRA fine-tuning cannot improve a capable base model for tasks it already handles reasonably well.</strong></p>

<p>The model’s knowledge is interconnected. Even 10,000 trainable parameters (0.0007% of the model) can break things. The baseline represents the ceiling, not the floor.</p>

<h2 id="what-we-learned">What We Learned</h2>

<ol>
  <li>
    <p><strong>Read the room.</strong> If your base model passes 73%, maybe it doesn’t need fine-tuning. Maybe it needs better prompts.</p>
  </li>
  <li>
    <p><strong>Negative results are results.</strong> 51 failed experiments taught us more than a successful one would have.</p>
  </li>
  <li>
    <p><strong>Catastrophic forgetting is real.</strong> Small models especially can’t absorb new knowledge without losing old.</p>
  </li>
  <li>
    <p><strong>Share prevents forgetting, not ignorance.</strong> The algorithm does what it claims—it just can’t create knowledge from nothing.</p>
  </li>
  <li>
    <p><strong>Sometimes the answer is “don’t.”</strong> The best LoRA adapter for this task is no adapter.</p>
  </li>
  <li>
    <p><strong>Task routing vs averaging matters.</strong> The Share paper assumes you select coefficients based on task type, not blend them together.</p>
  </li>
  <li>
    <p><strong>AI coding agents cut corners.</strong> When implementing research papers, AI agents repeatedly stopped before completing all phases of the algorithm. I had to direct the agent to re-read the papers many times before it implemented them correctly.</p>
  </li>
</ol>

<h2 id="paths-forward">Paths Forward</h2>

<p>Since fine-tuning doesn’t work here, alternatives:</p>

<table>
  <thead>
    <tr>
      <th>Approach</th>
      <th>Tradeoff</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Prompt engineering</td>
      <td>No weight changes, limited by context</td>
    </tr>
    <tr>
      <td>Multi-turn repair</td>
      <td>Uses base model reasoning, slower</td>
    </tr>
    <tr>
      <td>Larger model (7B+)</td>
      <td>More capacity to absorb knowledge</td>
    </tr>
    <tr>
      <td>Task routing with Share</td>
      <td>Select coefficients, don’t average</td>
    </tr>
    <tr>
      <td>Model ensemble</td>
      <td>Multiple models, pick best output</td>
    </tr>
    <tr>
      <td>Accept baseline</td>
      <td>73% may be good enough</td>
    </tr>
  </tbody>
</table>

<h2 id="the-numbers">The Numbers</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Experiments run:        51 adapters, multiple algorithms
Parameters trained:     From 10K to 1.6M per adapter
Best achieved:          73.3% (matches baseline)
Target:                 ≥76.7%
Conclusion:             Target not achievable with LoRA
</code></pre></div></div>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://github.com/softwarewrighter/sleepy-coder">sleepy-coder Repository</a></li>
  <li><a href="https://arxiv.org/abs/2602.06043">Share LoRA Subspaces Paper (arXiv:2602.06043)</a></li>
  <li><a href="https://arxiv.org/abs/2512.05117">UWSH Paper (arXiv:2512.05117)</a></li>
</ul>

<hr />

<p><em>Part 1 of the Towards Continuous LLM Learning series. <a href="/series/#towards-continuous-llm-learning">View all parts</a></em></p>

<p><em>Sometimes the most valuable research shows what doesn’t work. Fifty-one adapters later, we know: let sleeping models lie.</em></p>


          </div>





<div class="youtube-embed-container" id="yt-container-YhAbOvWEkzE">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-YhAbOvWEkzE"
      src="https://www.youtube.com/embed/YhAbOvWEkzE?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-YhAbOvWEkzE';
  const playerId = 'yt-player-YhAbOvWEkzE';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="24"><p class="post-meta">February 12, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">492 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Dropout (random disabling prevents overfitting), RLHF (learn from human preferences), Inference (using trained models), Quantization (lower precision for efficiency), Flash Attention (block-wise for memory savings).</div><h3>
          <a class="post-link" href="/2026/02/12/five-ml-concepts-9/">
            Five ML Concepts - #9
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-9">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-nine.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/C5ICKluukxI">Five ML Concepts #9</a><br /><a href="https://www.youtube.com/shorts/C5ICKluukxI"><img src="https://img.youtube.com/vi/C5ICKluukxI/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Dropout</strong></td>
        <td><a href="https://jmlr.org/papers/v15/srivastava14a.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a> (Srivastava et al. 2014)</td>
      </tr>
      <tr>
        <td><strong>RLHF</strong></td>
        <td><a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a> (Ouyang et al. 2022)</td>
      </tr>
      <tr>
        <td><strong>Inference</strong></td>
        <td><a href="https://www.deeplearningbook.org/">Deep Learning</a> (Goodfellow et al. 2016), Chapter 5</td>
      </tr>
      <tr>
        <td><strong>Quantization</strong></td>
        <td><a href="https://arxiv.org/abs/2103.13630">A Survey of Quantization Methods for Efficient Neural Network Inference</a> (Gholami et al. 2021)</td>
      </tr>
      <tr>
        <td><strong>Flash Attention</strong></td>
        <td><a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention</a> (Dao et al. 2022)</td>
      </tr>
    </tbody>
  </table>

</div>

<div style="display:none">

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-dropout">1. Dropout</h3>

<p><strong>A regularization technique that randomly disables units during training.</strong> This encourages the network to rely on multiple pathways instead of memorizing patterns.</p>

<p>It helps reduce overfitting, especially in large models.</p>

<blockquote>
  <p>Like training a team where random members sit out each practice, so no one becomes a single point of failure.</p>
</blockquote>

<h3 id="2-rlhf-reinforcement-learning-from-human-feedback">2. RLHF (Reinforcement Learning from Human Feedback)</h3>

<p><strong>A training approach where humans rank or compare model outputs to produce a reward signal.</strong> The model is then optimized to better match human preferences.</p>

<p>This technique is central to aligning language models with human intent.</p>

<blockquote>
  <p>Like teaching by grading essays instead of dictating every word.</p>
</blockquote>

<h3 id="3-inference">3. Inference</h3>

<p><strong>The process of running a trained model to make predictions on new data.</strong> Training updates the model’s parameters; inference uses them.</p>

<p>The distinction matters for optimization, deployment, and cost.</p>

<blockquote>
  <p>Like the difference between studying for an exam and actually taking it.</p>
</blockquote>

<h3 id="4-quantization">4. Quantization</h3>

<p><strong>Reducing the numerical precision used to store and compute model weights.</strong> This can shrink model size and speed up inference, sometimes with a small accuracy tradeoff.</p>

<p>Essential for deploying large models on limited hardware.</p>

<blockquote>
  <p>Like compressing a high-resolution photo into a smaller file that still looks good.</p>
</blockquote>

<h3 id="5-flash-attention">5. Flash Attention</h3>

<p><strong>An optimized attention algorithm designed to reduce memory usage.</strong> It avoids materializing the full attention matrix by computing attention in blocks.</p>

<p>This enables longer sequences and faster training.</p>

<blockquote>
  <p>Like reading a book chapter by chapter instead of photocopying the whole thing first.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Dropout</strong></td>
      <td>Random disabling to prevent overfitting</td>
    </tr>
    <tr>
      <td><strong>RLHF</strong></td>
      <td>Learn from human preference comparisons</td>
    </tr>
    <tr>
      <td><strong>Inference</strong></td>
      <td>Using a trained model for predictions</td>
    </tr>
    <tr>
      <td><strong>Quantization</strong></td>
      <td>Lower precision for smaller, faster models</td>
    </tr>
    <tr>
      <td><strong>Flash Attention</strong></td>
      <td>Block-wise attention for memory efficiency</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 9 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/13/five-ml-concepts-10/">Next: #10 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-C5ICKluukxI">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-C5ICKluukxI"
      src="https://www.youtube.com/embed/C5ICKluukxI?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-C5ICKluukxI';
  const playerId = 'yt-player-C5ICKluukxI';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="25"><p class="post-meta">February 11, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">499 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Bias-Variance Tradeoff (balance under/overfitting), Diffusion (generate by learning to denoise), KV Cache (store past keys/values), Mixed Precision (lower precision for speed), MLA (compress attention into latent space).</div><h3>
          <a class="post-link" href="/2026/02/11/five-ml-concepts-8/">
            Five ML Concepts - #8
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-8">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-eight.png" class="post-marker no-invert" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/iR4DYyzcdk8">Five ML Concepts #8</a><br /><a href="https://www.youtube.com/shorts/iR4DYyzcdk8"><img src="https://img.youtube.com/vi/iR4DYyzcdk8/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Bias-Variance</strong></td>
        <td><a href="https://hastie.su.domains/ElemStatLearn/">The Elements of Statistical Learning</a> (Hastie et al. 2009), Chapter 7</td>
      </tr>
      <tr>
        <td><strong>Diffusion</strong></td>
        <td><a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a> (Ho et al. 2020)</td>
      </tr>
      <tr>
        <td><strong>KV Cache</strong></td>
        <td><a href="https://arxiv.org/abs/2211.05102">Fast Transformer Decoding</a> (Pope et al. 2022)</td>
      </tr>
      <tr>
        <td><strong>Mixed Precision</strong></td>
        <td><a href="https://arxiv.org/abs/1710.03740">Mixed Precision Training</a> (Micikevicius et al. 2017)</td>
      </tr>
      <tr>
        <td><strong>MLA</strong></td>
        <td><a href="https://arxiv.org/abs/2405.04434">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</a> (DeepSeek-AI 2024)</td>
      </tr>
    </tbody>
  </table>

</div>

<div style="display:none">

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-bias-variance-tradeoff">1. Bias-Variance Tradeoff</h3>

<p><strong>A fundamental tension where simpler models tend to underfit (high bias), and more flexible models can overfit (high variance).</strong> The goal is finding a balance that generalizes well to unseen data.</p>

<p>One of the oldest ideas in machine learning, still relevant today.</p>

<blockquote>
  <p>Like choosing between a ruler that only draws straight lines and one so flexible it traces every bump.</p>
</blockquote>

<h3 id="2-diffusion-models">2. Diffusion Models</h3>

<p><strong>A generative approach that trains a model to reverse a gradual noising process.</strong> During generation, the model starts from noise and removes it step by step.</p>

<p>The foundation of image generators like Stable Diffusion and DALL-E.</p>

<blockquote>
  <p>Like learning to restore a photo by practicing on progressively more damaged versions.</p>
</blockquote>

<h3 id="3-kv-cache">3. KV Cache</h3>

<p><strong>A technique that stores attention key and value tensors from earlier tokens so they don’t need to be recomputed during generation.</strong> This significantly speeds up autoregressive inference.</p>

<p>Essential for efficient LLM serving.</p>

<blockquote>
  <p>Like keeping notes from earlier in a conversation instead of rereading everything.</p>
</blockquote>

<h3 id="4-mixed-precision">4. Mixed Precision</h3>

<p><strong>A training strategy that uses lower-precision math for most operations, while keeping some calculations in higher precision for stability.</strong> This reduces memory use and often speeds up training with little accuracy loss.</p>

<p>Standard practice for modern deep learning.</p>

<blockquote>
  <p>Like drafting in pencil and only using ink for the final signature.</p>
</blockquote>

<h3 id="5-mla-multi-head-latent-attention">5. MLA (Multi-head Latent Attention)</h3>

<p><strong>An attention variant that compresses key and value information into a lower-dimensional latent space.</strong> This reduces memory usage for long sequences while retaining useful context.</p>

<p>Used in DeepSeek-V2 and related architectures.</p>

<blockquote>
  <p>Like summarizing meeting notes instead of recording every word verbatim.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Bias-Variance</strong></td>
      <td>Balance underfitting vs overfitting</td>
    </tr>
    <tr>
      <td><strong>Diffusion</strong></td>
      <td>Generate by learning to denoise</td>
    </tr>
    <tr>
      <td><strong>KV Cache</strong></td>
      <td>Store past keys/values for fast inference</td>
    </tr>
    <tr>
      <td><strong>Mixed Precision</strong></td>
      <td>Lower precision for speed, higher for stability</td>
    </tr>
    <tr>
      <td><strong>MLA</strong></td>
      <td>Compress attention into latent space</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 8 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/12/five-ml-concepts-9/">Next: #9 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-iR4DYyzcdk8">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-iR4DYyzcdk8"
      src="https://www.youtube.com/embed/iR4DYyzcdk8?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-iR4DYyzcdk8';
  const playerId = 'yt-player-iR4DYyzcdk8';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="26"><p class="post-meta">February 11, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">1046 words</span> &bull; <span class="post-read-time">6 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">From behavioral emulation to real implementation: integrating hash-based Engram memory with HuggingFace models. The gating mechanism is critical---it learns when to trust memory lookup and when hash collisions would add noise. Engram excels at exact-match retrieval, not generalization.</div><h3>
          <a class="post-link" href="/2026/02/11/deepseek-papers-part3-engram-revisited/">
            Deepseek Papers (3/3): Engram Revisited - From Emulation to Implementation
          </a>
        </h3><nav class="toc" data-toc-id="deepseek-papers-part3-engram-revisited">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-gate.png" class="post-marker" alt="" /></p>

<p>We started by training models to <em>act</em> like they had memory. Then we found an open source implementation that does it for real. This is what we learned.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2601.07372">arXiv:2601.07372</a></td>
      </tr>
      <tr>
        <td><strong>Our Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/engram-poc">engram-poc</a></td>
      </tr>
      <tr>
        <td><strong>Reference</strong></td>
        <td><a href="https://github.com/weagan/Engram">weagan/Engram</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/watch?v=TZT_cWWv9Oc">Engram Revisited</a><br /><a href="https://www.youtube.com/watch?v=TZT_cWWv9Oc"><img src="https://img.youtube.com/vi/TZT_cWWv9Oc/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>Playlist</strong></td>
        <td><a href="https://www.youtube.com/playlist?list=PLKjvVAEaR4isTOri5dlPRIUK8Uy0jotX6">All Engram Videos</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-journey">The Journey</h2>

<h3 id="phase-1-behavioral-emulation">Phase 1: Behavioral Emulation</h3>

<p><a href="/2026/02/02/deepseek-papers-part2-engram/">Part 2</a> described our first approach: LoRA fine-tuning to make a model <em>behave</em> like it has memory. Train on patterns, and the model learns to respond consistently.</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Baseline</th>
      <th>LoRA-tuned</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Accuracy</td>
      <td>8.6%</td>
      <td>14.1%</td>
    </tr>
    <tr>
      <td>Improvement</td>
      <td>-</td>
      <td>+63% relative</td>
    </tr>
  </tbody>
</table>

<p>It worked, but the architecture was unchanged. We were approximating Engram benefits, not implementing them.</p>

<h3 id="phase-2-the-discovery">Phase 2: The Discovery</h3>

<p>Then we found <a href="https://github.com/weagan/Engram">weagan/Engram</a> on GitHub—real hash-based memory in ~300 lines of Python:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">EnhancedEngramModule</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">table_size</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">):</span>
        <span class="c1"># Large learnable memory table
</span>        <span class="n">self</span><span class="p">.</span><span class="n">memory_table</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">table_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>

        <span class="c1"># Gate decides when to trust memory
</span>        <span class="n">self</span><span class="p">.</span><span class="n">gate</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">):</span>
        <span class="c1"># O(1) hash lookup
</span>        <span class="n">indices</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">multi_head_hash</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
        <span class="n">retrieved</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">embedding</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">memory_table</span><span class="p">)</span>

        <span class="c1"># Gated injection
</span>        <span class="n">gate_score</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gate</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">retrieved</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">hidden_states</span> <span class="o">+</span> <span class="n">gate_score</span> <span class="o">*</span> <span class="n">retrieved</span>
</code></pre></div></div>

<p>The key insight: <strong>the gate decides when to trust the lookup</strong>. Not every token needs memory.</p>

<h3 id="phase-3-integration-with-huggingface">Phase 3: Integration with HuggingFace</h3>

<p>We ported the module to work with HuggingFace models:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SmolLM-135M (frozen)
        ↓
EnhancedEngramModule (per layer)
  - 50K slot memory table
  - O(1) hash-based lookup
  - Learned gating
        ↓
Output
</code></pre></div></div>

<p>The proof it works—O(1) lookup regardless of sequence length:</p>

<table>
  <thead>
    <tr>
      <th>Sequence Length</th>
      <th>Lookup Time</th>
      <th>Expected if O(n)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>64 tokens</td>
      <td>0.15 ms</td>
      <td>-</td>
    </tr>
    <tr>
      <td>2048 tokens</td>
      <td>2.77 ms</td>
      <td>4.8 ms</td>
    </tr>
  </tbody>
</table>

<p>Sub-linear scaling proves constant-time hash lookup.</p>

<h2 id="the-reality-check">The Reality Check</h2>

<p>Here’s where it gets interesting. Real Engram memory <strong>excels at some tasks and hurts others</strong>.</p>

<h3 id="where-engram-helps">Where Engram Helps</h3>

<table>
  <thead>
    <tr>
      <th>Task Type</th>
      <th>Baseline</th>
      <th>Engram</th>
      <th>Change</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Acronym expansion</td>
      <td>25%</td>
      <td>75%</td>
      <td>+200%</td>
    </tr>
    <tr>
      <td>Element symbols</td>
      <td>33%</td>
      <td>67%</td>
      <td>+103%</td>
    </tr>
    <tr>
      <td>Long-term fact recall</td>
      <td>90%</td>
      <td>100%</td>
      <td>+11%</td>
    </tr>
  </tbody>
</table>

<p>For <strong>exact-match lookups</strong> with structured keys, Engram dominates.</p>

<h3 id="where-engram-hurts">Where Engram Hurts</h3>

<table>
  <thead>
    <tr>
      <th>Task Type</th>
      <th>Baseline</th>
      <th>Engram</th>
      <th>Change</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>World capitals</td>
      <td>83%</td>
      <td>67%</td>
      <td>-19%</td>
    </tr>
    <tr>
      <td>Pattern completion</td>
      <td>14%</td>
      <td>11%</td>
      <td>-21%</td>
    </tr>
  </tbody>
</table>

<p>For tasks where the base model already knows the answer, Engram’s hash collisions add noise.</p>

<h2 id="the-key-insight">The Key Insight</h2>

<p><strong>Engram is a specialized tool, not a general enhancement.</strong></p>

<table>
  <thead>
    <tr>
      <th>Use Engram For</th>
      <th>Don’t Use Engram For</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>FAQ responses</td>
      <td>Creative generation</td>
    </tr>
    <tr>
      <td>Terminology lookup</td>
      <td>Novel combinations</td>
    </tr>
    <tr>
      <td>Entity facts</td>
      <td>Context-dependent answers</td>
    </tr>
    <tr>
      <td>Code boilerplate</td>
      <td>Reasoning tasks</td>
    </tr>
  </tbody>
</table>

<p>The gating mechanism is critical: it must learn to <strong>suppress memory when it doesn’t help</strong>. Without proper gating, hash collisions inject noise into every token.</p>

<h2 id="obstacles-encountered">Obstacles Encountered</h2>

<h3 id="1-hash-collisions">1. Hash Collisions</h3>

<p>Different inputs can map to the same memory slot. The gate must learn to ignore irrelevant retrievals.</p>

<h3 id="2-parameter-explosion">2. Parameter Explosion</h3>

<p>50K slots × 768 dimensions × 30 layers = 1.2B additional parameters. We had to inject selectively (every 4th layer) to stay practical.</p>

<h3 id="3-training-dynamics">3. Training Dynamics</h3>

<p>Memory tables start at zero. They need higher learning rates (10x) to develop meaningful representations before the model learns to use them.</p>

<h3 id="4-evaluation-mismatch">4. Evaluation Mismatch</h3>

<p>Our pattern completion task wasn’t ideal for hash-based memory. Engram shines on exact-match retrieval, not generalization.</p>

<h2 id="combined-approach">Combined Approach</h2>

<p>The best results came from combining both methods:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Base Model (SmolLM-135M)
        ↓
EnhancedEngramModule
  - Long-term fact storage
  - O(1) lookup for known patterns
        ↓
LoRA Adapters
  - Pattern completion
  - Domain-specific behaviors
        ↓
Output
</code></pre></div></div>

<p>This gives you:</p>
<ul>
  <li><strong>Long-term memory</strong> from hash tables</li>
  <li><strong>Pattern consistency</strong> from behavioral training</li>
  <li><strong>Flexibility</strong> to disable either component</li>
</ul>

<h2 id="what-we-learned">What We Learned</h2>

<ol>
  <li>
    <p><strong>Emulation vs Implementation</strong>: LoRA fine-tuning approximates memory behavior; hash tables implement it. Both have their place.</p>
  </li>
  <li>
    <p><strong>Gating is Essential</strong>: The learned gate prevents hash collisions from degrading performance. Never use Engram without gating.</p>
  </li>
  <li>
    <p><strong>Match Task to Tool</strong>: Hash-based memory excels at exact lookups, not pattern generalization. Use it where applicable.</p>
  </li>
  <li>
    <p><strong>Selective Application</strong>: Don’t inject Engram everywhere. Target layers and use cases where it helps.</p>
  </li>
  <li>
    <p><strong>The Gate as a Safety Valve</strong>: When the gate learns to output near-zero for a task, that’s the model telling you Engram doesn’t help there. Listen to it.</p>
  </li>
</ol>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2601.07372">Engram Paper (arXiv:2601.07372)</a></li>
  <li><a href="https://github.com/softwarewrighter/engram-poc">engram-poc Repository</a> - Our implementation</li>
  <li><a href="https://github.com/weagan/Engram">weagan/Engram</a> - Reference implementation</li>
  <li><a href="https://www.youtube.com/watch?v=TZT_cWWv9Oc">Engram Revisited Video</a></li>
  <li><a href="https://www.youtube.com/playlist?list=PLKjvVAEaR4isTOri5dlPRIUK8Uy0jotX6">Engram Video Playlist</a></li>
  <li><a href="/2026/02/01/deepseek-papers-part1-mhc/">Part 1: mHC</a></li>
  <li><a href="/2026/02/02/deepseek-papers-part2-engram/">Part 2: Engram Introduction</a></li>
</ul>

<h2 id="series-recap">Series Recap</h2>

<table>
  <thead>
    <tr>
      <th>Part</th>
      <th>Topic</th>
      <th>Key Insight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>mHC</td>
      <td>Doubly-stochastic constraints bound signal amplification</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Engram Intro</td>
      <td>O(1) lookup beats recomputing through attention</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Engram Revisited</td>
      <td>Use Engram where applicable; gate to avoid worse results</td>
    </tr>
  </tbody>
</table>

<hr />

<p><em>Part 3 of 3 in the Deepseek Papers series. <a href="/series/#deepseek-papers">View all parts</a></em></p>

<p><em>Hash-based memory is powerful but specialized. The gate decides when to use it—and when not to.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-TZT_cWWv9Oc">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-TZT_cWWv9Oc"
      src="https://www.youtube.com/embed/TZT_cWWv9Oc?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-TZT_cWWv9Oc';
  const playerId = 'yt-player-TZT_cWWv9Oc';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="27"><p class="post-meta">February 10, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">491 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Cross-Validation (rotate held-out data), GPT (predict next token at scale), GQA (shared keys/values for efficiency), Context Window (how much the model sees), Self-Attention (each token attends to all others).</div><h3>
          <a class="post-link" href="/2026/02/10/five-ml-concepts-7/">
            Five ML Concepts - #7
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-7">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-seven.png" class="post-marker no-invert" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/YLgwkSiSOWw">Five ML Concepts #7</a><br /><a href="https://www.youtube.com/shorts/YLgwkSiSOWw"><img src="https://img.youtube.com/vi/YLgwkSiSOWw/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Cross-Validation</strong></td>
        <td><a href="https://www.jstor.org/stable/2965703">A Study of Cross-Validation and Bootstrap</a> (Kohavi 1995)</td>
      </tr>
      <tr>
        <td><strong>GPT</strong></td>
        <td><a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a> (Radford et al. 2019)</td>
      </tr>
      <tr>
        <td><strong>GQA</strong></td>
        <td><a href="https://arxiv.org/abs/2305.13245">GQA: Training Generalized Multi-Query Transformer Models</a> (Ainslie et al. 2023)</td>
      </tr>
      <tr>
        <td><strong>Context Window</strong></td>
        <td><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> (Vaswani et al. 2017)</td>
      </tr>
      <tr>
        <td><strong>Self-Attention</strong></td>
        <td><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> (Vaswani et al. 2017)</td>
      </tr>
    </tbody>
  </table>

</div>

<div style="display:none">

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-cross-validation">1. Cross-Validation</h3>

<p><strong>A technique that splits data into multiple folds to evaluate model performance on data it wasn’t trained on.</strong> By rotating which data is held out, it gives a more reliable estimate of generalization.</p>

<p>Essential for honest model evaluation.</p>

<blockquote>
  <p>Like practicing with different sets of flashcards to see if you actually learned the material.</p>
</blockquote>

<h3 id="2-gpt">2. GPT</h3>

<p><strong>Generative Pre-trained Transformer.</strong> A family of autoregressive language models trained to predict the next token in a sequence.</p>

<p>Many AI assistants and chatbots are built on this approach.</p>

<blockquote>
  <p>Like autocomplete, but scaled up and trained on vast text data.</p>
</blockquote>

<h3 id="3-gqa-grouped-query-attention">3. GQA (Grouped Query Attention)</h3>

<p><strong>An attention variant where multiple query heads share key and value projections.</strong> This reduces memory usage and can speed up inference compared to standard multi-head attention.</p>

<p>Widely adopted in efficient transformer architectures.</p>

<blockquote>
  <p>Like several students sharing one set of notes instead of copying everything separately.</p>
</blockquote>

<h3 id="4-context-window">4. Context Window</h3>

<p><strong>The maximum number of tokens a model can process in a single forward pass.</strong> Larger context windows allow longer inputs, but increase memory and compute costs.</p>

<p>A key constraint in language model design.</p>

<blockquote>
  <p>Like the size of a desk that limits how many papers you can spread out at once.</p>
</blockquote>

<h3 id="5-self-attention">5. Self-Attention</h3>

<p><strong>A mechanism where each token computes attention scores with other tokens in the same sequence.</strong> This lets the model weigh which parts of the input are most relevant to each position.</p>

<p>The core operation inside transformers.</p>

<blockquote>
  <p>Like everyone in a meeting deciding who to listen to based on the conversation.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Cross-Validation</strong></td>
      <td>Rotate held-out data for reliable evaluation</td>
    </tr>
    <tr>
      <td><strong>GPT</strong></td>
      <td>Predict next token, at scale</td>
    </tr>
    <tr>
      <td><strong>GQA</strong></td>
      <td>Shared keys/values for efficient attention</td>
    </tr>
    <tr>
      <td><strong>Context Window</strong></td>
      <td>How much the model sees at once</td>
    </tr>
    <tr>
      <td><strong>Self-Attention</strong></td>
      <td>Each token attends to all others</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 7 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/11/five-ml-concepts-8/">Next: #8 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-YLgwkSiSOWw">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-YLgwkSiSOWw"
      src="https://www.youtube.com/embed/YLgwkSiSOWw?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-YLgwkSiSOWw';
  const playerId = 'yt-player-YLgwkSiSOWw';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="28"><p class="post-meta">February 9, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">513 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Regularization (constraints to prevent overfitting), BERT (bidirectional masked language modeling), RoPE (position via rotation in attention), Prompting (craft inputs to steer outputs), Positional Encoding (tell model where tokens are).</div><h3>
          <a class="post-link" href="/2026/02/09/five-ml-concepts-6/">
            Five ML Concepts - #6
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-6">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-six.png" class="post-marker no-invert" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/ROje4xAMJKg">Five ML Concepts #6</a><br /><a href="https://www.youtube.com/shorts/ROje4xAMJKg"><img src="https://img.youtube.com/vi/ROje4xAMJKg/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Regularization</strong></td>
        <td><a href="https://jmlr.org/papers/v15/srivastava14a.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a> (Srivastava et al. 2014)</td>
      </tr>
      <tr>
        <td><strong>BERT</strong></td>
        <td><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers</a> (Devlin et al. 2018)</td>
      </tr>
      <tr>
        <td><strong>RoPE</strong></td>
        <td><a href="https://arxiv.org/abs/2104.09864">RoFormer: Enhanced Transformer with Rotary Position Embedding</a> (Su et al. 2021)</td>
      </tr>
      <tr>
        <td><strong>Prompting</strong></td>
        <td><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> (Brown et al. 2020)</td>
      </tr>
      <tr>
        <td><strong>Positional Encoding</strong></td>
        <td><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> (Vaswani et al. 2017)</td>
      </tr>
    </tbody>
  </table>

</div>

<div style="display:none">

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-regularization">1. Regularization</h3>

<p><strong>Techniques that reduce overfitting by adding constraints or penalties during training.</strong> Common examples include L2 weight decay, L1 sparsity, dropout, and early stopping.</p>

<p>The goal is better generalization, not just fitting the training set.</p>

<blockquote>
  <p>Like adding friction so a model can’t take the easiest overfit path.</p>
</blockquote>

<h3 id="2-bert">2. BERT</h3>

<p><strong>Bidirectional Encoder Representations from Transformers.</strong> A transformer encoder trained with masked language modeling: predicting hidden tokens using context from both sides.</p>

<p>It was a major step forward for many NLP tasks after its 2018 release.</p>

<blockquote>
  <p>Like filling in blanks by reading the whole sentence, not just the words before it.</p>
</blockquote>

<h3 id="3-rope-rotary-positional-embeddings">3. RoPE (Rotary Positional Embeddings)</h3>

<p><strong>A way to represent token position inside attention by rotating query and key vectors as a function of position.</strong> This gives attention information about relative order and distance.</p>

<p>It’s widely used in modern transformer models.</p>

<blockquote>
  <p>Like turning a dial differently for each position so the model can tell where tokens are.</p>
</blockquote>

<h3 id="4-prompting">4. Prompting</h3>

<p><strong>Crafting inputs to steer a model toward the output you want.</strong> Small changes in instructions, examples, or format can change behavior significantly.</p>

<p>A key skill for working effectively with language models.</p>

<blockquote>
  <p>Like asking a question in just the right way to get a useful answer.</p>
</blockquote>

<h3 id="5-positional-encoding">5. Positional Encoding</h3>

<p><strong>Transformers need a way to represent token order, because attention alone doesn’t include sequence position.</strong> Different methods do this, including learned embeddings and rotary approaches like RoPE.</p>

<p>Without it, “the cat sat on the mat” would be indistinguishable from “mat the on sat cat the.”</p>

<blockquote>
  <p>Like numbering the pages of a shuffled book so you can read them in order.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Regularization</strong></td>
      <td>Add constraints to prevent overfitting</td>
    </tr>
    <tr>
      <td><strong>BERT</strong></td>
      <td>Bidirectional masked language modeling</td>
    </tr>
    <tr>
      <td><strong>RoPE</strong></td>
      <td>Position info via rotation in attention</td>
    </tr>
    <tr>
      <td><strong>Prompting</strong></td>
      <td>Craft inputs to steer model outputs</td>
    </tr>
    <tr>
      <td><strong>Positional Encoding</strong></td>
      <td>Tell the model where tokens are in sequence</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 6 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/10/five-ml-concepts-7/">Next: #7 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-ROje4xAMJKg">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-ROje4xAMJKg"
      src="https://www.youtube.com/embed/ROje4xAMJKg?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-ROje4xAMJKg';
  const playerId = 'yt-player-ROje4xAMJKg';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="29"><p class="post-meta">February 8, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">515 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Perceptron (single linear unit ancestor), Pre-training (learn general patterns first), Speculative Decoding (draft fast, verify in parallel), In-Context Learning (adapt from prompt examples), Latent Space (internal representations where similar things cluster).</div><h3>
          <a class="post-link" href="/2026/02/08/five-ml-concepts-5/">
            Five ML Concepts - #5
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-5">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-five.png" class="post-marker" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/8rzKVzfp2PU">Five ML Concepts #5</a><br /><a href="https://www.youtube.com/shorts/8rzKVzfp2PU"><img src="https://img.youtube.com/vi/8rzKVzfp2PU/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Perceptron</strong></td>
        <td><a href="https://psycnet.apa.org/record/1959-09865-001">The Perceptron: A Probabilistic Model</a> (Rosenblatt 1958)</td>
      </tr>
      <tr>
        <td><strong>Pre-training</strong></td>
        <td><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers</a> (Devlin et al. 2018)</td>
      </tr>
      <tr>
        <td><strong>Speculative Decoding</strong></td>
        <td><a href="https://arxiv.org/abs/2211.17192">Fast Inference from Transformers via Speculative Decoding</a> (Leviathan et al. 2022)</td>
      </tr>
      <tr>
        <td><strong>ICL</strong></td>
        <td><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> (Brown et al. 2020)</td>
      </tr>
      <tr>
        <td><strong>Latent Space</strong></td>
        <td><a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a> (Kingma &amp; Welling 2013)</td>
      </tr>
    </tbody>
  </table>

</div>

<div style="display:none">

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-perceptron">1. Perceptron</h3>

<p><strong>The simplest neural network: a single linear unit with weights and a bias.</strong> It computes a weighted sum and applies a threshold or activation.</p>

<p>It inspired modern neural networks, even though today’s models are far more complex.</p>

<blockquote>
  <p>Like a single voter weighing inputs before deciding yes or no.</p>
</blockquote>

<h3 id="2-pre-training">2. Pre-training</h3>

<p><strong>Training a model on a large, general dataset before adapting it to a specific task.</strong> This gives the model broad patterns that later training can refine.</p>

<p>BERT, GPT, and most modern LLMs use this approach.</p>

<blockquote>
  <p>Like going to medical school before choosing a specialty.</p>
</blockquote>

<h3 id="3-speculative-decoding">3. Speculative Decoding</h3>

<p><strong>A technique where a small, fast model proposes tokens, and a larger model verifies or rejects them in parallel.</strong> This can speed up inference without changing final outputs.</p>

<p>A key optimization for production LLM deployments.</p>

<blockquote>
  <p>Like a junior writer drafting text for a senior editor to approve in batches.</p>
</blockquote>

<h3 id="4-in-context-learning-icl">4. In-Context Learning (ICL)</h3>

<p><strong>When a model adapts its behavior using examples in the prompt, without updating its weights.</strong> It allows flexible task behavior at inference time.</p>

<p>This emergent capability surprised researchers when GPT-3 demonstrated it.</p>

<blockquote>
  <p>Like solving a new puzzle after seeing a few worked examples.</p>
</blockquote>

<h3 id="5-latent-space">5. Latent Space</h3>

<p><strong>The internal representations a model learns as it processes data.</strong> In this space, similar inputs tend to be located near each other.</p>

<p>It’s not a literal place, but a useful way to think about how models organize information.</p>

<blockquote>
  <p>Like a map where cities are arranged by similarity instead of geography.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Perceptron</strong></td>
      <td>Single linear unit—the neural network ancestor</td>
    </tr>
    <tr>
      <td><strong>Pre-training</strong></td>
      <td>Learn general patterns before specializing</td>
    </tr>
    <tr>
      <td><strong>Speculative Decoding</strong></td>
      <td>Draft fast, verify in parallel</td>
    </tr>
    <tr>
      <td><strong>ICL</strong></td>
      <td>Adapt from prompt examples without training</td>
    </tr>
    <tr>
      <td><strong>Latent Space</strong></td>
      <td>Internal representations where similar things cluster</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 5 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/09/five-ml-concepts-6/">Next: #6 →</a>*</td>
    </tr>
  </tbody>
</table>

<h2 id="related-posts">Related Posts</h2>

<ul>
  <li><a href="/2026/02/22/icl-revisited-from-mystery-to-engineering/">In-Context Learning Revisited: From Mystery to Engineering</a> — A deeper exploration of how ICL evolved from emergent surprise to engineered capability.</li>
</ul>

<hr />

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-8rzKVzfp2PU">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-8rzKVzfp2PU"
      src="https://www.youtube.com/embed/8rzKVzfp2PU?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-8rzKVzfp2PU';
  const playerId = 'yt-player-8rzKVzfp2PU';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="30"><p class="post-meta">February 7, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">475 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Activation Functions (introduce nonlinearity), Transfer Learning (reuse knowledge across tasks), VLM (joint image-text understanding), Adam (adaptive learning rates), Superposition (many concepts in overlapping representations).</div><h3>
          <a class="post-link" href="/2026/02/07/five-ml-concepts-4/">
            Five ML Concepts - #4
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-4">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-four.png" class="post-marker no-invert" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/10fb0EjFND8">Five ML Concepts #4</a><br /><a href="https://www.youtube.com/shorts/10fb0EjFND8"><img src="https://img.youtube.com/vi/10fb0EjFND8/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Activation Functions</strong></td>
        <td><a href="https://www.deeplearningbook.org/">Deep Learning</a> (Goodfellow et al. 2016), Chapter 6</td>
      </tr>
      <tr>
        <td><strong>Transfer Learning</strong></td>
        <td><a href="https://ieeexplore.ieee.org/document/5288526">A Survey on Transfer Learning</a> (Pan &amp; Yang 2010)</td>
      </tr>
      <tr>
        <td><strong>VLM</strong></td>
        <td><a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models (CLIP)</a> (Radford et al. 2021)</td>
      </tr>
      <tr>
        <td><strong>Adam</strong></td>
        <td><a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a> (Kingma &amp; Ba 2014)</td>
      </tr>
      <tr>
        <td><strong>Superposition</strong></td>
        <td><a href="https://transformer-circuits.pub/2022/toy_model/index.html">Toy Models of Superposition</a> (Elhage et al. 2022)</td>
      </tr>
    </tbody>
  </table>

</div>

<div style="display:none">

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-activation-functions">1. Activation Functions</h3>

<p><strong>Functions like ReLU, sigmoid, and tanh that transform neuron outputs.</strong> They introduce nonlinearity, allowing networks to learn complex patterns beyond simple linear relationships.</p>

<p>Without them, stacking layers would just be matrix multiplication.</p>

<blockquote>
  <p>Like an on-off switch that can also dim the lights.</p>
</blockquote>

<h3 id="2-transfer-learning">2. Transfer Learning</h3>

<p><strong>Using knowledge a model learned on one task to improve performance on a related task.</strong> This often reduces training time and data requirements dramatically.</p>

<p>Pre-trained models can be fine-tuned for specific applications.</p>

<blockquote>
  <p>Like a chef who already knows French cooking learning Japanese cuisine faster.</p>
</blockquote>

<h3 id="3-vlm-vision-language-models">3. VLM (Vision-Language Models)</h3>

<p><strong>Models trained to work with both images and text.</strong> They learn shared representations that connect visual and language understanding.</p>

<p>CLIP, GPT-4V, and LLaVA are examples of this approach.</p>

<blockquote>
  <p>Like someone who can look at a photo and describe what’s happening.</p>
</blockquote>

<h3 id="4-adam">4. Adam</h3>

<p><strong>An optimizer that adapts learning rates for each parameter using information from past gradients.</strong> It combines ideas from momentum and adaptive learning-rate methods.</p>

<p>One of the most popular optimizers in deep learning.</p>

<blockquote>
  <p>Like a hiker who adjusts step size for each part of the trail, steep or flat.</p>
</blockquote>

<h3 id="5-superposition">5. Superposition</h3>

<p><strong>A way neural networks represent many concepts using overlapping directions in the same space.</strong> This allows models to pack more information into fewer neurons than expected.</p>

<p>It’s why interpretability is hard—features aren’t neatly separated.</p>

<blockquote>
  <p>Like discovering a painting has hidden layers that appear under the right light.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Activation Functions</strong></td>
      <td>Introduce nonlinearity to enable complex patterns</td>
    </tr>
    <tr>
      <td><strong>Transfer Learning</strong></td>
      <td>Reuse knowledge from one task for another</td>
    </tr>
    <tr>
      <td><strong>VLM</strong></td>
      <td>Joint understanding of images and text</td>
    </tr>
    <tr>
      <td><strong>Adam</strong></td>
      <td>Adaptive per-parameter learning rates</td>
    </tr>
    <tr>
      <td><strong>Superposition</strong></td>
      <td>Many concepts packed into overlapping representations</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 4 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/08/five-ml-concepts-5/">Next: #5 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-10fb0EjFND8">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-10fb0EjFND8"
      src="https://www.youtube.com/embed/10fb0EjFND8?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-10fb0EjFND8';
  const playerId = 'yt-player-10fb0EjFND8';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="31"><p class="post-meta">February 6, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">546 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Loss Function (how far off predictions are), Overfitting (memorizing vs learning), Fine-tuning (specializing pre-trained models), LoRA (efficient adaptation with small matrices), Tokenization (breaking text into digestible pieces).</div><h3>
          <a class="post-link" href="/2026/02/06/five-ml-concepts-3/">
            Five ML Concepts - #3
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-3">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-three.png" class="post-marker no-invert" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/U-_yZZdZurU">Five ML Concepts #3</a><br /><a href="https://www.youtube.com/shorts/U-_yZZdZurU"><img src="https://img.youtube.com/vi/U-_yZZdZurU/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Loss Function</strong></td>
        <td><a href="https://arxiv.org/abs/1701.00160">A Survey of Loss Functions for Deep Neural Networks</a> (Janocha &amp; Czarnecki 2017)</td>
      </tr>
      <tr>
        <td><strong>Overfitting</strong></td>
        <td><a href="https://jmlr.org/papers/v15/srivastava14a.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a> (Srivastava et al. 2014)</td>
      </tr>
      <tr>
        <td><strong>Fine-tuning</strong></td>
        <td><a href="https://arxiv.org/abs/1911.02685">A Survey on Transfer Learning</a> (Zhuang et al. 2020)</td>
      </tr>
      <tr>
        <td><strong>LoRA</strong></td>
        <td><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a> (Hu et al. 2021)</td>
      </tr>
      <tr>
        <td><strong>Tokenization</strong></td>
        <td><a href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units</a> (Sennrich et al. 2015)</td>
      </tr>
    </tbody>
  </table>

</div>

<div style="display:none">

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-loss-function">1. Loss Function</h3>

<p><strong>A formula that measures how far off the model’s predictions are from the correct answers.</strong> It quantifies the gap between what the model predicted and what it should have predicted.</p>

<p>Training a neural network means minimizing this function.</p>

<blockquote>
  <p>Like a scorecard that tells the model how badly it messed up.</p>
</blockquote>

<h3 id="2-overfitting">2. Overfitting</h3>

<p><strong>When a model learns the training data too well, including noise and outliers, and fails on new data.</strong> The model performs great on examples it has seen but poorly on anything new.</p>

<p>One of the most common pitfalls in machine learning.</p>

<blockquote>
  <p>Like memorizing the answers to a test instead of understanding the subject.</p>
</blockquote>

<h3 id="3-fine-tuning">3. Fine-tuning</h3>

<p><strong>Taking a pre-trained model and training it further on a specific task or dataset.</strong> Instead of training from scratch, you start from a model that already understands language or images, then specialize it.</p>

<p>This makes powerful models accessible without massive compute budgets.</p>

<blockquote>
  <p>Like teaching a chef who already knows cooking to specialize in sushi.</p>
</blockquote>

<h3 id="4-lora-low-rank-adaptation">4. LoRA (Low-Rank Adaptation)</h3>

<p><strong>An efficient fine-tuning method that trains a small number of added parameters instead of the full model.</strong> It inserts small trainable matrices into each layer while keeping the original weights frozen.</p>

<p>This dramatically reduces the memory and compute needed for fine-tuning.</p>

<blockquote>
  <p>Like adding sticky notes to a textbook instead of rewriting the whole thing.</p>
</blockquote>

<h3 id="5-tokenization">5. Tokenization</h3>

<p><strong>The process of breaking text into smaller units called tokens that a model can process.</strong> Most modern models use subword tokenization, splitting words into common pieces rather than individual characters or whole words.</p>

<p>It determines what the model actually “sees” and affects everything from vocabulary size to multilingual performance.</p>

<blockquote>
  <p>Like chopping sentences into bite-sized pieces a model can digest.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Loss Function</strong></td>
      <td>How far off the model’s predictions are</td>
    </tr>
    <tr>
      <td><strong>Overfitting</strong></td>
      <td>Memorizing the test instead of learning the subject</td>
    </tr>
    <tr>
      <td><strong>Fine-tuning</strong></td>
      <td>Specializing a pre-trained model for a new task</td>
    </tr>
    <tr>
      <td><strong>LoRA</strong></td>
      <td>Efficient fine-tuning with small added matrices</td>
    </tr>
    <tr>
      <td><strong>Tokenization</strong></td>
      <td>Breaking text into the pieces a model actually reads</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 3 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/07/five-ml-concepts-4/">Next: #4 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-U-_yZZdZurU">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-U-_yZZdZurU"
      src="https://www.youtube.com/embed/U-_yZZdZurU?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-U-_yZZdZurU';
  const playerId = 'yt-player-U-_yZZdZurU';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="32"><p class="post-meta">February 5, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">1803 words</span> &bull; <span class="post-read-time">10 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Unix invented pipes. Mainframes reinvented them for records, not bytes. This Throwback Thursday recreates CMS/TSO Pipelines in Rust with a visual debugger, demonstrating record-oriented dataflow from the 1996 Olympics web server era.</div><h3>
          <a class="post-link" href="/2026/02/05/tbt-pipelines-os390/">
            TBT (2/?): Pipelines on OS/390
          </a>
        </h3><nav class="toc" data-toc-id="tbt-pipelines-os390">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/pipes-bw.png" class="post-marker" alt="" /></p>

<p>Unix invented pipes. Mainframes reinvented them—for records, not bytes.</p>

<p>This is the second <strong>Throwback Thursday</strong> post—revisiting technologies that shaped how I think about programming. This time: CMS/TSO Pipelines, and a vibe coding project that brings them back to life in Rust for education, fun, and nostalgic reasons.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/sw-comp-history/pipelines-rs">pipelines-rs</a></td>
      </tr>
      <tr>
        <td><strong>Demo</strong></td>
        <td><a href="https://sw-comp-history.github.io/pipelines-rs/">Live Demo</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://youtu.be/872RLMBzC_8">Pipelines on OS/390 #TBT</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-1996-olympics-and-a-pair-of-mainframes">The 1996 Olympics and a Pair of Mainframes</h2>

<p>In 1996, IBM hosted the Olympics Web Server—one of the largest public web properties at the time. Many distributed IBM systems in different regions served dynamic web pages. The logs from all of them were funneled to a pair of <strong>IBM S/390 mainframes</strong> I was in charge of, running <strong>OS/390</strong> (formerly MVS).</p>

<p>When you’re processing millions of log records for statistics and forensics, you need tools that think in records, not lines. That’s where <strong>Pipelines for TSO/E</strong> came in.</p>

<p>Pipelines for TSO/E was the MVS/ESA port of <strong>CMS Pipelines</strong>, which ran on VM/ESA. Both let you chain stages together to filter, transform, and aggregate record-oriented data—record-oriented pipelines that evolved in parallel with Unix’s byte-stream pipes.</p>

<h2 id="two-traditions-of-piping">Two Traditions of Piping</h2>

<p>Unix pipes came first—Thompson and McIlroy at Bell Labs, 1969–1974. Byte streams, file descriptors, the <code class="language-plaintext highlighter-rouge">|</code> operator. Brutally simple. Explosively powerful. POSIX.1-1988 standardized <code class="language-plaintext highlighter-rouge">pipe(2)</code> and shell pipelines, though POSIX work began in the mid-1980s.</p>

<p>CMS Pipelines emerged on IBM mainframes in the mid-to-late 1980s. They weren’t a Unix clone—they were convergent evolution under different pressures. Where Unix piped bytes between small programs, CMS piped <strong>records</strong> through declarative stages. Pipelines for TSO/E followed in the late 1980s and early 1990s, porting CMS concepts to the MVS multi-user environment. Unlike CMS Pipelines (which ships with z/VM), the TSO/E port is typically installed separately on z/OS.</p>

<p>Neither tradition was “behind.” They were optimizing different dimensions:</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>Unix Pipes</th>
      <th>CMS/TSO Pipelines</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Era</strong></td>
      <td>1969–1974</td>
      <td>Mid-to-late 1980s</td>
    </tr>
    <tr>
      <td><strong>Data unit</strong></td>
      <td>Byte stream</td>
      <td>Records (fixed or variable length)</td>
    </tr>
    <tr>
      <td><strong>Stage input</strong></td>
      <td>stdin (bytes)</td>
      <td>Record buffer</td>
    </tr>
    <tr>
      <td><strong>Field access</strong></td>
      <td><code class="language-plaintext highlighter-rouge">awk</code>, <code class="language-plaintext highlighter-rouge">cut</code> (text parsing)</td>
      <td>Column positions (direct)</td>
    </tr>
    <tr>
      <td><strong>Execution</strong></td>
      <td>Typically a process per stage</td>
      <td>Stages in one address space</td>
    </tr>
    <tr>
      <td><strong>Topology</strong></td>
      <td>Linear by default; fan-out/fan-in via <code class="language-plaintext highlighter-rouge">tee</code>, FIFOs, or process substitution</td>
      <td>Multi-stream, fan-out/fan-in built in</td>
    </tr>
    <tr>
      <td><strong>Philosophy</strong></td>
      <td>Small tools, ad hoc composition</td>
      <td>Declarative data transformation</td>
    </tr>
  </tbody>
</table>

<p>Many datasets on mainframes are record-structured. Records can be fixed-length or variable-length. CMS and TSO/E Pipelines treat records as byte arrays—character-oriented stages assume EBCDIC text, while position/length stages are binary-safe. A fixed-length 80-byte record isn’t arbitrary text—columns 1-8 are the name, 9-18 are the department, 19-26 are the salary. You don’t parse. You just read the right columns.</p>

<p>Unix won culturally—cheap hardware, academic distribution, C portability. But IBM’s record-oriented pipelines were better at structured dataflow, and they anticipate or parallel patterns seen in ETL frameworks like Spark and Beam.</p>

<p>CMS Pipelines ships with z/VM and is still used; Pipelines for TSO/E exists for z/OS but isn’t universally installed. These are not historical curiosities—mainframes continue to process a significant share of high-value transactions, and pipelines remain an available tool for data transformation on those systems.</p>

<h2 id="what-a-pipeline-looks-like">What a Pipeline Looks Like</h2>

<p>CMS Pipelines uses a DSL with <code class="language-plaintext highlighter-rouge">PIPE</code> as the command, <code class="language-plaintext highlighter-rouge">|</code> to chain stages, and <code class="language-plaintext highlighter-rouge">?</code> as a command terminator (it suppresses the console from being used as implicit input):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PIPE CONSOLE
| FILTER 18,10 = "SALES"
| SELECT 0,8,0; 8,10,8
| CONSOLE
?
</code></pre></div></div>

<p>This reads input records, keeps only those where columns 18–27 equal “SALES”, extracts the name fields, and writes the result. No regex. No string splitting. Just column positions.</p>

<p><em>Note: pipelines-rs uses 0-based offsets (e.g., <code class="language-plaintext highlighter-rouge">SELECT 0,8,0</code>). Historical CMS Pipelines uses 1-based column positions.</em></p>

<p>Compare with the Unix equivalent:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat </span>input.txt | <span class="nb">awk</span> <span class="s1">'$3 == "SALES" {print $1, $2}'</span>
</code></pre></div></div>

<p>The Unix version looks simpler—until your fields contain spaces, or your records contain non-text bytes, or you need to chain 15 stages without spawning 15 processes.</p>

<h2 id="bringing-it-back-in-rust-vibe-coding">Bringing It Back in Rust (Vibe Coding)</h2>

<p><strong>pipelines-rs</strong> is a nostalgia-driven vibe coding project—my attempt to emulate Pipelines for TSO/E in Rust, not because it’s practical, but because these ideas deserve to be celebrated. It supports a subset of stages and features two execution models:</p>

<h3 id="the-two-executors">The Two Executors</h3>

<p><strong>Batched</strong> processes all records through one stage before moving to the next:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>All records → Stage 1 → All records → Stage 2 → All records → Stage 3
</code></pre></div></div>

<p>This emulates the correct output and is faster, but doesn’t demonstrate record-oriented dataflow well.</p>

<p><strong>Record-At-a-Time (RAT)</strong> sends each record through the entire pipeline before reading the next:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Record 1 → Stage 1 → Stage 2 → Stage 3 → Output
Record 2 → Stage 1 → Stage 2 → Stage 3 → Output
Record 3 → Stage 1 → Stage 2 → Stage 3 → Output
</code></pre></div></div>

<p>RAT is the implementation shown in the video. It’s a naive approach—more buffers, more copying—but it shows the dataflow concepts clearly and enables the visual debugger. Both run in linear time (records × stages) and produce identical output for all 23 test specifications.</p>

<p>A future version will aim for fewer buffers and fewer copy operations. Whether it’s faster than Batched remains to be seen.</p>

<h3 id="the-80-byte-record">The 80-Byte Record</h3>

<p>The Rust implementation supports fixed-length records only. The fundamental data type is the <code class="language-plaintext highlighter-rouge">Record</code>—exactly 80 bytes, matching historical punch card width. Variable-length input lines are accepted and padded to 80 bytes:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">const</span> <span class="n">RECORD_WIDTH</span><span class="p">:</span> <span class="nb">usize</span> <span class="o">=</span> <span class="mi">80</span><span class="p">;</span>

<span class="k">pub</span> <span class="k">struct</span> <span class="n">Record</span> <span class="p">{</span>
    <span class="n">data</span><span class="p">:</span> <span class="p">[</span><span class="nb">u8</span><span class="p">;</span> <span class="n">RECORD_WIDTH</span><span class="p">],</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Fields are accessed by column position and length. No parsing, no delimiters. The data is always right where you expect it.</p>

<h3 id="supported-stages">Supported Stages</h3>

<p>The current implementation supports 14 stages:</p>

<table>
  <thead>
    <tr>
      <th>Stage</th>
      <th>Purpose</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>FILTER</strong></td>
      <td>Keep/reject records by field value</td>
      <td><code class="language-plaintext highlighter-rouge">FILTER 18,10 = "SALES"</code></td>
    </tr>
    <tr>
      <td><strong>LOCATE</strong></td>
      <td>Keep records containing a pattern</td>
      <td><code class="language-plaintext highlighter-rouge">LOCATE "ERROR"</code></td>
    </tr>
    <tr>
      <td><strong>NLOCATE</strong></td>
      <td>Keep records NOT containing a pattern</td>
      <td><code class="language-plaintext highlighter-rouge">NLOCATE "DEBUG"</code></td>
    </tr>
    <tr>
      <td><strong>SELECT</strong></td>
      <td>Extract and reposition fields</td>
      <td><code class="language-plaintext highlighter-rouge">SELECT 0,8,0; 8,10,8</code></td>
    </tr>
    <tr>
      <td><strong>CHANGE</strong></td>
      <td>Text replacement</td>
      <td><code class="language-plaintext highlighter-rouge">CHANGE "SALES" "MKTG"</code></td>
    </tr>
    <tr>
      <td><strong>COUNT</strong></td>
      <td>Count records</td>
      <td><code class="language-plaintext highlighter-rouge">COUNT</code></td>
    </tr>
    <tr>
      <td><strong>TAKE</strong></td>
      <td>Keep first N records</td>
      <td><code class="language-plaintext highlighter-rouge">TAKE 5</code></td>
    </tr>
    <tr>
      <td><strong>SKIP</strong></td>
      <td>Skip first N records</td>
      <td><code class="language-plaintext highlighter-rouge">SKIP 2</code></td>
    </tr>
    <tr>
      <td><strong>DUPLICATE</strong></td>
      <td>Repeat each record N times</td>
      <td><code class="language-plaintext highlighter-rouge">DUPLICATE 3</code></td>
    </tr>
    <tr>
      <td><strong>LITERAL</strong></td>
      <td>Append a literal record</td>
      <td><code class="language-plaintext highlighter-rouge">LITERAL "--- END ---"</code></td>
    </tr>
    <tr>
      <td><strong>UPPER/LOWER</strong></td>
      <td>Case conversion</td>
      <td><code class="language-plaintext highlighter-rouge">UPPER</code></td>
    </tr>
    <tr>
      <td><strong>REVERSE</strong></td>
      <td>Reverse record text</td>
      <td><code class="language-plaintext highlighter-rouge">REVERSE</code></td>
    </tr>
    <tr>
      <td><strong>HOLE</strong></td>
      <td>Discard all input</td>
      <td><code class="language-plaintext highlighter-rouge">HOLE</code></td>
    </tr>
    <tr>
      <td><strong>CONSOLE</strong></td>
      <td>Driver stage: source or sink depending on position</td>
      <td><code class="language-plaintext highlighter-rouge">CONSOLE</code></td>
    </tr>
  </tbody>
</table>

<h2 id="the-cli">The CLI</h2>

<p>Both executors have identical CLIs:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Batch executor</span>
pipe-run specs/filter-sales.pipe specs/input-fixed-80.data <span class="nt">-v</span>

<span class="c"># Record-at-a-time executor</span>
pipe-run-rat specs/filter-sales.pipe specs/input-fixed-80.data <span class="nt">-v</span>
</code></pre></div></div>

<p>Given this input data:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SMITH   JOHN      SALES     00050000
JONES   MARY      ENGINEER  00075000
DOE     JANE      SALES     00060000
WILSON  ROBERT    MARKETING 00055000
CHEN    LISA      ENGINEER  00080000
GARCIA  CARLOS    SALES     00045000
TAYLOR  SUSAN     MARKETING 00065000
BROWN   MICHAEL   ENGINEER  00090000
</code></pre></div></div>

<p>And this pipeline:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PIPE CONSOLE
| FILTER 18,10 = "SALES"
| CONSOLE
?
</code></pre></div></div>

<p>The output is:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SMITH   JOHN      SALES     00050000
DOE     JANE      SALES     00060000
GARCIA  CARLOS    SALES     00045000
Records:  8 in -&gt; 3 out
</code></pre></div></div>

<p>Exactly what I’d have gotten on OS/390 in 1996, but with Web Server log data showing client IP address, OS, browser type/version, user cookies, timestamps, URLs, and more, instead of accounting data. 😊</p>

<h2 id="the-web-ui-for-two-pipelines-rs-implementations">The Web UI for Two pipelines-rs Implementations</h2>

<p>The web interface runs entirely in the browser via WebAssembly. It has three panels: input records with an 80-column ruler, the pipeline editor, and the output.</p>

<h3 id="tutorial-mode">Tutorial Mode</h3>

<p>The tutorial walks through each stage with examples, running pipelines automatically to show results. You can step through manually or let it auto-advance.</p>

<h3 id="the-visual-debugger">The Visual Debugger</h3>

<p>The debugger is the reason RAT exists. It lets you:</p>

<ul>
  <li><strong>Step</strong> through execution one pipe point at a time</li>
  <li><strong>Watch</strong> data at specific pipe points between stages</li>
  <li><strong>Set breakpoints</strong> to pause at specific stages</li>
  <li><strong>See stage state</strong> for stateful stages like COUNT</li>
</ul>

<p>You load a pipeline, click Run, then Step to watch each record flow through each stage. The debugger highlights which stages have been reached with a green border. For COUNT and other aggregation stages, you can watch the flush phase where accumulated state becomes output.</p>

<h2 id="whats-next">What’s Next</h2>

<p>The current RAT executor is intentionally naive—it uses a buffer at every pipe point and copies each record between them. A better implementation would minimize buffers and copy operations while preserving the record-at-a-time semantics.</p>

<p>Multi-pipe features are also planned—CMS Pipelines supported fan-out (one input, multiple output streams) and fan-in (multiple inputs merged), which enabled complex processing topologies beyond simple linear chains.</p>

<h2 id="how-pipelines-rs-differs-from-ibm-pipelines">How pipelines-rs Differs from IBM Pipelines</h2>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>IBM CMS/TSO/E Pipelines</th>
      <th>pipelines-rs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Indexing</strong></td>
      <td>1-based column positions</td>
      <td>0-based offsets</td>
    </tr>
    <tr>
      <td><strong>Record format</strong></td>
      <td>Fixed or variable length, EBCDIC</td>
      <td>Fixed 80-byte ASCII only (variable-length input padded)</td>
    </tr>
    <tr>
      <td><strong>Stages</strong></td>
      <td>Hundreds of built-in stages</td>
      <td>14 implemented so far</td>
    </tr>
    <tr>
      <td><strong>Topology</strong></td>
      <td>Multi-stream: fan-out, fan-in, multi-pipe</td>
      <td>Linear only (multi-pipe planned)</td>
    </tr>
    <tr>
      <td><strong>Environment</strong></td>
      <td>z/VM, z/OS mainframes</td>
      <td>CLI (native) and browser (WASM)</td>
    </tr>
    <tr>
      <td><strong>Character set</strong></td>
      <td>EBCDIC</td>
      <td>ASCII/UTF-8</td>
    </tr>
  </tbody>
</table>

<p>This is a teaching tool and nostalgia project, not a production replacement.</p>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Language</strong></td>
      <td>Rust (2024 edition)</td>
    </tr>
    <tr>
      <td><strong>Web UI</strong></td>
      <td>Yew framework, compiled to WASM</td>
    </tr>
    <tr>
      <td><strong>Stages</strong></td>
      <td>14 implemented</td>
    </tr>
    <tr>
      <td><strong>Test Specs</strong></td>
      <td>23 pipeline specifications</td>
    </tr>
    <tr>
      <td><strong>Tests</strong></td>
      <td>60+ (including batch/RAT equivalence)</td>
    </tr>
    <tr>
      <td><strong>License</strong></td>
      <td>MIT</td>
    </tr>
    <tr>
      <td><strong>Live Demo</strong></td>
      <td><a href="https://sw-comp-history.github.io/pipelines-rs/">sw-comp-history.github.io/pipelines-rs</a></td>
    </tr>
  </tbody>
</table>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://github.com/sw-comp-history/pipelines-rs">pipelines-rs Repository</a></li>
  <li><a href="https://sw-comp-history.github.io/pipelines-rs/">Live Demo</a></li>
  <li><a href="https://www.ibm.com/docs/en/zvm/7.3?topic=reference-cms-pipelines">CMS Pipelines Reference (IBM)</a></li>
</ul>

<h2 id="credits">Credits</h2>

<table>
  <thead>
    <tr>
      <th>Role</th>
      <th>Who</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Concept &amp; direction</strong></td>
      <td>Mike Wright</td>
    </tr>
    <tr>
      <td><strong>Content creation</strong></td>
      <td>Claude (Anthropic)</td>
    </tr>
    <tr>
      <td><strong>Editorial review</strong></td>
      <td>ChatGPT (OpenAI)</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 2 of the Throwback Thursday series. <a href="/series/#throwback-thursday">View all parts</a></td>
      <td>Next: <a href="/2026/02/13/tbt-vector-graphics-games/">TBT (3): Vector Graphics Games</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Mainframe ideas, modern tools. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-872RLMBzC_8">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-872RLMBzC_8"
      src="https://www.youtube.com/embed/872RLMBzC_8?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-872RLMBzC_8';
  const playerId = 'yt-player-872RLMBzC_8';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="33"><p class="post-meta">February 5, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">985 words</span> &bull; <span class="post-read-time">5 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Which small AI fits your laptop? Benchmarking Phi-2, Gemma-2B, and SmolLM on the 2-3B efficient frontier. Phi-2 achieves 61.7% MMLU with only 2.7B parameters, beating models 5x larger through synthetic textbook training. Data quality beats parameters.</div><h3>
          <a class="post-link" href="/2026/02/05/small-models-part6-efficient-frontier/">
            Small Models (6/6): Which Small AI Fits YOUR Laptop?
          </a>
        </h3><nav class="toc" data-toc-id="small-models-part6-efficient-frontier">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/kitchen-tools.png" class="post-marker" alt="" /></p>

<p>Maximum AI capability on minimum hardware. The 2-3B efficient frontier.</p>

<p>This is Part 6 (the finale) of the <strong>Small Models, Big Brains</strong> series. We’re benchmarking the best small models to help you choose the right one for your laptop.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/efficient-llm">efficient-llm</a></td>
      </tr>
      <tr>
        <td><strong>Phi-2</strong></td>
        <td><a href="https://huggingface.co/microsoft/phi-2">microsoft/phi-2</a></td>
      </tr>
      <tr>
        <td><strong>Gemma</strong></td>
        <td><a href="https://ai.google.dev/gemma">ai.google.dev/gemma</a></td>
      </tr>
      <tr>
        <td><strong>SmolLM</strong></td>
        <td><a href="https://huggingface.co/blog/smollm">HuggingFace Blog</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/DlxhTXcW-og">Which Small AI Fits YOUR Laptop?</a><br /><a href="https://www.youtube.com/shorts/DlxhTXcW-og"><img src="https://img.youtube.com/vi/DlxhTXcW-og/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-efficient-frontier">The Efficient Frontier</h2>

<p>In economics, the “efficient frontier” is the set of optimal portfolios offering the highest return for a given level of risk.</p>

<p>In AI, it’s the models offering the <strong>best capability for a given size</strong>.</p>

<h2 id="the-contenders">The Contenders</h2>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Params</th>
      <th>Source</th>
      <th>Key Strength</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Phi-2</strong></td>
      <td>2.7B</td>
      <td>Microsoft</td>
      <td>Reasoning, synthetic data</td>
    </tr>
    <tr>
      <td><strong>Gemma-2B</strong></td>
      <td>2B</td>
      <td>Google</td>
      <td>Distillation, multilingual</td>
    </tr>
    <tr>
      <td><strong>SmolLM2-1.7B</strong></td>
      <td>1.7B</td>
      <td>HuggingFace</td>
      <td>11T tokens, fast inference</td>
    </tr>
    <tr>
      <td><strong>SmolLM3-3B</strong></td>
      <td>3B</td>
      <td>HuggingFace</td>
      <td>Dual reasoning, 6 languages</td>
    </tr>
  </tbody>
</table>

<h2 id="benchmark-results">Benchmark Results</h2>

<p>Actual measurements on Apple Silicon (M-series) from <a href="https://github.com/softwarewrighter/efficient-llm">efficient-llm</a>:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>MMLU</th>
      <th>GSM8K</th>
      <th>HumanEval</th>
      <th>Speed (CPU)</th>
      <th>Memory</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Phi-2</strong></td>
      <td>61.7%</td>
      <td>57.0%</td>
      <td>50.0%</td>
      <td>7.1 tok/s</td>
      <td>5.2GB</td>
    </tr>
    <tr>
      <td><strong>Gemma-2B</strong></td>
      <td>38.9%</td>
      <td>18.0%</td>
      <td>90.0%</td>
      <td>8.5 tok/s</td>
      <td>4.7GB</td>
    </tr>
    <tr>
      <td><strong>SmolLM2</strong></td>
      <td>55.6%</td>
      <td>*</td>
      <td>*</td>
      <td>3.7 tok/s</td>
      <td>3.2GB</td>
    </tr>
  </tbody>
</table>

<p>*SmolLM2 GSM8K/HumanEval scores reflect prompt format incompatibility, not capability.</p>

<h2 id="the-key-insight-data-quality-beats-parameters">The Key Insight: Data Quality Beats Parameters</h2>

<p><strong>Phi-2 achieves 61.7% MMLU with only 2.7B parameters.</strong></p>

<p>For comparison:</p>
<ul>
  <li>Llama-2-7B: ~46% MMLU</li>
  <li>Llama-2-13B: ~55% MMLU</li>
</ul>

<p>Phi-2 beats models 5x its size. The secret? <strong>Synthetic textbook training.</strong></p>

<p>Microsoft generated high-quality educational content specifically designed to teach reasoning. Quality data &gt; quantity data &gt; model size.</p>

<h2 id="model-profiles">Model Profiles</h2>

<h3 id="phi-2-the-reasoning-champion">Phi-2: The Reasoning Champion</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Strengths: Math, logic, code understanding
Weakness:  Less conversational
Best for:  Technical tasks, chain-of-thought
</code></pre></div></div>

<p>Phi-2 was trained on “textbook quality” synthetic data. It thinks like a textbook explains.</p>

<h3 id="gemma-2b-the-distillation-expert">Gemma-2B: The Distillation Expert</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Strengths: Multilingual, edge deployment
Weakness:  Lower benchmark scores
Best for:  Production apps, Google ecosystem
</code></pre></div></div>

<p>Google distilled knowledge from larger models into this compact package. Great tooling and documentation.</p>

<h3 id="smollm2-17b-the-speed-demon">SmolLM2-1.7B: The Speed Demon</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Strengths: Fastest inference, smallest footprint
Weakness:  Prompt format sensitivity
Best for:  Memory-constrained environments
</code></pre></div></div>

<p>HuggingFace trained on 11T tokens—massive overtraining like TinyLlama but at a slightly larger scale.</p>

<h3 id="smollm3-3b-the-balanced-choice">SmolLM3-3B: The Balanced Choice</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Strengths: Dual reasoning modes, 6 languages
Weakness:  Newest, less battle-tested
Best for:  General-purpose small model needs
</code></pre></div></div>

<p>The latest from HuggingFace, designed to be the go-to small model.</p>

<h2 id="decision-framework">Decision Framework</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>├── Need best reasoning?           → Phi-2
├── Need instruction following?    → SmolLM2 or SmolLM3
├── Need multilingual?             → Gemma-2B or SmolLM3
├── Memory constrained (&lt;4GB)?     → SmolLM2 + INT4
├── Need Google ecosystem?         → Gemma-2B
├── General purpose?               → SmolLM3
└── Maximum quality per byte?      → Phi-2
</code></pre></div></div>

<h2 id="running-the-benchmarks">Running the Benchmarks</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/softwarewrighter/efficient-llm
<span class="nb">cd </span>efficient-llm

<span class="c"># Setup</span>
uv venv <span class="o">&amp;&amp;</span> <span class="nb">source</span> .venv/bin/activate
uv pip <span class="nb">install </span>torch transformers accelerate bitsandbytes datasets tqdm

<span class="c"># HuggingFace login (required for Gemma)</span>
huggingface-cli login

<span class="c"># Download and benchmark</span>
python download_models.py
python benchmark_quality.py
python benchmark_speed.py
python benchmark_memory.py

<span class="c"># Interactive demos</span>
python demo_reasoning.py
python demo_code.py
python demo_chat.py
</code></pre></div></div>

<h2 id="hardware-requirements">Hardware Requirements</h2>

<table>
  <thead>
    <tr>
      <th>Setup</th>
      <th>Models You Can Run</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>4GB RAM</td>
      <td>SmolLM2 (INT4)</td>
    </tr>
    <tr>
      <td>8GB RAM</td>
      <td>All models (INT4)</td>
    </tr>
    <tr>
      <td>16GB RAM</td>
      <td>All models (FP16)</td>
    </tr>
    <tr>
      <td>Apple Silicon</td>
      <td>All models (MPS)</td>
    </tr>
  </tbody>
</table>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Primary Language</strong></td>
      <td>Python</td>
    </tr>
    <tr>
      <td><strong>Source Files</strong></td>
      <td>7 <code class="language-plaintext highlighter-rouge">.py</code> files</td>
    </tr>
    <tr>
      <td><strong>Estimated Size</strong></td>
      <td>~1.4 KLOC</td>
    </tr>
    <tr>
      <td><strong>Framework</strong></td>
      <td>Transformers, PyTorch</td>
    </tr>
    <tr>
      <td><strong>Build System</strong></td>
      <td>uv / pip</td>
    </tr>
    <tr>
      <td><strong>Key Features</strong></td>
      <td>MMLU/GSM8K/HumanEval benchmarks, demos</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to benchmark 2-3B models, compare quality vs speed tradeoffs, or run interactive comparisons between Phi-2, Gemma, and SmolLM.</p>

<p><strong>Complexity:</strong> Low. Similar structure to billion-llm. Standalone Python scripts for each benchmark and demo. Requires HuggingFace authentication for Gemma access.</p>

<h2 id="series-recap">Series Recap</h2>

<p>Over six parts, we’ve explored the cutting edge of small model research:</p>

<table>
  <thead>
    <tr>
      <th>Part</th>
      <th>Model/Topic</th>
      <th>Key Insight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>TRM (&lt;1K params)</td>
      <td>Iteration beats scale</td>
    </tr>
    <tr>
      <td>2</td>
      <td>MobileLLM (350M)</td>
      <td>Offline AI is practical</td>
    </tr>
    <tr>
      <td>3</td>
      <td>HRM (27M)</td>
      <td>Hierarchy enables reasoning</td>
    </tr>
    <tr>
      <td>4</td>
      <td>BDH</td>
      <td>Sparsity enables interpretability</td>
    </tr>
    <tr>
      <td>5</td>
      <td>1B models</td>
      <td>The efficiency sweet spot</td>
    </tr>
    <tr>
      <td>6</td>
      <td>2-3B models</td>
      <td>Data quality beats parameters</td>
    </tr>
  </tbody>
</table>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>Data quality beats parameter count.</strong> Phi-2 proves careful curation outperforms brute scaling.</p>
  </li>
  <li>
    <p><strong>The 2-3B range is remarkably capable.</strong> These models handle real tasks, not just demos.</p>
  </li>
  <li>
    <p><strong>Each model has its niche.</strong> Match the model to your use case.</p>
  </li>
  <li>
    <p><strong>Quantization makes everything accessible.</strong> INT4 lets you run 3B models on 4GB RAM.</p>
  </li>
  <li>
    <p><strong>The frontier keeps moving.</strong> SmolLM3 is weeks old. Better models are coming.</p>
  </li>
</ol>

<h2 id="what-weve-learned">What We’ve Learned</h2>

<p>Small models aren’t a compromise—they’re a different optimization target. When you can’t throw compute at a problem, you’re forced to be clever:</p>

<ul>
  <li>Recursive reasoning (TRM)</li>
  <li>Mobile-optimized architectures (MobileLLM)</li>
  <li>Hierarchical decomposition (HRM)</li>
  <li>Sparse interpretable activations (BDH)</li>
  <li>Overtraining on quality data (TinyLlama, Phi-2)</li>
</ul>

<p>These techniques will eventually feed back into large models too. Small model research isn’t a dead end—it’s the frontier.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://github.com/softwarewrighter/efficient-llm">efficient-llm Repository</a></li>
  <li><a href="https://huggingface.co/microsoft/phi-2">Phi-2 on HuggingFace</a></li>
  <li><a href="https://ai.google.dev/gemma">Gemma</a></li>
  <li><a href="https://huggingface.co/blog/smollm">SmolLM</a></li>
  <li><a href="https://youtube.com/shorts/VIDEO_ID">Video: Which Small AI Fits YOUR Laptop?</a></li>
</ul>

<hr />

<p><em>Part 6 of 6 in the Small Models, Big Brains series. Thanks for following along!</em></p>

<p><em>Have questions? Find me on <a href="https://www.youtube.com/@SoftwareWrighter">YouTube @SoftwareWrighter</a> or <a href="https://discord.gg/softwarewrighter">Discord</a>.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-DlxhTXcW-og">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-DlxhTXcW-og"
      src="https://www.youtube.com/embed/DlxhTXcW-og?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-DlxhTXcW-og';
  const playerId = 'yt-player-DlxhTXcW-og';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="34"><p class="post-meta">February 5, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">468 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Gradient Descent (walk downhill to minimize error), Attention (focus on what matters), DPO (align from preference pairs), Learning Rate (step size tradeoff), Temperature (dial between predictable and creative).</div><h3>
          <a class="post-link" href="/2026/02/05/five-ml-concepts-2/">
            Five ML Concepts - #2
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-2">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-two.png" class="post-marker no-invert" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/-Y4T0_vETB4">Five ML Concepts #2</a><br /><a href="https://www.youtube.com/shorts/-Y4T0_vETB4"><img src="https://img.youtube.com/vi/-Y4T0_vETB4/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Gradient Descent</strong></td>
        <td><a href="https://arxiv.org/abs/1609.04747">An overview of gradient descent optimization algorithms</a> (Ruder 2016)</td>
      </tr>
      <tr>
        <td><strong>Attention</strong></td>
        <td><a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a> (Bahdanau et al. 2014)</td>
      </tr>
      <tr>
        <td><strong>DPO</strong></td>
        <td><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization</a> (Rafailov et al. 2023)</td>
      </tr>
      <tr>
        <td><strong>Learning Rate</strong></td>
        <td><a href="https://arxiv.org/abs/1506.01186">Cyclical Learning Rates</a> (Smith 2015)</td>
      </tr>
      <tr>
        <td><strong>Temperature</strong></td>
        <td><a href="https://arxiv.org/abs/1409.1259">On the Properties of Neural Machine Translation</a> (Cho et al. 2014)</td>
      </tr>
    </tbody>
  </table>

</div>

<div style="display:none">

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-gradient-descent">1. Gradient Descent</h3>

<p><strong>A general optimization method used across machine learning.</strong> It improves a model by taking small steps in the direction that reduces error the most.</p>

<p>Many learning algorithms rely on it, especially neural networks.</p>

<blockquote>
  <p>Like walking downhill in fog, adjusting each step based on the slope beneath your feet.</p>
</blockquote>

<h3 id="2-attention">2. Attention</h3>

<p><strong>A mechanism that lets models weigh different parts of the input by importance.</strong> Instead of treating everything equally, attention highlights what matters most.</p>

<p>This was key to breakthroughs in translation and language models.</p>

<blockquote>
  <p>Like reading a sentence and focusing more on the important words.</p>
</blockquote>

<h3 id="3-dpo-direct-preference-optimization">3. DPO (Direct Preference Optimization)</h3>

<p><strong>A method for aligning language models with human preferences.</strong> Unlike RLHF, it trains directly on preference comparisons and avoids an explicit reward model.</p>

<p>This simplifies training while achieving comparable alignment.</p>

<blockquote>
  <p>Like learning preferences by observing choices, not by designing a scoring system.</p>
</blockquote>

<h3 id="4-learning-rate">4. Learning Rate</h3>

<p><strong>Controls how large each update step is during training.</strong> Too large and learning becomes unstable. Too small and training is slow or gets stuck.</p>

<p>One of the most important hyperparameters to tune.</p>

<blockquote>
  <p>Like choosing how fast to walk downhill without losing balance.</p>
</blockquote>

<h3 id="5-temperature">5. Temperature</h3>

<p><strong>A parameter that controls randomness during text generation.</strong> Low temperature favors predictable, high-probability outputs. Higher temperature increases variety and surprise.</p>

<p>A tradeoff between consistency and creativity.</p>

<blockquote>
  <p>Like adjusting a dial from cautious to adventurous.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Gradient Descent</strong></td>
      <td>Walk downhill to minimize error</td>
    </tr>
    <tr>
      <td><strong>Attention</strong></td>
      <td>Focus on what matters in the input</td>
    </tr>
    <tr>
      <td><strong>DPO</strong></td>
      <td>Align models from preference pairs directly</td>
    </tr>
    <tr>
      <td><strong>Learning Rate</strong></td>
      <td>Step size that balances speed and stability</td>
    </tr>
    <tr>
      <td><strong>Temperature</strong></td>
      <td>Dial between predictable and creative</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 2 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/06/five-ml-concepts-3/">Next: #3 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container--Y4T0_vETB4">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player--Y4T0_vETB4"
      src="https://www.youtube.com/embed/-Y4T0_vETB4?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container--Y4T0_vETB4';
  const playerId = 'yt-player--Y4T0_vETB4';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="35"><p class="post-meta">February 4, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">865 words</span> &bull; <span class="post-read-time">5 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">One billion parameters: the sweet spot for AI. Big enough to reason, small enough to run anywhere. Comparing TinyLlama, Llama-3.2-1B, StableLM, and Pythia with LoRA fine-tuning in minutes and speculative decoding for 2-3x speedups.</div><h3>
          <a class="post-link" href="/2026/02/04/small-models-part5-billion-llm/">
            Small Models (5/6): Max AI Per Watt
          </a>
        </h3><nav class="toc" data-toc-id="small-models-part5-billion-llm">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/brain-puzzle.png" class="post-marker" alt="" /></p>

<p>One billion parameters. The sweet spot for AI.</p>

<p>Big enough to reason. Small enough to run anywhere. Maximum capability per watt.</p>

<p>This is Part 5 of the <strong>Small Models, Big Brains</strong> series, comparing four models at the 1B parameter point.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/billion-llm">billion-llm</a></td>
      </tr>
      <tr>
        <td><strong>TinyLlama</strong></td>
        <td><a href="https://github.com/jzhang38/TinyLlama">jzhang38/TinyLlama</a></td>
      </tr>
      <tr>
        <td><strong>Llama 3.2</strong></td>
        <td><a href="https://ai.meta.com/llama/">ai.meta.com/llama</a></td>
      </tr>
      <tr>
        <td><strong>Pythia</strong></td>
        <td><a href="https://github.com/EleutherAI/pythia">EleutherAI/pythia</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/B4uKx-DL1HY">Max AI Per Watt</a><br /><a href="https://www.youtube.com/shorts/B4uKx-DL1HY"><img src="https://img.youtube.com/vi/B4uKx-DL1HY/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="why-one-billion">Why One Billion?</h2>

<table>
  <thead>
    <tr>
      <th>Range</th>
      <th>Reality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Below 1B</td>
      <td>Models struggle with complex reasoning</td>
    </tr>
    <tr>
      <td>Above 1B</td>
      <td>Hardware requirements increase significantly</td>
    </tr>
    <tr>
      <td><strong>At 1B</strong></td>
      <td><strong>Maximum capability per watt</strong></td>
    </tr>
  </tbody>
</table>

<p>1B parameters is where you get:</p>
<ul>
  <li>Real language understanding</li>
  <li>Ability to follow instructions</li>
  <li>Fine-tuning in minutes on a laptop</li>
  <li>Deployment anywhere (phone, Raspberry Pi, browser)</li>
</ul>

<h2 id="the-contenders">The Contenders</h2>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Params</th>
      <th>Key Strength</th>
      <th>Training Data</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>TinyLlama</strong></td>
      <td>1.1B</td>
      <td>Overtrained on 3T tokens</td>
      <td>Community</td>
    </tr>
    <tr>
      <td><strong>Llama-3.2-1B</strong></td>
      <td>1B</td>
      <td>Official Meta ecosystem</td>
      <td>Meta</td>
    </tr>
    <tr>
      <td><strong>StableLM-1.6B</strong></td>
      <td>1.6B</td>
      <td>Multilingual, 2T tokens</td>
      <td>Stability AI</td>
    </tr>
    <tr>
      <td><strong>Pythia-1B</strong></td>
      <td>1.08B</td>
      <td>154 research checkpoints</td>
      <td>EleutherAI</td>
    </tr>
  </tbody>
</table>

<h2 id="tinyllama-the-overtraining-champion">TinyLlama: The Overtraining Champion</h2>

<p>TinyLlama breaks the rules. The Chinchilla scaling laws suggest training tokens should scale with parameters. TinyLlama uses <strong>100x more data</strong> than optimal.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Chinchilla-optimal for 1B: ~30B tokens
TinyLlama actual:          3T tokens (3,000B)
</code></pre></div></div>

<p>The result? A tiny model that punches well above its weight.</p>

<h2 id="benchmarks">Benchmarks</h2>

<p>From the <a href="https://github.com/softwarewrighter/billion-llm">billion-llm</a> repository:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>MMLU</th>
      <th>HumanEval</th>
      <th>Speed</th>
      <th>Memory</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>TinyLlama</td>
      <td>25.3%</td>
      <td>12.2%</td>
      <td>Fast</td>
      <td>2.2GB</td>
    </tr>
    <tr>
      <td>Llama-3.2-1B</td>
      <td>32.1%</td>
      <td>18.5%</td>
      <td>Fast</td>
      <td>2.4GB</td>
    </tr>
    <tr>
      <td>StableLM-1.6B</td>
      <td>30.8%</td>
      <td>15.1%</td>
      <td>Medium</td>
      <td>3.2GB</td>
    </tr>
    <tr>
      <td>Pythia-1B</td>
      <td>26.4%</td>
      <td>10.3%</td>
      <td>Fast</td>
      <td>2.2GB</td>
    </tr>
  </tbody>
</table>

<p>Llama-3.2-1B leads on quality. TinyLlama offers the best value when you factor in the open training recipe.</p>

<h2 id="lora-fine-tuning-in-minutes">LoRA Fine-Tuning in Minutes</h2>

<p>All these models can be fine-tuned on a laptop using <a href="https://arxiv.org/abs/2106.09685">LoRA</a>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>billion-llm
python finetune_demo.py <span class="nt">--model</span> tinyllama <span class="nt">--epochs</span> 3
</code></pre></div></div>

<p>LoRA adds small trainable adapters without modifying base weights:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Base Model (frozen): 1.1B parameters
LoRA Adapters:       ~4M parameters (0.4%)
Training time:       5-10 minutes on M1 Mac
</code></pre></div></div>

<h2 id="speculative-decoding-2-3x-speedup">Speculative Decoding: 2-3x Speedup</h2>

<p>Use a fast 1B model to draft tokens, verify with a slower 7B model:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Draft (1B):   "The quick brown fox" → [jumps, over, the, lazy]
Verify (7B):  Accept [jumps, over, the] → Reject [lazy] → Generate [sleepy]
</code></pre></div></div>

<p>The 1B model generates candidates quickly. The 7B model only needs to verify, not generate from scratch.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python speculative_demo.py
</code></pre></div></div>

<p>Results: <strong>2-3x speedup</strong> on autoregressive generation.</p>

<h2 id="hardware-requirements">Hardware Requirements</h2>

<table>
  <thead>
    <tr>
      <th>Setup</th>
      <th>What You Can Run</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CPU only</td>
      <td>All models (slower, INT4 quantized)</td>
    </tr>
    <tr>
      <td>4GB VRAM</td>
      <td>All models (INT4 quantized)</td>
    </tr>
    <tr>
      <td>8GB VRAM</td>
      <td>All models (FP16)</td>
    </tr>
    <tr>
      <td>Apple Silicon</td>
      <td>All models (MPS acceleration)</td>
    </tr>
  </tbody>
</table>

<h2 id="quick-start">Quick Start</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/softwarewrighter/billion-llm
<span class="nb">cd </span>billion-llm

<span class="c"># Setup</span>
uv venv <span class="o">&amp;&amp;</span> <span class="nb">source</span> .venv/bin/activate
uv pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt

<span class="c"># Download models</span>
python download_models.py

<span class="c"># Run benchmarks</span>
python benchmark.py

<span class="c"># Interactive comparison</span>
python demo_chat.py <span class="nt">--compare</span> tinyllama llama3.2-1b
</code></pre></div></div>

<h2 id="which-model-should-you-choose">Which Model Should You Choose?</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>├── Need Meta ecosystem compatibility? → Llama-3.2-1B
├── Need multilingual support?         → StableLM-1.6B
├── Need research reproducibility?     → Pythia-1B (154 checkpoints)
├── Need maximum performance/size?     → TinyLlama
└── Just getting started?              → Any of them work!
</code></pre></div></div>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Primary Language</strong></td>
      <td>Python</td>
    </tr>
    <tr>
      <td><strong>Source Files</strong></td>
      <td>8 <code class="language-plaintext highlighter-rouge">.py</code> files</td>
    </tr>
    <tr>
      <td><strong>Estimated Size</strong></td>
      <td>~1.4 KLOC</td>
    </tr>
    <tr>
      <td><strong>Framework</strong></td>
      <td>Transformers, PyTorch</td>
    </tr>
    <tr>
      <td><strong>Build System</strong></td>
      <td>uv / pip</td>
    </tr>
    <tr>
      <td><strong>Key Features</strong></td>
      <td>Benchmarking, LoRA fine-tuning, speculative decoding</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to benchmark small LLMs, learn LoRA fine-tuning, experiment with speculative decoding, or compare models head-to-head.</p>

<p><strong>Complexity:</strong> Low. Clean Python scripts with HuggingFace Transformers. Each script is standalone—run benchmarks, chat demos, or fine-tuning independently. Well-documented with shell scripts for common tasks.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>1B is the efficiency sweet spot.</strong> Below this, capability drops. Above, hardware costs rise.</p>
  </li>
  <li>
    <p><strong>Overtraining works.</strong> TinyLlama proves you can compensate for size with data.</p>
  </li>
  <li>
    <p><strong>LoRA makes fine-tuning accessible.</strong> Customize models on consumer hardware.</p>
  </li>
  <li>
    <p><strong>Speculative decoding is free speed.</strong> Use small models to accelerate large ones.</p>
  </li>
  <li>
    <p><strong>All roads lead to open weights.</strong> Every model here is fully open.</p>
  </li>
</ol>

<h2 id="whats-next">What’s Next</h2>

<p>Part 6 explores the <strong>2-3B efficient frontier</strong>—Phi-2, Gemma, and SmolLM pushing the limits of small model capability.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://github.com/softwarewrighter/billion-llm">billion-llm Repository</a></li>
  <li><a href="https://github.com/jzhang38/TinyLlama">TinyLlama</a></li>
  <li><a href="https://ai.meta.com/llama/">Llama 3.2</a></li>
  <li><a href="https://github.com/EleutherAI/pythia">Pythia</a></li>
  <li><a href="https://arxiv.org/abs/2106.09685">LoRA Paper</a></li>
  <li><a href="https://arxiv.org/abs/2211.17192">Speculative Decoding Paper</a></li>
  <li><a href="https://youtube.com/shorts/VIDEO_ID">Video: Max AI Per Watt</a></li>
</ul>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 5 of 6 in the Small Models, Big Brains series. <a href="/series/#small-models-big-brains">View all parts</a></td>
      <td><a href="/2026/02/05/small-models-part6-efficient-frontier/">Next: Part 6 →</a>*</td>
    </tr>
  </tbody>
</table>

          </div>





<div class="youtube-embed-container" id="yt-container-B4uKx-DL1HY">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-B4uKx-DL1HY"
      src="https://www.youtube.com/embed/B4uKx-DL1HY?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-B4uKx-DL1HY';
  const playerId = 'yt-player-B4uKx-DL1HY';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="36"><p class="post-meta">February 4, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">433 words</span> &bull; <span class="post-read-time">3 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Five ML concepts in under 30 seconds each: Backpropagation (learning by flowing error backward), Transformers (attention over all tokens), Mamba (linear-time sequence modeling), Hallucination (confident nonsense), and Embeddings (meaning as coordinates).</div><h3>
          <a class="post-link" href="/2026/02/04/five-ml-concepts-1/">
            Five ML Concepts - #1
          </a>
        </h3><nav class="toc" data-toc-id="five-ml-concepts-1">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/block-one.png" class="post-marker no-invert" alt="" /></p>

<p>5 machine learning concepts. Under 30 seconds each.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td>Links in References section</td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/Zu3zreN8c0Q">Five ML Concepts #1</a><br /><a href="https://www.youtube.com/shorts/Zu3zreN8c0Q"><img src="https://img.youtube.com/vi/Zu3zreN8c0Q/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Concept</th>
        <th>Reference</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Backprop</strong></td>
        <td><a href="https://www.nature.com/articles/323533a0">Learning representations by back-propagating errors</a> (Rumelhart, Hinton, Williams 1986)</td>
      </tr>
      <tr>
        <td><strong>Transformer</strong></td>
        <td><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> (Vaswani et al. 2017)</td>
      </tr>
      <tr>
        <td><strong>Mamba</strong></td>
        <td><a href="https://arxiv.org/abs/2312.00752">Mamba: Linear-Time Sequence Modeling</a> (Gu &amp; Dao 2023)</td>
      </tr>
      <tr>
        <td><strong>Hallucination</strong></td>
        <td><a href="https://arxiv.org/abs/2202.03629">Survey of Hallucination in NLG</a> (Ji et al. 2023)</td>
      </tr>
      <tr>
        <td><strong>Embedding</strong></td>
        <td><a href="https://arxiv.org/abs/1301.3781">Word2Vec</a> (Mikolov et al. 2013)</td>
      </tr>
    </tbody>
  </table>

</div>

<div style="display:none">

</div>

<h2 id="todays-five">Today’s Five</h2>

<h3 id="1-backpropagation">1. Backpropagation</h3>

<p><strong>Back propagation of errors.</strong> It’s how neural networks learn—flowing error backward through the network to adjust each weight.</p>

<p>Without it, modern deep learning wouldn’t be practical.</p>

<blockquote>
  <p>Think of it like retracing your steps to see which earlier choices caused the mistake.</p>
</blockquote>

<h3 id="2-transformer">2. Transformer</h3>

<p><strong>The architecture behind GPT, Claude, and most modern language models.</strong> Instead of processing words one at a time, transformers use attention to weigh relationships between all tokens.</p>

<p>This enables parallel training and rich context awareness.</p>

<blockquote>
  <p>Like understanding a sentence by seeing how every word relates to every other.</p>
</blockquote>

<h3 id="3-mamba-state-space-models">3. Mamba (State Space Models)</h3>

<p><strong>A newer alternative to transformers</strong> that processes sequences in linear time instead of quadratic.</p>

<p>This allows scaling to very long documents with much lower memory use.</p>

<blockquote>
  <p>Like a smart conveyor belt that carries forward only what matters.</p>
</blockquote>

<h3 id="4-hallucination">4. Hallucination</h3>

<p><strong>When a model generates confident-sounding nonsense.</strong> It happens because language models predict plausible next words, not true facts.</p>

<p>They optimize for likelihood, not correctness.</p>

<blockquote>
  <p>Like a student who writes confidently without verifying sources.</p>
</blockquote>

<h3 id="5-embedding">5. Embedding</h3>

<p><strong>Turning words, images, or concepts into vectors of numbers.</strong> Similar meanings end up close together in this space.</p>

<p>This lets math capture semantic relationships.</p>

<blockquote>
  <p>Think of it as a coordinate system for meaning.</p>
</blockquote>

<h2 id="quick-reference">Quick Reference</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>One-liner</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Backprop</strong></td>
      <td>Learn by flowing error backward</td>
    </tr>
    <tr>
      <td><strong>Transformer</strong></td>
      <td>Attention over all tokens at once</td>
    </tr>
    <tr>
      <td><strong>Mamba</strong></td>
      <td>Linear-time sequence modeling</td>
    </tr>
    <tr>
      <td><strong>Hallucination</strong></td>
      <td>Confident nonsense from likelihood optimization</td>
    </tr>
    <tr>
      <td><strong>Embedding</strong></td>
      <td>Meaning as coordinates in vector space</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 1 of the Five ML Concepts series. <a href="/series/#five-ml-concepts">View all parts</a></td>
      <td><a href="/2026/02/05/five-ml-concepts-2/">Next: #2 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Short, accurate ML explainers. Follow for more.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-Zu3zreN8c0Q">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-Zu3zreN8c0Q"
      src="https://www.youtube.com/embed/Zu3zreN8c0Q?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-Zu3zreN8c0Q';
  const playerId = 'yt-player-Zu3zreN8c0Q';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="37"><p class="post-meta">February 3, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">870 words</span> &bull; <span class="post-read-time">5 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">LLMs are black boxes. Baby Dragon Hatchling uses brain-inspired sparse coding with 80% sparsity, making only 20% of neurons active per token. When fewer neurons fire, each one carries interpretable meaning. Train it on Shakespeare and actually see what's happening inside.</div><h3>
          <a class="post-link" href="/2026/02/03/small-models-part4-bdh/">
            Small Models (4/6): This AI Has a Visible Brain
          </a>
        </h3><nav class="toc" data-toc-id="small-models-part4-bdh">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/veggies.png" class="post-marker" alt="" /></p>

<p>LLMs are black boxes. Baby Dragon Hatchling (BDH) is different—a brain-inspired language model with sparse, interpretable activations.</p>

<p>Train it on Shakespeare and actually <em>see</em> what’s happening inside.</p>

<p>This is Part 4 of the <strong>Small Models, Big Brains</strong> series, exploring interpretability through sparsity.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2410.16179">Pathway (Sparse Coding)</a></td>
      </tr>
      <tr>
        <td><strong>Original Code</strong></td>
        <td><a href="https://github.com/pathwaycom/bdh">pathwaycom/bdh</a></td>
      </tr>
      <tr>
        <td><strong>Fork (with tools)</strong></td>
        <td><a href="https://github.com/softwarewrighter/bdh">softwarewrighter/bdh</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/pldadqycEQs">This AI Has a Visible Brain</a><br /><a href="https://www.youtube.com/shorts/pldadqycEQs"><img src="https://img.youtube.com/vi/pldadqycEQs/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-black-box-problem">The Black Box Problem</h2>

<p>Modern neural networks are opaque:</p>

<ul>
  <li>Billions of parameters</li>
  <li>Dense activations everywhere</li>
  <li>No clear mapping from neurons to concepts</li>
  <li>“It works, but we don’t know why”</li>
</ul>

<p>This isn’t just an academic concern. We’re deploying AI systems we don’t understand.</p>

<h2 id="baby-dragon-hatchling-a-different-approach">Baby Dragon Hatchling: A Different Approach</h2>

<p>BDH takes inspiration from biological brains, which use <strong>sparse coding</strong>:</p>

<table>
  <thead>
    <tr>
      <th>Biological Brains</th>
      <th>Dense Neural Networks</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>~1-5% neurons active</td>
      <td>~100% neurons active</td>
    </tr>
    <tr>
      <td>Energy efficient</td>
      <td>Computationally expensive</td>
    </tr>
    <tr>
      <td>Interpretable patterns</td>
      <td>Distributed, opaque</td>
    </tr>
    <tr>
      <td>Robust to noise</td>
      <td>Brittle</td>
    </tr>
  </tbody>
</table>

<h3 id="sparse-activations">Sparse Activations</h3>

<p>BDH enforces <strong>80% sparsity</strong>—only 20% of neurons are active for any given token.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dense Network:    [████████████████████] 100% active
BDH:              [████░░░░░░░░░░░░░░░░]  20% active
</code></pre></div></div>

<p>This constraint forces the network to learn meaningful, localized representations.</p>

<h2 id="training-on-shakespeare">Training on Shakespeare</h2>

<p>The demo trains BDH on Shakespeare’s works:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training Progress:
Epoch 1:   Loss 0.86
Epoch 50:  Loss 0.54
Epoch 100: Loss 0.38
Epoch 200: Loss 0.22
</code></pre></div></div>

<p>Loss drops from 0.86 to 0.22—the architecture works.</p>

<h2 id="seeing-inside-the-model">Seeing Inside the Model</h2>

<p>With sparse activations, you can actually inspect what neurons mean:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Which neurons fire for "love"?
</span><span class="n">activations</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="sh">"</span><span class="s">love</span><span class="sh">"</span><span class="p">)</span>
<span class="n">active_neurons</span> <span class="o">=</span> <span class="n">activations</span><span class="p">.</span><span class="nf">nonzero</span><span class="p">()</span>

<span class="c1"># Neuron 47: fires for emotional words
# Neuron 112: fires for abstract nouns
# Neuron 203: fires for relationship terms
</span></code></pre></div></div>

<p>When only 20% of neurons fire, each one carries interpretable meaning.</p>

<h2 id="running-the-code">Running the Code</h2>

<p>The <a href="https://github.com/softwarewrighter/bdh">bdh</a> repository is a fork of Pathway’s original with added inspection tools:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/softwarewrighter/bdh
<span class="nb">cd </span>bdh
pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt

<span class="c"># Train on Shakespeare</span>
python train.py <span class="nt">--dataset</span> shakespeare <span class="nt">--sparsity</span> 0.8

<span class="c"># Inspect activations</span>
python inspect.py <span class="nt">--model</span> checkpoint.pt <span class="nt">--text</span> <span class="s2">"To be or not to be"</span>
</code></pre></div></div>

<p>GPU recommended (Nvidia or Apple Silicon) for reasonable training times.</p>

<h2 id="why-sparsity-enables-interpretability">Why Sparsity Enables Interpretability</h2>

<h3 id="dense-networks">Dense Networks</h3>

<p>Every neuron participates in every computation. The “meaning” of any single neuron is distributed across all inputs it ever sees.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: "cat"  → All neurons contribute → Output
Input: "dog"  → All neurons contribute → Output
Input: "love" → All neurons contribute → Output
</code></pre></div></div>

<p>Trying to understand one neuron means understanding <em>everything</em>.</p>

<h3 id="sparse-networks">Sparse Networks</h3>

<p>Only a small subset of neurons fire for each input. Neurons develop <em>specialization</em>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: "cat"  → Neurons [12, 47, 89] fire → Output
Input: "dog"  → Neurons [12, 52, 89] fire → Output
Input: "love" → Neurons [47, 112, 203] fire → Output
</code></pre></div></div>

<p>Neuron 12 might mean “animal.” Neuron 47 might mean “emotional/living.” You can actually trace meaning.</p>

<h2 id="comparison-with-other-sparse-architectures">Comparison with Other Sparse Architectures</h2>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Sparsity Type</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Mixture of Experts</td>
      <td>Routing sparsity</td>
      <td>Efficiency</td>
    </tr>
    <tr>
      <td>Top-k attention</td>
      <td>Attention sparsity</td>
      <td>Memory</td>
    </tr>
    <tr>
      <td><strong>BDH</strong></td>
      <td>Activation sparsity</td>
      <td><strong>Interpretability</strong></td>
    </tr>
  </tbody>
</table>

<p>BDH’s sparsity is specifically designed for understanding, not just efficiency.</p>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Primary Language</strong></td>
      <td>Python</td>
    </tr>
    <tr>
      <td><strong>Source Files</strong></td>
      <td>9 <code class="language-plaintext highlighter-rouge">.py</code> files</td>
    </tr>
    <tr>
      <td><strong>Estimated Size</strong></td>
      <td>~1.5 KLOC</td>
    </tr>
    <tr>
      <td><strong>Framework</strong></td>
      <td>PyTorch</td>
    </tr>
    <tr>
      <td><strong>Build System</strong></td>
      <td>pip / requirements.txt</td>
    </tr>
    <tr>
      <td><strong>GPU Support</strong></td>
      <td>CUDA, MPS (Apple Silicon)</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to experiment with sparse neural architectures, study interpretability techniques, or train small language models with visible internals.</p>

<p><strong>Complexity:</strong> Low-Moderate. Standard PyTorch project structure. The sparse activation mechanism is well-documented. Fork includes additional inspection tools not in the original.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>Sparsity enables interpretability.</strong> When fewer neurons fire, each one means more.</p>
  </li>
  <li>
    <p><strong>Brain-inspired design works.</strong> Biological neural coding principles transfer to AI.</p>
  </li>
  <li>
    <p><strong>Interpretability doesn’t require sacrifice.</strong> BDH learns effectively despite constraints.</p>
  </li>
  <li>
    <p><strong>We can build AI we understand.</strong> Black boxes aren’t inevitable.</p>
  </li>
</ol>

<h2 id="current-limitations">Current Limitations</h2>

<ul>
  <li>Early research stage</li>
  <li>Smaller scale than production models</li>
  <li>Training requires more epochs</li>
  <li>Not yet competitive with dense models on benchmarks</li>
</ul>

<p>But the principle is sound: <strong>constraint breeds clarity</strong>.</p>

<h2 id="whats-next">What’s Next</h2>

<p>Part 5 dives into the <strong>1B parameter sweet spot</strong>—comparing TinyLlama, Llama 3.2, StableLM, and Pythia.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2410.16179">Pathway Paper</a></li>
  <li><a href="https://github.com/pathwaycom/bdh">Original Pathway Code</a></li>
  <li><a href="https://github.com/softwarewrighter/bdh">bdh Repository (with inspection tools)</a></li>
  <li><a href="https://youtube.com/shorts/VIDEO_ID">Video: This AI Has a Visible Brain</a></li>
</ul>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 4 of 6 in the Small Models, Big Brains series. <a href="/series/#small-models-big-brains">View all parts</a></td>
      <td><a href="/2026/02/04/small-models-part5-billion-llm/">Next: Part 5 →</a>*</td>
    </tr>
  </tbody>
</table>

          </div>





<div class="youtube-embed-container" id="yt-container-pldadqycEQs">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-pldadqycEQs"
      src="https://www.youtube.com/embed/pldadqycEQs?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-pldadqycEQs';
  const playerId = 'yt-player-pldadqycEQs';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="38"><p class="post-meta">February 3, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">1465 words</span> &bull; <span class="post-read-time">8 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Single explorer: 0% success. Five explorers: 60% success. Sparse rewards are an information problem, not a compute problem. Using multiple scouts with different exploration strategies, we gather diverse discoveries that benefit a shared learner.</div><h3>
          <a class="post-link" href="/2026/02/03/many-eyes-learning/">
            Solving Sparse Rewards with Many Eyes
          </a>
        </h3><nav class="toc" data-toc-id="many-eyes-learning">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/many-eyes-monster.gif" class="post-marker" alt="" /></p>

<p>Single explorer: 0% success. Five explorers: 60% success.</p>

<p>Learning often fails not because models are slow, but because they <strong>see too little</strong>. In sparse-reward environments, a single explorer is likely to miss the rare feedback entirely. The solution? <strong>Put many eyes on the problem.</strong></p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Papers</strong></td>
        <td><a href="https://arxiv.org/abs/2601.21391">IRPO</a> · <a href="https://arxiv.org/abs/2601.22154">Reagent</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/many-eyes-learning">many-eyes-learning</a></td>
      </tr>
      <tr>
        <td><strong>ELI5</strong></td>
        <td><a href="https://github.com/softwarewrighter/many-eyes-learning/blob/main/docs/eli5.md">eli5.md</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/KgtiBy4rdm4">Given enough eyeballs…</a><br /><a href="https://www.youtube.com/shorts/KgtiBy4rdm4"><img src="https://img.youtube.com/vi/KgtiBy4rdm4/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-problem-sparse-rewards-create-blindness">The Problem: Sparse Rewards Create Blindness</h2>

<p>As IRPO formalizes: in sparse-reward RL, <strong>the true policy gradient is basically uninformative most of the time</strong>. No reward signal → no gradient signal.</p>

<p>A 7x7 grid with a single goal demonstrates this perfectly:</p>

<ul>
  <li>Random agent success rate: ~9%</li>
  <li>With limited training (75 episodes), a single learner exploring alone <strong>never finds the goal</strong></li>
</ul>

<p>This isn’t a compute problem. It’s an <strong>information problem</strong>.</p>

<div class="references-float">

  <table>
    <thead>
      <tr>
        <th>Challenge</th>
        <th>Effect</th>
        <th>Paper Connection</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Rare rewards</td>
        <td>Weak gradient signal</td>
        <td>IRPO’s core problem statement</td>
      </tr>
      <tr>
        <td>Single explorer</td>
        <td>Limited coverage</td>
        <td>Why multiple scouts help</td>
      </tr>
      <tr>
        <td>Random exploration</td>
        <td>Misses valuable states</td>
        <td>Why intrinsic rewards matter</td>
      </tr>
      <tr>
        <td>No feedback structure</td>
        <td>Can’t distinguish “almost right” from “nonsense”</td>
        <td>Reagent’s motivation</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-solution-many-eyes">The Solution: Many Eyes</h2>

<p>Instead of one explorer, use multiple <strong>scouts</strong>—independent exploratory agents that gather diverse information.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│   Scout 1   │  │   Scout 2   │  │   Scout N   │
│ (strategy A)│  │ (strategy B)│  │ (strategy N)│
└──────┬──────┘  └──────┬──────┘  └──────┬──────┘
       │                │                │
       v                v                v
┌─────────────────────────────────────────────────┐
│              Experience Buffer                   │
└─────────────────────────────────────────────────┘
                       │
                       v
┌─────────────────────────────────────────────────┐
│               Shared Learner                     │
└─────────────────────────────────────────────────┘
</code></pre></div></div>

<p>Each scout explores with its own strategy. Their discoveries are aggregated to improve a shared learner.</p>

<h2 id="results">Results</h2>

<p>On a 7x7 sparse grid with 75 training episodes:</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Success Rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Random baseline</td>
      <td>9%</td>
    </tr>
    <tr>
      <td><strong>Single scout</strong></td>
      <td><strong>0%</strong></td>
    </tr>
    <tr>
      <td>Many eyes (3 scouts)</td>
      <td>40%</td>
    </tr>
    <tr>
      <td><strong>Many eyes (5 scouts)</strong></td>
      <td><strong>60%</strong></td>
    </tr>
  </tbody>
</table>

<p>Same total environment steps. Dramatically better outcomes.</p>

<h2 id="why-it-works">Why It Works</h2>

<h3 id="single-scout-fails-because">Single Scout Fails Because:</h3>

<p>In IRPO terms: <strong>sparse reward → sparse gradient signal → no learning</strong>.</p>

<ol>
  <li>Random exploration rarely reaches the goal (~9%)</li>
  <li>Insufficient successful trajectories</li>
  <li>DQN can’t learn from sparse positive examples</li>
  <li>The policy gradient has near-zero magnitude</li>
</ol>

<h3 id="many-eyes-succeeds-because">Many Eyes Succeeds Because:</h3>

<p>IRPO’s key insight: <strong>multiple exploratory policies manufacture signal</strong>.</p>

<ol>
  <li><strong>More coverage</strong>: Different scouts explore different regions (intrinsic rewards drive novelty-seeking)</li>
  <li><strong>More discoveries</strong>: Higher probability of reaching goal (scouts find extrinsic reward)</li>
  <li><strong>Signal routing</strong>: Scout discoveries update the shared learner (surrogate gradient in IRPO, experience pooling in many-eyes)</li>
  <li><strong>Better gradients</strong>: Aggregated experience provides meaningful learning signal</li>
</ol>

<h2 id="scout-strategies-intrinsic-rewards">Scout Strategies (Intrinsic Rewards)</h2>

<p>IRPO uses <strong>intrinsic rewards</strong> to drive exploration. The many-eyes-learning project implements several strategies:</p>

<table>
  <thead>
    <tr>
      <th>Strategy</th>
      <th>Intrinsic Motivation</th>
      <th>IRPO Connection</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Epsilon-greedy</strong></td>
      <td>Random action with probability ε</td>
      <td>Simple exploration noise</td>
    </tr>
    <tr>
      <td><strong>Curious</strong></td>
      <td>Bonus for novel states: <code class="language-plaintext highlighter-rouge">1/√(count+1)</code></td>
      <td>Count-based intrinsic reward</td>
    </tr>
    <tr>
      <td><strong>Optimistic</strong></td>
      <td>High initial Q-values</td>
      <td>Optimism under uncertainty</td>
    </tr>
    <tr>
      <td><strong>Random</strong></td>
      <td>Pure random baseline</td>
      <td>Maximum entropy exploration</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># CuriousScout intrinsic reward (simplified)
</span><span class="k">def</span> <span class="nf">intrinsic_reward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">state_counts</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">bonus_scale</span> <span class="o">/</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">count</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Scouts can be <strong>homogeneous</strong> (same strategy, different seeds) or <strong>heterogeneous</strong> (different strategies). IRPO supports swapping intrinsic reward functions—many-eyes makes this concrete with pluggable scout types.</p>

<h2 id="running-the-demo">Running the Demo</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/softwarewrighter/many-eyes-learning
<span class="nb">cd </span>many-eyes-learning

<span class="c"># Setup</span>
uv venv .venv
<span class="nb">source</span> .venv/bin/activate
uv pip <span class="nb">install</span> <span class="nt">-e</span> <span class="s2">".[dev]"</span>

<span class="c"># Interactive CLI demo</span>
python experiments/cli_demo.py

<span class="c"># Full experiment</span>
python experiments/run_experiment.py <span class="nt">--episodes</span> 75 <span class="nt">--scouts</span> 1 3 5

<span class="c"># Generate plots</span>
python experiments/plot_results.py
</code></pre></div></div>

<p>Results appear in ~5-10 minutes on a laptop.</p>

<h2 id="diversity-experiment">Diversity Experiment</h2>

<p>Does <strong>diversity of strategies</strong> matter, or just <strong>number of scouts</strong>?</p>

<table>
  <thead>
    <tr>
      <th>Configuration</th>
      <th>Success Rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>5 random scouts</td>
      <td>20%</td>
    </tr>
    <tr>
      <td>5 epsilon-greedy scouts</td>
      <td>40%</td>
    </tr>
    <tr>
      <td>5 diverse scouts (mixed strategies)</td>
      <td>40%</td>
    </tr>
  </tbody>
</table>

<p><strong>Finding</strong>: In simple environments, strategy quality matters more than diversity. Epsilon-greedy beats random regardless of diversity.</p>

<h2 id="key-insight">Key Insight</h2>

<blockquote>
  <p><em>The problem isn’t that learning is slow. The problem is that learning is blind.</em></p>
</blockquote>

<p>Many eyes make learning better.</p>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Primary Language</strong></td>
      <td>Python</td>
    </tr>
    <tr>
      <td><strong>Source Files</strong></td>
      <td>~12 <code class="language-plaintext highlighter-rouge">.py</code> files</td>
    </tr>
    <tr>
      <td><strong>Estimated Size</strong></td>
      <td>~1.5 KLOC</td>
    </tr>
    <tr>
      <td><strong>Framework</strong></td>
      <td>PyTorch, NumPy</td>
    </tr>
    <tr>
      <td><strong>Platform</strong></td>
      <td>CPU (no GPU required)</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to understand exploration in RL, experiment with sparse-reward environments, or see a clean implementation of scout-based learning.</p>

<p><strong>Complexity:</strong> Low-Moderate. Clean codebase with CLI demos. Runs on a laptop in minutes.</p>

<h2 id="design-philosophy">Design Philosophy</h2>

<p>The project prioritizes <strong>clarity over performance</strong>:</p>

<ul>
  <li>Single-file implementations where practical</li>
  <li>Minimal dependencies</li>
  <li>Sequential mode is first-class (parallel optional)</li>
  <li>Reproducible experiments with fixed seeds</li>
</ul>

<h3 id="simplifications-from-irpo">Simplifications from IRPO</h3>

<p>Full IRPO computes Jacobians to route gradients from exploratory policies back to the base policy. Many-eyes-learning simplifies this:</p>

<table>
  <thead>
    <tr>
      <th>IRPO</th>
      <th>Many-Eyes-Learning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Jacobian chain rule</td>
      <td>Experience pooling</td>
    </tr>
    <tr>
      <td>Surrogate gradient</td>
      <td>Standard DQN updates</td>
    </tr>
    <tr>
      <td>Learned intrinsic rewards</td>
      <td>Hand-designed strategies</td>
    </tr>
  </tbody>
</table>

<p>The core insight remains: <strong>scouts explore with intrinsic motivation, discoveries benefit the shared learner</strong>. The math is simpler, the demo runs on a laptop, and the concept is clear.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>Sparse rewards create information bottlenecks.</strong> Learning fails not from lack of compute, but lack of signal.</p>
  </li>
  <li>
    <p><strong>More eyes = more information.</strong> Multiple scouts increase coverage and discovery rate.</p>
  </li>
  <li>
    <p><strong>Diversity helps, but quality matters more.</strong> In simple environments, good exploration strategy beats diversity.</p>
  </li>
  <li>
    <p><strong>Same compute, better outcomes.</strong> Many-eyes improves sample efficiency, not wall-clock speed.</p>
  </li>
</ol>

<h2 id="the-papers-behind-many-eyes">The Papers Behind Many-Eyes</h2>

<p>This project builds on two recent papers that attack the same fundamental problem: <strong>sparse rewards starve learning of signal</strong>.</p>

<h3 id="irpo-intrinsic-reward-policy-optimization">IRPO: Intrinsic Reward Policy Optimization</h3>

<p><a href="https://arxiv.org/abs/2601.21391">IRPO</a> (Cho &amp; Tran, UIUC) formalizes the scouts concept mathematically.</p>

<p><strong>The core insight:</strong> In sparse-reward RL, the true policy gradient is basically uninformative most of the time. No reward signal → no gradient signal. Learning stalls.</p>

<p><strong>IRPO’s solution:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────────────────────┐
│  1. Train exploratory policies (scouts)         │
│     using INTRINSIC rewards                     │
├─────────────────────────────────────────────────┤
│  2. Scouts discover EXTRINSIC rewards           │
│     through exploration                         │
├─────────────────────────────────────────────────┤
│  3. Route extrinsic signal back to base policy  │
│     via surrogate gradient (Jacobian chain)     │
└─────────────────────────────────────────────────┘
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>IRPO Concept</th>
      <th>What It Means</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Intrinsic rewards</strong></td>
      <td>“Explore what’s new” - reward novelty</td>
    </tr>
    <tr>
      <td><strong>Exploratory policies</strong></td>
      <td>Scouts driven by intrinsic motivation</td>
    </tr>
    <tr>
      <td><strong>Surrogate gradient</strong></td>
      <td>Trade bias for signal - approximate gradient that actually has magnitude</td>
    </tr>
    <tr>
      <td><strong>Base policy</strong></td>
      <td>The learner that benefits from scout discoveries</td>
    </tr>
  </tbody>
</table>

<p><strong>How many-eyes-learning demonstrates this:</strong></p>

<ul>
  <li><strong>Scouts</strong> implement intrinsic motivation (CuriousScout uses count-based novelty bonuses)</li>
  <li><strong>Multiple exploration strategies</strong> create diverse coverage</li>
  <li><strong>Aggregated experience</strong> routes discoveries to the shared DQN learner</li>
  <li><strong>Simplified gradient routing</strong> - we pool experiences rather than compute full Jacobians</li>
</ul>

<h3 id="reagent-reasoning-reward-models-for-agents">Reagent: Reasoning Reward Models for Agents</h3>

<p><a href="https://arxiv.org/abs/2601.22154">Reagent</a> (Fan et al., CUHK/Meituan) takes a different approach: make feedback <strong>richer and more structured</strong>.</p>

<p><strong>The problem with sparse rewards:</strong> They can’t distinguish “almost right, failed at the end” from “complete nonsense.” Both get the same zero reward.</p>

<p><strong>Reagent’s solution:</strong> Build a Reasoning Reward Model that emits:</p>

<table>
  <thead>
    <tr>
      <th>Signal</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">&lt;think&gt;</code></td>
      <td>Explicit reasoning trace</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">&lt;critique&gt;</code></td>
      <td>Targeted natural-language feedback</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">&lt;score&gt;</code></td>
      <td>Overall scalar reward</td>
    </tr>
  </tbody>
</table>

<p>This provides <strong>dense-ish supervision without hand-labeling every step</strong>.</p>

<p><strong>How many-eyes-learning relates:</strong></p>

<ul>
  <li>Both papers recognize sparse rewards as an <strong>information problem</strong></li>
  <li>Reagent enriches the reward signal; IRPO multiplies the exploration</li>
  <li>Many-eyes takes the IRPO path: <strong>more explorers finding the sparse signal</strong></li>
  <li>Future work could combine both: scouts + richer feedback per trajectory</li>
</ul>

<h3 id="the-shared-meta-lesson">The Shared Meta-Lesson</h3>

<p>Both papers are saying the same thing:</p>

<blockquote>
  <p><strong>Sparse signals are a tragedy. Let’s smuggle in richer ones.</strong></p>
</blockquote>

<ul>
  <li>IRPO: via intrinsic-reward exploration gradients</li>
  <li>Reagent: via language-based reward feedback</li>
</ul>

<p>Many-eyes-learning demonstrates the IRPO intuition in a simple, visual, reproducible way.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2601.21391">IRPO Paper (arXiv:2601.21391)</a></li>
  <li><a href="https://arxiv.org/abs/2601.22154">Reagent Paper (arXiv:2601.22154)</a></li>
  <li><a href="https://github.com/softwarewrighter/many-eyes-learning">many-eyes-learning Repository</a></li>
  <li><a href="https://github.com/softwarewrighter/many-eyes-learning/blob/main/docs/results.md">Results Documentation</a></li>
  <li><a href="https://github.com/softwarewrighter/many-eyes-learning/blob/main/docs/architecture.md">Architecture Documentation</a></li>
</ul>

<hr />

<p><em>Sparse rewards are an information problem. Many eyes provide the solution.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-KgtiBy4rdm4">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-KgtiBy4rdm4"
      src="https://www.youtube.com/embed/KgtiBy4rdm4?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-KgtiBy4rdm4';
  const playerId = 'yt-player-KgtiBy4rdm4';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="39"><p class="post-meta">February 2, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">661 words</span> &bull; <span class="post-read-time">4 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Teaching Claude to play tic-tac-toe and trash talk using Model Context Protocol (MCP). A Rust server exposes 6 tools via JSON-RPC over stdio, proving MCP standardizes AI tool integration across any compatible language model.</div><h3>
          <a class="post-link" href="/2026/02/02/game-mcp-poc/">
            MCP: Teaching Claude to Play (and Trash Talk)
          </a>
        </h3><nav class="toc" data-toc-id="game-mcp-poc">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/posts/game-mcp-poc/tic-tac-toe.png" class="post-marker" alt="" /></p>

<p>Claude learned to play tic-tac-toe. And trash talk. Using one protocol that works with any language model.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/sw-game-dev/game-mcp-poc">game-mcp-poc</a></td>
      </tr>
      <tr>
        <td><strong>MCP Spec</strong></td>
        <td><a href="https://modelcontextprotocol.io/">modelcontextprotocol.io</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/n_NFFLCtj_M">Claude Plays Tic-Tac-Toe</a><br /><a href="https://www.youtube.com/shorts/n_NFFLCtj_M"><img src="https://img.youtube.com/vi/n_NFFLCtj_M/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-problem">The Problem</h2>

<p>Language models are stuck in text. They can’t click buttons, make moves, or interact with real systems. Every integration is custom—different for Claude, GPT, Gemini.</p>

<h2 id="the-solution-mcp">The Solution: MCP</h2>

<p><strong>Model Context Protocol</strong> is a standard way for models to use tools. Define your tools once, they work with Claude, GPT, or any MCP-compatible agent.</p>

<p>The protocol is simple:</p>
<ul>
  <li><strong>JSON-RPC 2.0</strong> over stdio</li>
  <li>No HTTP needed</li>
  <li>Clean request/response cycle</li>
</ul>

<h2 id="the-demo-trash-talkin-tic-tac-toe">The Demo: Trash Talkin’ Tic Tac Toe</h2>

<p>This proof-of-concept implements 6 MCP tools:</p>

<table>
  <thead>
    <tr>
      <th>Tool</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">view_game_state</code></td>
      <td>See the board, players, status</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">get_turn</code></td>
      <td>Whose turn is it?</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">make_move</code></td>
      <td>Play a square (row, col)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">taunt_player</code></td>
      <td>Send trash talk to opponent</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">restart_game</code></td>
      <td>Start a new game</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">get_game_history</code></td>
      <td>All moves with timestamps</td>
    </tr>
  </tbody>
</table>

<p>The AI calls tools, the server responds. Claude can play a full game AND talk trash—all through the same protocol.</p>

<h2 id="architecture">Architecture</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────────────────┐
│            Claude Code (AI)                 │
│              (MCP Client)                   │
└──────────────────┬──────────────────────────┘
                   │ JSON-RPC 2.0 via stdio
                   ▼
┌─────────────────────────────────────────────┐
│         MCP Server (Rust Binary)            │
│  ┌───────────────────────────────────────┐  │
│  │  6 Tools: view, turn, move, taunt,   │  │
│  │           restart, history            │  │
│  └───────────────────────────────────────┘  │
│                   ▼                         │
│  ┌───────────────────────────────────────┐  │
│  │      SQLite (game.db)                 │  │
│  │  • Games • Moves • Taunts             │  │
│  └───────────────────────────────────────┘  │
└─────────────────────────────────────────────┘
         ▲                           ▲
         │ REST API                  │ MCP
         │                           │
    Browser (UI)              AI Agent
    (Yew/WASM)              (Claude Code)
</code></pre></div></div>

<h2 id="running-it">Running It</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/sw-game-dev/game-mcp-poc
<span class="nb">cd </span>game-mcp-poc

<span class="c"># Development mode (with hot-reload)</span>
./scripts/dev.sh

<span class="c"># Or production build</span>
./scripts/build.sh
./scripts/serve.sh
</code></pre></div></div>

<p>The server runs on <code class="language-plaintext highlighter-rouge">http://localhost:7397</code> serving:</p>
<ul>
  <li>REST API for UI interactions</li>
  <li>MCP endpoint for AI agents</li>
  <li>SSE for real-time updates</li>
  <li>Yew/WASM frontend</li>
</ul>

<h2 id="configuring-claude-code">Configuring Claude Code</h2>

<p>Add to <code class="language-plaintext highlighter-rouge">~/.config/claude-code/mcp.json</code>:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"mcpServers"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"tic-tac-toe"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"command"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/path/to/game-mcp-poc/target/release/game-mcp-server"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"args"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
      </span><span class="nl">"env"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"GAME_DB_PATH"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/path/to/game.db"</span><span class="w">
      </span><span class="p">}</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Restart Claude Code, then:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>You: "Let's play tic-tac-toe! Show me the board."
You: "I'll take the center."
You: "Your turn!"
You: "Can you taunt me?"
</code></pre></div></div>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Language</strong></td>
      <td>Rust 2024 Edition</td>
    </tr>
    <tr>
      <td><strong>Frontend</strong></td>
      <td>Yew + WebAssembly</td>
    </tr>
    <tr>
      <td><strong>Database</strong></td>
      <td>SQLite</td>
    </tr>
    <tr>
      <td><strong>Tests</strong></td>
      <td>175+ passing</td>
    </tr>
    <tr>
      <td><strong>LOC</strong></td>
      <td>~2,500 (backend) + ~1,500 (tests)</td>
    </tr>
    <tr>
      <td><strong>Binary Size</strong></td>
      <td>~8 MB</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to learn MCP, build AI-tool integrations, or see a production-quality Rust game server.</p>

<p><strong>Complexity:</strong> Moderate. Clean architecture with TDD. Requires Rust toolchain and understanding of JSON-RPC.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>MCP standardizes AI tools.</strong> Define once, works with any compatible model.</p>
  </li>
  <li>
    <p><strong>JSON-RPC over stdio is elegant.</strong> No HTTP complexity for local tools.</p>
  </li>
  <li>
    <p><strong>Rust + WASM = fast everywhere.</strong> Same language for server and (via Yew) frontend.</p>
  </li>
  <li>
    <p><strong>Trash talk is essential.</strong> Games without taunting are just… exercises.</p>
  </li>
</ol>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://github.com/sw-game-dev/game-mcp-poc">game-mcp-poc Repository</a></li>
  <li><a href="https://modelcontextprotocol.io/">MCP Specification</a></li>
  <li><a href="https://www.jsonrpc.org/specification">JSON-RPC 2.0 Spec</a></li>
  <li><a href="https://www.youtube.com/shorts/n_NFFLCtj_M">Video: Claude Plays Tic-Tac-Toe</a></li>
</ul>

<hr />

<p><em>MCP turns language models into tool users. This demo proves it works—and that AI can talk trash.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-n_NFFLCtj_M">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-n_NFFLCtj_M"
      src="https://www.youtube.com/embed/n_NFFLCtj_M?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-n_NFFLCtj_M';
  const playerId = 'yt-player-n_NFFLCtj_M';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="40"><p class="post-meta">February 2, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">815 words</span> &bull; <span class="post-read-time">5 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">27 million parameters beats o3-mini on ARC. The Hierarchical Reasoning Model separates planning from execution, mimicking the brain's dual-process theory. It achieves 40% on the hardest reasoning benchmark where most LLMs score under 5%.</div><h3>
          <a class="post-link" href="/2026/02/02/small-models-part3-hrm/">
            Small Models (3/6): Planner + Doer = Genius
          </a>
        </h3><nav class="toc" data-toc-id="small-models-part3-hrm">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/site/post-marker-floral.png" class="post-marker" alt="" /></p>

<p>27 million parameters beats o3-mini on ARC.</p>

<p>The hardest reasoning benchmark. Most LLMs score under 5 percent. This tiny model scores 40 percent.</p>

<p>This is Part 3 of the <strong>Small Models, Big Brains</strong> series, exploring the <strong>Hierarchical Reasoning Model (HRM)</strong>—a brain-inspired architecture that separates planning from execution.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2506.21734">Hierarchical Reasoning Model</a></td>
      </tr>
      <tr>
        <td><strong>Original Code</strong></td>
        <td><a href="https://github.com/sapientinc/HRM">sapientinc/HRM</a></td>
      </tr>
      <tr>
        <td><strong>Visualization</strong></td>
        <td><a href="https://github.com/softwarewrighter/viz-hrm-ft">viz-hrm-ft</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/q_8qX-S9SxA">Planner + Doer = Genius</a><br /><a href="https://www.youtube.com/shorts/q_8qX-S9SxA"><img src="https://img.youtube.com/vi/q_8qX-S9SxA/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-arc-challenge">The ARC Challenge</h2>

<p>The <a href="https://arcprize.org/">Abstraction and Reasoning Corpus (ARC)</a> tests:</p>

<ul>
  <li>Abstract reasoning</li>
  <li>Pattern matching</li>
  <li>Spatial logic</li>
  <li>Puzzles requiring real thinking</li>
</ul>

<p>These aren’t problems you can memorize. Each puzzle is unique, requiring genuine understanding of the underlying pattern.</p>

<h3 id="why-llms-struggle">Why LLMs Struggle</h3>

<table>
  <thead>
    <tr>
      <th>Challenge</th>
      <th>LLM Limitation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Novel patterns</td>
      <td>Can’t rely on training data</td>
    </tr>
    <tr>
      <td>Spatial reasoning</td>
      <td>Text-based thinking is linearized</td>
    </tr>
    <tr>
      <td>Multi-step logic</td>
      <td>Each step compounds errors</td>
    </tr>
    <tr>
      <td>Abstraction</td>
      <td>Pattern matching isn’t generalization</td>
    </tr>
  </tbody>
</table>

<h2 id="meet-hrm-the-hierarchical-reasoning-model">Meet HRM: The Hierarchical Reasoning Model</h2>

<p>HRM uses just 27 million parameters but achieves remarkable results by mimicking how the brain thinks: <strong>plan first, then act</strong>.</p>

<h3 id="two-module-architecture">Two-Module Architecture</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────────┐
│           PLANNER                   │
│   Thinks slow and abstract          │
│   Sets goals and strategies         │
└─────────────┬───────────────────────┘
              │ Goals
              ▼
┌─────────────────────────────────────┐
│            DOER                     │
│   Moves fast                        │
│   Takes concrete actions            │
└─────────────────────────────────────┘
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>Module</th>
      <th>Speed</th>
      <th>Function</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Planner</strong></td>
      <td>Slow</td>
      <td>Abstract thinking, goal setting</td>
    </tr>
    <tr>
      <td><strong>Doer</strong></td>
      <td>Fast</td>
      <td>Concrete actions, execution</td>
    </tr>
  </tbody>
</table>

<p>This mirrors the brain’s dual-process theory: System 1 (fast, intuitive) and System 2 (slow, deliberate).</p>

<h2 id="results">Results</h2>

<table>
  <thead>
    <tr>
      <th>Benchmark</th>
      <th>HRM (27M)</th>
      <th>o3-mini</th>
      <th>GPT-4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>ARC</strong></td>
      <td>40%</td>
      <td>&lt;40%</td>
      <td>&lt;5%</td>
    </tr>
    <tr>
      <td>Hard Mazes</td>
      <td>99%</td>
      <td>-</td>
      <td>~0%</td>
    </tr>
    <tr>
      <td>Complex Sudoku</td>
      <td>99%</td>
      <td>-</td>
      <td>-</td>
    </tr>
  </tbody>
</table>

<p>A 27M parameter model outperforming models 1000x larger on reasoning tasks.</p>

<h2 id="the-visualization-tool">The Visualization Tool</h2>

<p>The <a href="https://github.com/softwarewrighter/viz-hrm-ft">viz-hrm-ft</a> repository provides a React app to visualize HRM’s reasoning process:</p>

<ul>
  <li>Watch the Planner form strategies</li>
  <li>See the Doer execute actions</li>
  <li>Visualize the feedback loop between modules</li>
  <li>Simulate fine-tuning on BabyAI tasks</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/softwarewrighter/viz-hrm-ft
<span class="nb">cd </span>viz-hrm-ft
npm <span class="nb">install
</span>npm start
</code></pre></div></div>

<h2 id="why-hierarchy-works">Why Hierarchy Works</h2>

<h3 id="traditional-flat-models">Traditional Flat Models</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input → [Single Network] → Output
</code></pre></div></div>

<p>Everything happens in one pass. Complex problems overwhelm the network.</p>

<h3 id="hierarchical-models">Hierarchical Models</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input → [Planner] → Strategy
                  ↓
Strategy → [Doer] → Action
                  ↓
Action → [Environment] → Feedback
                       ↓
Feedback → [Planner] → Refined Strategy
                     ↓
                    ...
</code></pre></div></div>

<p>The Planner doesn’t worry about details. The Doer doesn’t worry about strategy. Each module focuses on what it does best.</p>

<h2 id="key-insights">Key Insights</h2>

<ol>
  <li>
    <p><strong>Separation of concerns scales.</strong> Splitting planning from execution lets each module specialize.</p>
  </li>
  <li>
    <p><strong>Iteration enables refinement.</strong> The Planner-Doer loop allows course correction.</p>
  </li>
  <li>
    <p><strong>Small can beat big.</strong> 27M parameters with good architecture beats 100B+ with brute force.</p>
  </li>
  <li>
    <p><strong>Brain-inspired design works.</strong> Mimicking cognitive architecture yields better results.</p>
  </li>
</ol>

<h2 id="comparison-with-part-1-trm">Comparison with Part 1 (TRM)</h2>

<table>
  <thead>
    <tr>
      <th>Aspect</th>
      <th>TRM</th>
      <th>HRM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Parameters</td>
      <td>&lt;1,000</td>
      <td>27M</td>
    </tr>
    <tr>
      <td>Architecture</td>
      <td>Think-Act cycles</td>
      <td>Planner-Doer hierarchy</td>
    </tr>
    <tr>
      <td>Strength</td>
      <td>Maze solving</td>
      <td>Abstract reasoning</td>
    </tr>
    <tr>
      <td>Key insight</td>
      <td>Iteration</td>
      <td>Hierarchical decomposition</td>
    </tr>
  </tbody>
</table>

<p>Both use recursive reasoning, but HRM adds hierarchical structure for more complex tasks.</p>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Primary Language</strong></td>
      <td>TypeScript</td>
    </tr>
    <tr>
      <td><strong>Source Files</strong></td>
      <td>26 <code class="language-plaintext highlighter-rouge">.ts</code>/<code class="language-plaintext highlighter-rouge">.tsx</code>, 7 <code class="language-plaintext highlighter-rouge">.js</code></td>
    </tr>
    <tr>
      <td><strong>Estimated Size</strong></td>
      <td>~4 KLOC</td>
    </tr>
    <tr>
      <td><strong>Framework</strong></td>
      <td>React</td>
    </tr>
    <tr>
      <td><strong>Build System</strong></td>
      <td>npm / Create React App</td>
    </tr>
    <tr>
      <td><strong>Visualization</strong></td>
      <td>Canvas-based rendering</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to visualize neural reasoning processes, build interactive ML demos, or learn React with a real project.</p>

<p><strong>Complexity:</strong> Low-Moderate. Standard React/TypeScript project. No ML training code—this is a visualization tool for understanding the HRM architecture. Easy to extend with new visualizations.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>Plan, then act.</strong> Separating strategy from execution mirrors effective human thinking.</p>
  </li>
  <li>
    <p><strong>Hierarchy enables complexity.</strong> Multi-level reasoning handles problems flat networks can’t.</p>
  </li>
  <li>
    <p><strong>Architecture &gt; Scale</strong> for reasoning tasks.</p>
  </li>
  <li>
    <p><strong>ARC remains unsolved</strong> by brute-force scaling—clever architectures are the path forward.</p>
  </li>
</ol>

<h2 id="whats-next">What’s Next</h2>

<p>Part 4 explores <strong>Baby Dragon Hatchling (BDH)</strong>—a brain-inspired model with visible, interpretable activations.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2506.21734">HRM Paper</a></li>
  <li><a href="https://github.com/sapientinc/HRM">Original HRM Code</a></li>
  <li><a href="https://github.com/softwarewrighter/viz-hrm-ft">viz-hrm-ft Repository</a></li>
  <li><a href="https://arcprize.org/">ARC Prize</a></li>
  <li><a href="https://youtube.com/shorts/VIDEO_ID">Video: Planner + Doer = Genius</a></li>
</ul>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 3 of 6 in the Small Models, Big Brains series. <a href="/series/#small-models-big-brains">View all parts</a></td>
      <td><a href="/2026/02/03/small-models-part4-bdh/">Next: Part 4 →</a>*</td>
    </tr>
  </tbody>
</table>

          </div>





<div class="youtube-embed-container" id="yt-container-q_8qX-S9SxA">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-q_8qX-S9SxA"
      src="https://www.youtube.com/embed/q_8qX-S9SxA?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-q_8qX-S9SxA';
  const playerId = 'yt-player-q_8qX-S9SxA';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="41"><p class="post-meta">February 2, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">729 words</span> &bull; <span class="post-read-time">4 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Implementing Deepseek's Engram paper on conditional memory. Instead of recomputing common patterns through O(n^2) attention, Engram provides O(1) lookup for cached results. Our LoRA-based behavioral approximation achieves 58% loss reduction in 10 seconds.</div><h3>
          <a class="post-link" href="/2026/02/02/deepseek-papers-part2-engram/">
            Deepseek Papers (2/3): Engram - Conditional Memory for Transformers
          </a>
        </h3><nav class="toc" data-toc-id="deepseek-papers-part2-engram">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/site/post-marker-wide-stamp.png" class="post-marker" alt="" /></p>

<p>Deepseek publishes papers. I implement them. This paper tackles another fundamental transformer problem: redundant computation.</p>

<p>This post covers my implementation of <strong>Engram</strong> (Conditional Memory via Scalable Lookup)—running on both Apple Silicon and NVIDIA GPUs.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2601.07372">arXiv:2601.07372</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/engram-poc">engram-poc</a></td>
      </tr>
      <tr>
        <td><strong>Video 1</strong></td>
        <td><a href="https://youtube.com/shorts/aGoQHs6S1nk">Engram Part 1</a><br /><a href="https://youtube.com/shorts/aGoQHs6S1nk"><img src="https://img.youtube.com/vi/aGoQHs6S1nk/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>Video 2</strong></td>
        <td><a href="https://youtube.com/shorts/uvbfu0WKa3A">Engram Part 2</a><br /><a href="https://youtube.com/shorts/uvbfu0WKa3A"><img src="https://img.youtube.com/vi/uvbfu0WKa3A/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-problem-redundant-computation">The Problem: Redundant Computation</h2>

<p>LLMs waste compute reconstructing patterns they’ve seen before:</p>

<ul>
  <li>Style rules repeated across files</li>
  <li>Common code idioms re-derived each call</li>
  <li>Boilerplate knowledge injected repeatedly</li>
</ul>

<p>Attention computes everything from scratch every time. For recurring patterns, this is wasteful.</p>

<h2 id="the-engram-solution-o1-lookup">The Engram Solution: O(1) Lookup</h2>

<p>Engram introduces <strong>conditional memory as a complementary sparsity axis</strong>. Instead of recomputing common patterns through attention, look them up in O(1) time.</p>

<p>Think of it as a cache for the model’s learned patterns:</p>

<table>
  <thead>
    <tr>
      <th>Without Engram</th>
      <th>With Engram</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Recompute pattern every call</td>
      <td>Look up cached result</td>
    </tr>
    <tr>
      <td>O(n²) attention</td>
      <td>O(1) deterministic lookup</td>
    </tr>
    <tr>
      <td>Implicit knowledge</td>
      <td>Explicit, inspectable memory</td>
    </tr>
  </tbody>
</table>

<h2 id="the-poc-approach">The PoC Approach</h2>

<p>The full Engram paper describes in-model memory. The <a href="https://github.com/softwarewrighter/engram-poc">engram-poc</a> repo approximates the benefits through <strong>behavioral fine-tuning</strong>:</p>

<ol>
  <li><strong>Pattern Injection</strong>: Training data encodes lookup-like patterns</li>
  <li><strong>LoRA Adapters</strong>: Learn to recognize and consistently respond</li>
  <li><strong>Evaluation</strong>: Compare baseline vs tuned model</li>
</ol>

<h2 id="pattern-categories">Pattern Categories</h2>

<p>The PoC includes 131 patterns across 4 categories:</p>

<table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Examples</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Code Idioms</strong></td>
      <td><code class="language-plaintext highlighter-rouge">for i in range(</code> → <code class="language-plaintext highlighter-rouge">len(items)):</code></td>
    </tr>
    <tr>
      <td><strong>Factual Recall</strong></td>
      <td><code class="language-plaintext highlighter-rouge">HTTP status for 'Not Found'?</code> → <code class="language-plaintext highlighter-rouge">404</code></td>
    </tr>
    <tr>
      <td><strong>Format Transforms</strong></td>
      <td><code class="language-plaintext highlighter-rouge">snake_case: getUserName</code> → <code class="language-plaintext highlighter-rouge">get_user_name</code></td>
    </tr>
    <tr>
      <td><strong>Error Fixes</strong></td>
      <td><code class="language-plaintext highlighter-rouge">Fix: if x = 5:</code> → <code class="language-plaintext highlighter-rouge">if x == 5:</code></td>
    </tr>
  </tbody>
</table>

<h2 id="results">Results</h2>

<p>Training on SmolLM-135M-Instruct:</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Training Examples</td>
      <td>337</td>
    </tr>
    <tr>
      <td>Training Time</td>
      <td>~10 seconds (M-series Mac)</td>
    </tr>
    <tr>
      <td>Loss Reduction</td>
      <td><strong>58.2%</strong> (4.34 → 1.82)</td>
    </tr>
  </tbody>
</table>

<p>Behavioral change:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Prompt: Complete: for i in range(

Baseline:     "Here is a Python function that implements this approach..."
Engram-tuned: "len(items)):"
</code></pre></div></div>

<p>The tuned model produces direct, pattern-completing responses instead of verbose explanations.</p>

<h2 id="running-the-engram-demo">Running the Engram Demo</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/softwarewrighter/engram-poc
<span class="nb">cd </span>engram-poc

<span class="c"># Apple Silicon</span>
uv venv <span class="o">&amp;&amp;</span> <span class="nb">source</span> .venv/bin/activate
uv pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
./scripts/run_all.sh

<span class="c"># NVIDIA GPU (separate directory)</span>
<span class="nb">cd </span>unsloth-nvidia
uv venv <span class="o">&amp;&amp;</span> <span class="nb">source</span> .venv/bin/activate
uv pip <span class="nb">install </span>torch <span class="nt">--index-url</span> https://download.pytorch.org/whl/cu124
uv pip <span class="nb">install</span> <span class="s2">"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"</span>
./scripts/run_all.sh
</code></pre></div></div>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Primary Language</strong></td>
      <td>Python</td>
    </tr>
    <tr>
      <td><strong>Source Files</strong></td>
      <td>24 <code class="language-plaintext highlighter-rouge">.py</code>, 10 <code class="language-plaintext highlighter-rouge">.sh</code>, 6 <code class="language-plaintext highlighter-rouge">.yaml</code></td>
    </tr>
    <tr>
      <td><strong>Estimated Size</strong></td>
      <td>~3.0 KLOC</td>
    </tr>
    <tr>
      <td><strong>Frameworks</strong></td>
      <td>MLX-LM, Unsloth</td>
    </tr>
    <tr>
      <td><strong>Platforms</strong></td>
      <td>Apple Silicon, NVIDIA CUDA</td>
    </tr>
    <tr>
      <td><strong>Key Features</strong></td>
      <td>LoRA fine-tuning, pattern evaluation, interactive demo</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to experiment with LoRA fine-tuning, understand behavioral pattern injection, or compare MLX vs Unsloth workflows.</p>

<p><strong>Complexity:</strong> Moderate. Includes extensive documentation and video recording guides. Pattern data is human-readable YAML.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>Engram reduces redundant computation.</strong> O(1) lookup for recurring patterns beats recomputing through attention.</p>
  </li>
  <li>
    <p><strong>LoRA makes experimentation accessible.</strong> Fine-tune small models in seconds on a laptop.</p>
  </li>
  <li>
    <p><strong>Cross-platform matters.</strong> The repo runs on Apple Silicon and NVIDIA, with different tooling for each.</p>
  </li>
  <li>
    <p><strong>Deepseek publishes useful research.</strong> Their papers address real problems with practical solutions.</p>
  </li>
</ol>

<h2 id="whats-next">What’s Next</h2>

<p>Part 3 will cover <strong>Engram Revisited</strong>—what happened when we moved from behavioral emulation to real hash-based memory implementation. Spoiler: it works, but not everywhere.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2601.07372">Engram Paper (arXiv:2601.07372)</a></li>
  <li><a href="https://github.com/softwarewrighter/engram-poc">engram-poc Repository</a></li>
  <li><a href="https://youtube.com/shorts/aGoQHs6S1nk">Engram Video Part 1</a></li>
  <li><a href="https://youtube.com/shorts/uvbfu0WKa3A">Engram Video Part 2</a></li>
  <li><a href="/2026/02/01/deepseek-papers-part1-mhc/">Part 1: mHC</a></li>
</ul>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 2 of 3 in the Deepseek Papers series. <a href="/series/#deepseek-papers">View all parts</a></td>
      <td><a href="/2026/02/11/deepseek-papers-part3-engram-revisited/">Next: Part 3 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Implementing papers is the best way to understand them. Clone the repo and run the demo yourself.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-aGoQHs6S1nk">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-aGoQHs6S1nk"
      src="https://www.youtube.com/embed/aGoQHs6S1nk?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-aGoQHs6S1nk';
  const playerId = 'yt-player-aGoQHs6S1nk';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="42"><p class="post-meta">February 1, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">705 words</span> &bull; <span class="post-read-time">4 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">A 135M parameter model goes from 0% to 75% accuracy in 5 minutes. Using knowledge graph-guided training with rejection sampling, we teach multi-hop reasoning with scaffolding during training, then remove it at inference.</div><h3>
          <a class="post-link" href="/2026/02/01/multi-hop-reasoning/">
            Multi-Hop Reasoning (1/2): Training Wheels for Small LLMs
          </a>
        </h3><nav class="toc" data-toc-id="multi-hop-reasoning">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/site/post-marker-gimlet.png" class="post-marker" alt="" /></p>

<p>A tiny 135M parameter model goes from 0% to 75% accuracy in 5 minutes of training. The secret? Knowledge graph-guided training with rejection sampling.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2601.15160">KG-Guided RAG (arXiv)</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/multi-hop-reasoning">multi-hop-reasoning</a></td>
      </tr>
      <tr>
        <td><strong>ELI5</strong></td>
        <td><a href="https://github.com/softwarewrighter/multi-hop-reasoning/blob/main/documentation/eli5.md">eli5.md</a></td>
      </tr>
      <tr>
        <td><strong>Demo</strong></td>
        <td><a href="https://softwarewrighter.github.io/multi-hop-reasoning/">Live Demo</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://youtube.com/shorts/jCUomUai-9U">LLM with Training Wheels</a><br /><a href="https://youtube.com/shorts/jCUomUai-9U"><img src="https://img.youtube.com/vi/jCUomUai-9U/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>Part 2</strong></td>
        <td><a href="/2026/02/18/multi-hop-reasoning-distribution-trap/">The Distribution Trap</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-problem-multi-hop-reasoning">The Problem: Multi-Hop Reasoning</h2>

<p>LLMs struggle with questions requiring multiple reasoning steps. “What’s the fix for a crash caused by a corrupted config file on a system running outdated firmware?” requires connecting several facts:</p>

<ol>
  <li>Corrupted config → need config reset</li>
  <li>Outdated firmware → need firmware update</li>
  <li>Crash context → check dependencies between these fixes</li>
</ol>

<p>Standard fine-tuning teaches pattern matching. Multi-hop reasoning requires following logical chains.</p>

<h2 id="the-papers-approach">The Paper’s Approach</h2>

<div class="training-wheels-box" style="float: right; max-width: 240px; margin: 0 0 1em 1.5em; padding: 1em; background-color: #ffe4e6; border-radius: 8px; text-align: center;">
<img src="/assets/images/posts/multi-hop-reasoning/training-wheels.png" alt="Training wheels" style="max-width: 180px;" class="no-invert" />
<p style="margin: 0.5em 0 0 0; font-style: italic; font-size: 0.9em; color: #333;">Learn with training wheels, remove them after learning completes.</p>
</div>

<p><a href="https://arxiv.org/abs/2601.15160">Knowledge Graph-Guided RAG</a> from Princeton proposes using knowledge graphs during training to score reasoning quality—then removing the graph at inference.</p>

<p>The key insight: <strong>train with scaffolding, test without it</strong>.</p>

<h2 id="my-implementation">My Implementation</h2>

<p>The repo implements this for a software troubleshooting domain:</p>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Details</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Knowledge Graph</strong></td>
      <td>~200 entities, ~600 edges (symptoms, causes, fixes)</td>
    </tr>
    <tr>
      <td><strong>Training Data</strong></td>
      <td>MCQs with 1-3 hop paths</td>
    </tr>
    <tr>
      <td><strong>Eval Data</strong></td>
      <td>MCQs with 4-5 hop paths (harder)</td>
    </tr>
    <tr>
      <td><strong>Model</strong></td>
      <td>SmolLM-135M-Instruct</td>
    </tr>
    <tr>
      <td><strong>Framework</strong></td>
      <td>MLX (Apple Silicon native)</td>
    </tr>
  </tbody>
</table>

<h3 id="the-training-pipeline">The Training Pipeline</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────────────┐
│  1. SFT: Learn output format            │
│     TRACE: &lt;reasoning&gt;                  │
│     ANSWER: A|B|C|D                     │
├─────────────────────────────────────────┤
│  2. RSFT: Rejection Sampling FT         │
│     - Generate multiple answers         │
│     - Score with knowledge graph        │
│     - Keep only correct traces          │
│     - Train on winners                  │
└─────────────────────────────────────────┘
</code></pre></div></div>

<h3 id="the-reward-function">The Reward Function</h3>

<p>The knowledge graph scores outputs during training:</p>

<ul>
  <li><strong>R_corr</strong>: +1.0 correct answer, -2.0 incorrect</li>
  <li><strong>R_path</strong>: Entity coverage (did the trace mention relevant nodes?)</li>
  <li><strong>P_spam</strong>: -0.5 penalty for repeating entities (prevents gaming)</li>
</ul>

<p>At inference, the graph is removed. The model must reason from learned patterns.</p>

<h2 id="results">Results</h2>

<table>
  <thead>
    <tr>
      <th>Phase</th>
      <th>Accuracy</th>
      <th>Training Time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>0%</td>
      <td>-</td>
    </tr>
    <tr>
      <td>After SFT</td>
      <td>30%</td>
      <td>~2 min</td>
    </tr>
    <tr>
      <td>After RSFT</td>
      <td><strong>75%</strong></td>
      <td>~3 min</td>
    </tr>
  </tbody>
</table>

<p>The critical finding: <strong>distribution matching matters</strong>.</p>

<p>Training on easy examples (1-2 hops) hurt performance on hard eval (4-5 hops). Training on examples matching the eval distribution achieved 75%.</p>

<h2 id="running-it">Running It</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/softwarewrighter/multi-hop-reasoning
<span class="nb">cd </span>multi-hop-reasoning

<span class="c"># Setup (Apple Silicon)</span>
make setup-mlx

<span class="c"># Full pipeline</span>
make train
</code></pre></div></div>

<p>Results appear in ~5 minutes on an M-series Mac.</p>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Primary Language</strong></td>
      <td>Python</td>
    </tr>
    <tr>
      <td><strong>Source Files</strong></td>
      <td>12 <code class="language-plaintext highlighter-rouge">.py</code> files</td>
    </tr>
    <tr>
      <td><strong>Estimated Size</strong></td>
      <td>~1.5 KLOC</td>
    </tr>
    <tr>
      <td><strong>Framework</strong></td>
      <td>MLX, Transformers</td>
    </tr>
    <tr>
      <td><strong>Platform</strong></td>
      <td>Apple Silicon (MLX native)</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to understand knowledge graph-guided training, experiment with rejection sampling fine-tuning, or see how small models can learn reasoning patterns.</p>

<p><strong>Complexity:</strong> Moderate. Clean codebase with Make targets for each step. Requires understanding of fine-tuning concepts.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>Scaffolded training works.</strong> Use structured feedback during training, remove it at inference.</p>
  </li>
  <li>
    <p><strong>Distribution matching matters.</strong> Train on examples that match your eval distribution.</p>
  </li>
  <li>
    <p><strong>Small models can reason.</strong> 135M parameters is enough for 75% accuracy on 4-5 hop questions.</p>
  </li>
  <li>
    <p><strong>MLX makes iteration fast.</strong> Full pipeline runs in 5 minutes on a MacBook.</p>
  </li>
</ol>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2601.15160">Paper: Knowledge Graph-Guided RAG</a></li>
  <li><a href="https://github.com/softwarewrighter/multi-hop-reasoning">Repository: multi-hop-reasoning</a></li>
  <li><a href="https://softwarewrighter.github.io/multi-hop-reasoning/">Live Demo</a></li>
  <li><a href="https://youtube.com/shorts/jCUomUai-9U">Video: LLM with Training Wheels</a></li>
</ul>

<hr />

<p><em>Part 1 of 2 in the Multi-Hop Reasoning series. <a href="/series/#multi-hop-reasoning">View all parts</a></em></p>

<p><em>Knowledge graphs as training wheels—helping small models learn to reason, then letting go.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-jCUomUai-9U">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-jCUomUai-9U"
      src="https://www.youtube.com/embed/jCUomUai-9U?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-jCUomUai-9U';
  const playerId = 'yt-player-jCUomUai-9U';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="43"><p class="post-meta">February 1, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">791 words</span> &bull; <span class="post-read-time">4 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">AI in your pocket, no internet required. Pocket Eliza++ runs MobileLLM-350M on Android via llama.cpp and JNI, creating a privacy-first therapist chatbot. The 260MB quantized model achieves ~10 tokens/second on mid-range phones.</div><h3>
          <a class="post-link" href="/2026/02/01/small-models-part2-pocket-llm/">
            Small Models (2/6): AI in Your Pocket
          </a>
        </h3><nav class="toc" data-toc-id="small-models-part2-pocket-llm">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/site/post-marker-pot.png" class="post-marker" alt="" /></p>

<p>AI on your phone. All day. No internet required.</p>

<p>This is Part 2 of the <strong>Small Models, Big Brains</strong> series. Today we’re putting a language model in your pocket with <strong>Pocket Eliza++</strong>—a modern AI therapist that runs completely offline on Android.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2402.14905">MobileLLM (ICML 2024)</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/pocket-llm">pocket-llm</a></td>
      </tr>
      <tr>
        <td><strong>Runtime</strong></td>
        <td><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/fyEuH1BprLI">AI in Your Pocket</a><br /><a href="https://www.youtube.com/shorts/fyEuH1BprLI"><img src="https://img.youtube.com/vi/fyEuH1BprLI/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="why-offline-matters">Why Offline Matters</h2>

<table>
  <thead>
    <tr>
      <th>Benefit</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Privacy</strong></td>
      <td>Data never leaves your device</td>
    </tr>
    <tr>
      <td><strong>Speed</strong></td>
      <td>No network latency</td>
    </tr>
    <tr>
      <td><strong>Cost</strong></td>
      <td>No API fees</td>
    </tr>
    <tr>
      <td><strong>Offline</strong></td>
      <td>Works without internet</td>
    </tr>
    <tr>
      <td><strong>Battery</strong></td>
      <td>Efficient on-device inference</td>
    </tr>
  </tbody>
</table>

<p>Cloud AI is convenient, but sometimes you want a conversation that stays on your device.</p>

<h2 id="mobilellm-metas-edge-champion">MobileLLM: Meta’s Edge Champion</h2>

<p><a href="https://arxiv.org/abs/2402.14905">MobileLLM</a> is Meta’s sub-500M parameter model optimized specifically for on-device inference.</p>

<h3 id="architecture-optimizations">Architecture Optimizations</h3>

<table>
  <thead>
    <tr>
      <th>Technique</th>
      <th>Benefit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Deep-thin design</strong></td>
      <td>More layers, fewer parameters per layer</td>
    </tr>
    <tr>
      <td><strong>SwiGLU activation</strong></td>
      <td>Better performance than ReLU</td>
    </tr>
    <tr>
      <td><strong>Embedding sharing</strong></td>
      <td>Saves 30% of parameters</td>
    </tr>
    <tr>
      <td><strong>Grouped-query attention</strong></td>
      <td>Faster inference</td>
    </tr>
  </tbody>
</table>

<p>The result: a 260MB quantized model (Q4_K_M) that runs smoothly on phones.</p>

<h2 id="pocket-eliza">Pocket Eliza++</h2>

<p><img src="/assets/images/posts/small-models-part2/eliza-taking-notes.gif" alt="Eliza taking notes" style="float: right; max-width: 300px; margin: 0 0 1em 1.5em; border-radius: 8px;" /></p>

<p>The original ELIZA (1966) used pattern matching to simulate a Rogerian therapist. Pocket Eliza++ uses the same therapeutic approach but with actual language understanding.</p>

<h3 id="therapeutic-design">Therapeutic Design</h3>

<p>The system prompt instructs the model to:</p>

<ul>
  <li>Ask one short question at a time</li>
  <li>Never repeat questions</li>
  <li>Vary question types (feelings, motivations, specifics)</li>
  <li>Never give advice or explanations</li>
</ul>

<p>It’s a reflective listener, not a problem solver.</p>

<h2 id="technical-stack">Technical Stack</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────┐
│     Kotlin + Jetpack Compose    │  UI Layer
├─────────────────────────────────┤
│            JNI Bridge           │
├─────────────────────────────────┤
│           llama.cpp             │  Inference Engine
├─────────────────────────────────┤
│    MobileLLM-350M (Q4_K_M)      │  Model (260MB)
└─────────────────────────────────┘
</code></pre></div></div>

<ul>
  <li><strong>Model</strong>: MobileLLM-350M quantized to Q4_K_M (260MB GGUF)</li>
  <li><strong>Runtime</strong>: llama.cpp compiled for Android via NDK</li>
  <li><strong>Interface</strong>: Kotlin + Jetpack Compose</li>
  <li><strong>Bridge</strong>: JNI bindings connect Kotlin to native llama.cpp</li>
</ul>

<h2 id="building-the-app">Building the App</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Clone the repository</span>
git clone https://github.com/softwarewrighter/pocket-llm
<span class="nb">cd </span>pocket-llm/android-demo

<span class="c"># Clone llama.cpp into native source</span>
git clone https://github.com/ggerganov/llama.cpp.git <span class="se">\</span>
    app/src/main/cpp/llama.cpp

<span class="c"># Download the model (260MB)</span>
<span class="nb">mkdir</span> <span class="nt">-p</span> app/src/main/assets
curl <span class="nt">-L</span> <span class="nt">-o</span> app/src/main/assets/MobileLLM-376M-Q4_K_M.gguf <span class="se">\</span>
    <span class="s2">"https://huggingface.co/pjh64/MobileLLM-350M-GGUF/resolve/main/MobileLLM-376M-Q4_K_M.gguf"</span>

<span class="c"># Build and install</span>
./gradlew assembleDebug
adb <span class="nb">install</span> <span class="nt">-r</span> app/build/outputs/apk/debug/app-debug.apk
</code></pre></div></div>

<h3 id="build-requirements">Build Requirements</h3>

<table>
  <thead>
    <tr>
      <th>Requirement</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Target SDK</td>
      <td>35 (Android 15)</td>
    </tr>
    <tr>
      <td>Min SDK</td>
      <td>28 (Android 9.0)</td>
    </tr>
    <tr>
      <td>ABI</td>
      <td>arm64-v8a</td>
    </tr>
    <tr>
      <td>NDK</td>
      <td>CMake for native build</td>
    </tr>
    <tr>
      <td>Kotlin</td>
      <td>2.0.0</td>
    </tr>
  </tbody>
</table>

<h2 id="quick-cli-demo">Quick CLI Demo</h2>

<p>Don’t want to build the Android app? Test with Ollama:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
ollama pull smollm:360m
python3 eliza.py
</code></pre></div></div>

<h2 id="performance">Performance</h2>

<p>On a mid-range Android phone (Snapdragon 7 series):</p>

<ul>
  <li><strong>First token</strong>: ~500ms</li>
  <li><strong>Generation</strong>: ~10 tokens/second</li>
  <li><strong>Memory</strong>: ~400MB RAM</li>
  <li><strong>Battery</strong>: Minimal impact for short sessions</li>
</ul>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Languages</strong></td>
      <td>Kotlin (UI), Python (CLI), C++ (JNI)</td>
    </tr>
    <tr>
      <td><strong>Source Files</strong></td>
      <td>6 <code class="language-plaintext highlighter-rouge">.kt</code>, 4 <code class="language-plaintext highlighter-rouge">.py</code>, 2 <code class="language-plaintext highlighter-rouge">.cpp</code></td>
    </tr>
    <tr>
      <td><strong>Estimated Size</strong></td>
      <td>~1.3 KLOC</td>
    </tr>
    <tr>
      <td><strong>Android Target</strong></td>
      <td>SDK 28+ (Android 9.0)</td>
    </tr>
    <tr>
      <td><strong>Build System</strong></td>
      <td>Gradle + CMake (NDK)</td>
    </tr>
    <tr>
      <td><strong>Key Dependency</strong></td>
      <td>llama.cpp (vendored)</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to deploy LLMs on Android, learn JNI/NDK integration, or build privacy-focused mobile AI apps.</p>

<p><strong>Complexity:</strong> Moderate-High. Requires Android Studio, NDK setup, and understanding of JNI bridges. The llama.cpp integration is the tricky part; the Kotlin UI is straightforward Jetpack Compose.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>Sub-500M models are phone-ready.</strong> MobileLLM proves useful AI fits in your pocket.</p>
  </li>
  <li>
    <p><strong>llama.cpp is the universal runtime.</strong> Same engine runs on Mac, Linux, Windows, and Android.</p>
  </li>
  <li>
    <p><strong>Privacy doesn’t require sacrifice.</strong> Offline AI can still be conversational and helpful.</p>
  </li>
  <li>
    <p><strong>Quantization is essential.</strong> Q4_K_M brings 350M parameters down to 260MB with minimal quality loss.</p>
  </li>
</ol>

<h2 id="whats-next">What’s Next</h2>

<p>Part 3 explores the <strong>Hierarchical Reasoning Model (HRM)</strong>—a 27M parameter model that beats o3-mini on abstract reasoning.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2402.14905">MobileLLM Paper (ICML 2024)</a></li>
  <li><a href="https://github.com/softwarewrighter/pocket-llm">pocket-llm Repository</a></li>
  <li><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a></li>
  <li><a href="https://huggingface.co/pjh64/MobileLLM-350M-GGUF">MobileLLM GGUF on HuggingFace</a></li>
  <li><a href="https://youtube.com/shorts/VIDEO_ID">Video: AI in Your Pocket</a></li>
</ul>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 2 of 6 in the Small Models, Big Brains series. <a href="/series/#small-models-big-brains">View all parts</a></td>
      <td><a href="/2026/02/02/small-models-part3-hrm/">Next: Part 3 →</a>*</td>
    </tr>
  </tbody>
</table>

          </div>





<div class="youtube-embed-container" id="yt-container-fyEuH1BprLI">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-fyEuH1BprLI"
      src="https://www.youtube.com/embed/fyEuH1BprLI?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-fyEuH1BprLI';
  const playerId = 'yt-player-fyEuH1BprLI';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="44"><p class="post-meta">February 1, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">784 words</span> &bull; <span class="post-read-time">4 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Implementing Deepseek's mHC (Manifold-Constrained Hyper-Connections) paper. Using Sinkhorn-Knopp iteration to create doubly-stochastic matrices, mHC maintains training stability at 48 layers where standard hyper-connections explode. Cross-platform validation on Apple Silicon and NVIDIA.</div><h3>
          <a class="post-link" href="/2026/02/01/deepseek-papers-part1-mhc/">
            Deepseek Papers (1/3): mHC - Training Stability at Any Depth
          </a>
        </h3><nav class="toc" data-toc-id="deepseek-papers-part1-mhc">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/site/post-marker-stamp.png" class="post-marker" alt="" /></p>

<p>Deepseek publishes papers. I implement them. This paper tackles a fundamental transformer problem: training stability in deep networks.</p>

<p>This post covers my implementation of <strong>mHC</strong> (Manifold-Constrained Hyper-Connections)—running on both Apple Silicon and NVIDIA GPUs.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2512.24880">arXiv:2512.24880</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/mHC-poc">mHC-poc</a></td>
      </tr>
      <tr>
        <td><strong>ELI5</strong></td>
        <td><a href="https://github.com/softwarewrighter/mHC-poc/blob/main/docs/eli5-mHC.md">eli5-mHC.md</a></td>
      </tr>
      <tr>
        <td><strong>ELI4</strong></td>
        <td><a href="https://github.com/softwarewrighter/mHC-poc/blob/main/docs/eli4-mHC.md">eli4-mHC.md</a></td>
      </tr>
      <tr>
        <td><strong>Video 1</strong></td>
        <td><a href="https://youtube.com/shorts/fh21_zIK2ZE">mHC Demo</a><br /><a href="https://youtube.com/shorts/fh21_zIK2ZE"><img src="https://img.youtube.com/vi/fh21_zIK2ZE/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>Video 2</strong></td>
        <td><a href="https://www.youtube.com/watch?v=MYTXVYDtCEU">mHC Explained</a><br /><a href="https://www.youtube.com/watch?v=MYTXVYDtCEU"><img src="https://img.youtube.com/vi/MYTXVYDtCEU/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
      <tr>
        <td><strong>Video 3</strong></td>
        <td><a href="https://youtube.com/shorts/BOuBFn5e1gA">mHC Results</a><br /><a href="https://youtube.com/shorts/BOuBFn5e1gA"><img src="https://img.youtube.com/vi/BOuBFn5e1gA/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-problem-deep-networks-explode">The Problem: Deep Networks Explode</h2>

<p>Residual connections revolutionized deep learning. Skip connections let gradients flow through hundreds of layers. But there’s a catch.</p>

<p>Standard residual connections:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>output = layer(input) + input
</code></pre></div></div>

<p>This works, but the signal accumulates. With many layers, small amplifications compound into instability.</p>

<p><strong>Hyper-Connections (HC)</strong> tried to fix this by learning connection weights:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>output = α₁ × layer(input) + α₂ × input
</code></pre></div></div>

<p>Better expressiveness, but learned weights can still cause explosion. At 48 layers, HC becomes unstable.</p>

<h2 id="the-mhc-solution-doubly-stochastic-constraints">The mHC Solution: Doubly-Stochastic Constraints</h2>

<p>mHC constrains the connection weights using <strong>Sinkhorn-Knopp iteration</strong>—a mathematical technique that ensures weights form a doubly-stochastic matrix.</p>

<p>What does “doubly-stochastic” mean?</p>
<ul>
  <li>Each row sums to 1</li>
  <li>Each column sums to 1</li>
</ul>

<p>This bounds the total signal flow. No matter how deep the network, amplification stays controlled.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sinkhorn-Knopp iteration (simplified)
</span><span class="k">def</span> <span class="nf">make_doubly_stochastic</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">weights</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Column normalize
</span>        <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">weights</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Row normalize
</span>    <span class="k">return</span> <span class="n">weights</span>
</code></pre></div></div>

<h2 id="results-stability-at-depth">Results: Stability at Depth</h2>

<p>The <a href="https://github.com/softwarewrighter/mHC-poc">mHC-poc</a> repo stress-tests this with a depth sweep:</p>

<table>
  <thead>
    <tr>
      <th>Depth</th>
      <th>Baseline</th>
      <th>HC</th>
      <th>mHC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>12 layers</td>
      <td>Stable</td>
      <td>Stable</td>
      <td>Stable</td>
    </tr>
    <tr>
      <td>24 layers</td>
      <td>Struggling</td>
      <td>Stable</td>
      <td>Stable</td>
    </tr>
    <tr>
      <td>48 layers</td>
      <td>Oscillating</td>
      <td><strong>Explodes</strong></td>
      <td><strong>Stable</strong></td>
    </tr>
  </tbody>
</table>

<p>At 48 layers:</p>
<ul>
  <li><strong>HC</strong> gain proxy: 10²⁷ (catastrophic amplification)</li>
  <li><strong>mHC</strong> gain proxy: 10⁻⁰·⁶ (bounded, healthy)</li>
</ul>

<p>HC’s final loss at 48 layers: <strong>5.54</strong> (never learns)
mHC’s final loss at 48 layers: <strong>0.0002</strong> (perfect convergence)</p>

<h2 id="cross-platform-validation">Cross-Platform Validation</h2>

<p>The implementation runs on both Apple Silicon (MLX) and NVIDIA (PyTorch/CUDA):</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>MLX (Apple)</th>
      <th>CUDA (NVIDIA)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Gain Proxy (24L)</td>
      <td>-0.6</td>
      <td>-0.602</td>
    </tr>
    <tr>
      <td>Gradient Stability</td>
      <td>Stable</td>
      <td>Stable</td>
    </tr>
    <tr>
      <td>NaN Events</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>Identical results confirm the Sinkhorn-Knopp projection works correctly on both platforms.</p>

<h2 id="running-the-mhc-demo">Running the mHC Demo</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/softwarewrighter/mHC-poc
<span class="nb">cd </span>mHC-poc

<span class="c"># Apple Silicon (MLX)</span>
uv venv <span class="o">&amp;&amp;</span> <span class="nb">source</span> .venv/bin/activate
uv pip <span class="nb">install</span> <span class="nt">-r</span> mlx/requirements.txt
bash scripts/run_depth_sweep.sh

<span class="c"># NVIDIA (CUDA)</span>
<span class="nb">cd </span>cuda
uv venv <span class="o">&amp;&amp;</span> <span class="nb">source</span> .venv/bin/activate
uv pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
bash scripts/run_cuda_depth_sweep.sh
</code></pre></div></div>

<p>Results go to <code class="language-plaintext highlighter-rouge">runs/</code> with plots showing loss, gradient norms, and gain proxy across depths.</p>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Primary Language</strong></td>
      <td>Python</td>
    </tr>
    <tr>
      <td><strong>Source Files</strong></td>
      <td>29 <code class="language-plaintext highlighter-rouge">.py</code>, 3 <code class="language-plaintext highlighter-rouge">.sh</code>, 10 <code class="language-plaintext highlighter-rouge">.yaml</code></td>
    </tr>
    <tr>
      <td><strong>Estimated Size</strong></td>
      <td>~2.5 KLOC</td>
    </tr>
    <tr>
      <td><strong>Frameworks</strong></td>
      <td>MLX, PyTorch</td>
    </tr>
    <tr>
      <td><strong>Platforms</strong></td>
      <td>Apple Silicon, NVIDIA CUDA</td>
    </tr>
    <tr>
      <td><strong>Key Features</strong></td>
      <td>Depth sweep, cross-platform validation, visualization</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to understand mHC’s stability benefits, compare MLX vs PyTorch implementations, or experiment with residual connection variants.</p>

<p><strong>Complexity:</strong> Moderate. Well-documented with ELI5 explanations in <code class="language-plaintext highlighter-rouge">docs/</code>. Requires understanding of residual connections and matrix constraints.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>mHC solves deep network instability.</strong> Doubly-stochastic constraints bound signal amplification at any depth.</p>
  </li>
  <li>
    <p><strong>Cross-platform matters.</strong> The repo runs on Apple Silicon and NVIDIA, validated to produce identical results.</p>
  </li>
  <li>
    <p><strong>Deepseek publishes useful research.</strong> Their papers address real problems with practical solutions.</p>
  </li>
</ol>

<h2 id="whats-next">What’s Next</h2>

<p>Part 2 covers <strong>Engram</strong>—Deepseek’s approach to reducing redundant computation through conditional memory.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2512.24880">mHC Paper (arXiv:2512.24880)</a></li>
  <li><a href="https://github.com/softwarewrighter/mHC-poc">mHC-poc Repository</a></li>
  <li><a href="https://youtube.com/shorts/fh21_zIK2ZE">mHC Video Demo</a></li>
  <li><a href="https://www.youtube.com/watch?v=MYTXVYDtCEU">mHC Explained</a></li>
  <li><a href="https://youtube.com/shorts/BOuBFn5e1gA">mHC Results</a></li>
</ul>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 1 of 3 in the Deepseek Papers series. <a href="/series/#deepseek-papers">View all parts</a></td>
      <td><a href="/2026/02/02/deepseek-papers-part2-engram/">Next: Part 2 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Implementing papers is the best way to understand them. Clone the repo and run the demo yourself.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-fh21_zIK2ZE">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-fh21_zIK2ZE"
      src="https://www.youtube.com/embed/fh21_zIK2ZE?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-fh21_zIK2ZE';
  const playerId = 'yt-player-fh21_zIK2ZE';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="45"><p class="post-meta">January 31, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">729 words</span> &bull; <span class="post-read-time">4 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">The best LLMs score zero on hard mazes. A model with 976 parameters scores 85%. The Tiny Recursive Model uses think-act cycles with deep supervision, proving iteration beats scale for tasks requiring backtracking and spatial reasoning.</div><h3>
          <a class="post-link" href="/2026/01/31/small-models-part1-tiny-recursive-model/">
            Small Models (1/6): 976 Parameters Beat Billions
          </a>
        </h3><nav class="toc" data-toc-id="small-models-part1-tiny-recursive-model">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/site/post-marker-coffee-pot.png?ts=1769815140000" class="post-marker" alt="" /></p>

<p>The best large language models score zero on hard mazes. A model with under 1,000 parameters scores 85 percent.</p>

<p>This is Part 1 of the <strong>Small Models, Big Brains</strong> series, exploring how tiny models with clever architectures outperform massive ones on specific tasks.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2510.04871">Tiny Recursive Model</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/train-trm">train-trm</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/O6U06cGkKc4">976 parameters is more than billions?!</a><br /><a href="https://www.youtube.com/shorts/O6U06cGkKc4"><img src="https://img.youtube.com/vi/O6U06cGkKc4/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="why-llms-fail-at-mazes">Why LLMs Fail at Mazes</h2>

<p>Large language models generate one token at a time. They cannot backtrack. One wrong move and the entire solution fails.</p>

<p>Maze solving requires:</p>
<ul>
  <li>Exploring dead ends</li>
  <li>Backtracking when stuck</li>
  <li>Maintaining spatial awareness</li>
  <li>Planning multiple steps ahead</li>
</ul>

<p>Autoregressive generation is fundamentally incompatible with these requirements.</p>

<h2 id="meet-trm-the-tiny-recursive-model">Meet TRM: The Tiny Recursive Model</h2>

<p>The Tiny Recursive Model uses under 1,000 parameters. Instead of being bigger, it thinks in loops.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input → Think → Act → Think → Act → ... → Output
</code></pre></div></div>

<p>A simple two-layer network that iterates until the solution emerges.</p>

<h3 id="the-architecture">The Architecture</h3>

<p>TRM alternates between two phases:</p>

<table>
  <thead>
    <tr>
      <th>Phase</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Think</strong></td>
      <td>Update internal latent state by processing input, current answer, and previous state</td>
    </tr>
    <tr>
      <td><strong>Act</strong></td>
      <td>Update the answer based on the refined latent state</td>
    </tr>
  </tbody>
</table>

<p>This process repeats for multiple cycles, progressively improving the output.</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">TRMConfig</span> <span class="p">{</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">output_dim</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">l_layers</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>      <span class="c1">// Network depth</span>
    <span class="n">h_cycles</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>      <span class="c1">// Outer think-act cycles</span>
    <span class="n">l_cycles</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>      <span class="c1">// Inner think cycles</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="the-secret-deep-supervision">The Secret: Deep Supervision</h2>

<p>The key insight isn’t just recursion—it’s supervising every step, not just the final answer.</p>

<p>Traditional training:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input → [black box] → Final Output → Loss
</code></pre></div></div>

<p>TRM training:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input → Step 1 → Loss₁
      → Step 2 → Loss₂
      → Step 3 → Loss₃
      → ...
      → Final  → Loss_n
</code></pre></div></div>

<p>Every iteration gets feedback. The model learns to make progress at each step.</p>

<h2 id="results">Results</h2>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Maze Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GPT-4</td>
      <td>~0% on hard mazes</td>
    </tr>
    <tr>
      <td>Claude</td>
      <td>~0% on hard mazes</td>
    </tr>
    <tr>
      <td><strong>TRM (976 params)</strong></td>
      <td><strong>85%</strong></td>
    </tr>
  </tbody>
</table>

<p>Iteration beats scale.</p>

<h2 id="running-the-code">Running the Code</h2>

<p>The <a href="https://github.com/softwarewrighter/train-trm">train-trm</a> repo provides a complete Rust implementation:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Clone and build</span>
git clone https://github.com/softwarewrighter/train-trm
<span class="nb">cd </span>train-trm
./scripts/build.sh <span class="nt">--release</span>

<span class="c"># Train a model</span>
./scripts/train.sh <span class="nt">--epochs</span> 1000 <span class="nt">--lr</span> 0.01

<span class="c"># Evaluate</span>
./scripts/eval.sh

<span class="c"># Or launch the web UI</span>
cargo <span class="nb">install</span> <span class="nt">--locked</span> trunk
./scripts/web-serve.sh
</code></pre></div></div>

<p>The web UI includes interactive maze visualization with solution paths and real-time training charts.</p>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Primary Language</strong></td>
      <td>Rust</td>
    </tr>
    <tr>
      <td><strong>Source Files</strong></td>
      <td>21 <code class="language-plaintext highlighter-rouge">.rs</code> files</td>
    </tr>
    <tr>
      <td><strong>Estimated Size</strong></td>
      <td>~2.5 KLOC</td>
    </tr>
    <tr>
      <td><strong>Also Includes</strong></td>
      <td>HTML (web UI), Shell scripts</td>
    </tr>
    <tr>
      <td><strong>Build System</strong></td>
      <td>Cargo + Trunk (WASM)</td>
    </tr>
    <tr>
      <td><strong>Dependencies</strong></td>
      <td>ndarray, serde, clap, wasm-bindgen</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to learn Rust ML from scratch, experiment with recursive architectures, or need a web-based training visualization.</p>

<p><strong>Complexity:</strong> Moderate. Clean Rust code with good documentation. The neural network is implemented from scratch (no PyTorch/TensorFlow), making it educational but requiring Rust familiarity.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>Parameter count isn’t everything.</strong> Architecture and training strategy matter more for certain tasks.</p>
  </li>
  <li>
    <p><strong>Recursion enables backtracking.</strong> By iterating, TRM can explore and refine solutions.</p>
  </li>
  <li>
    <p><strong>Deep supervision accelerates learning.</strong> Feedback at every step, not just the end.</p>
  </li>
  <li>
    <p><strong>Task-specific models excel.</strong> TRM isn’t a general-purpose LLM—it’s optimized for maze-like reasoning.</p>
  </li>
</ol>

<h2 id="whats-next">What’s Next</h2>

<p>Part 2 explores <strong>MobileLLM</strong> and running AI completely offline on your Android phone.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2510.04871">TRM Paper</a></li>
  <li><a href="https://github.com/softwarewrighter/train-trm">train-trm Repository</a></li>
  <li><a href="https://www.youtube.com/shorts/O6U06cGkKc4">Video: 976 parameters is more than billions?!</a></li>
</ul>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 1 of 6 in the Small Models, Big Brains series. <a href="/series/#small-models-big-brains">View all parts</a></td>
      <td><a href="/2026/02/01/small-models-part2-pocket-llm/">Next: Part 2 →</a>*</td>
    </tr>
  </tbody>
</table>

          </div>





<div class="youtube-embed-container" id="yt-container-O6U06cGkKc4">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-O6U06cGkKc4"
      src="https://www.youtube.com/embed/O6U06cGkKc4?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-O6U06cGkKc4';
  const playerId = 'yt-player-O6U06cGkKc4';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="46"><p class="post-meta">January 30, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">1013 words</span> &bull; <span class="post-read-time">6 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Introduction to Software Wrighter Lab: a blog, YouTube channel, and GitHub repos exploring AI coding agents, systems programming in Rust, and practical ML implementations. Written by Mike Wright, a software engineer with 40+ years of experience from mainframes to modern AI.</div><h3>
          <a class="post-link" href="/2026/01/30/welcome-to-software-wrighter-lab/">
            Welcome to Software Wrighter Lab
          </a>
        </h3><div class="post-content">
          <p><img src="/assets/images/posts/welcome/avatar.png" class="post-marker no-invert" alt="" /></p>

<p>Welcome to Software Wrighter Lab—a blog, <a href="https://www.youtube.com/@SoftwareWrighter">YouTube channel</a>, <a href="https://discord.com/invite/Ctzk5uHggZ">Discord server</a>, and <a href="https://github.com/softwarewrighter">GitHub repos</a> for exploring the intersection of AI coding agents, systems programming, and practical machine learning.</p>

<p>I’m Mike Wright, a software engineer with over four decades of experience, currently focused on AI-assisted development with Rust and WebAssembly.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Quick Links</th>
        <th> </th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>YouTube</strong></td>
        <td><a href="https://www.youtube.com/@SoftwareWrighter">@SoftwareWrighter</a></td>
      </tr>
      <tr>
        <td><strong>GitHub</strong></td>
        <td><a href="https://github.com/softwarewrighter">softwarewrighter</a></td>
      </tr>
      <tr>
        <td><strong>Discord</strong></td>
        <td><a href="https://discord.com/invite/Ctzk5uHggZ">SW Lab</a></td>
      </tr>
    </tbody>
  </table>

</div>

<p><strong>Contents:</strong></p>
<ul>
  <li><a href="#about-me">About Me</a></li>
  <li><a href="#programming-languages">Programming Languages</a></li>
  <li><a href="#what-this-blog-covers">What This Blog Covers</a></li>
  <li><a href="#why-software-wrighter">Why “Software Wrighter”?</a></li>
  <li><a href="#what-to-expect">What to Expect</a></li>
  <li><a href="#current-projects">Current Projects</a></li>
  <li><a href="#technology-stack">Technology Stack</a></li>
  <li><a href="#get-involved">Get Involved</a></li>
  <li><a href="#whats-next">What’s Next</a></li>
</ul>

<h2 id="about-me">About Me</h2>

<p>I’ve been writing code professionally for over 35 years—an Emacs user since 1989, still going strong.</p>

<p>My background spans mainframes to startups:</p>

<ul>
  <li><strong>IBM Data Processing Division</strong> - MVS Dynamic Reconfiguration and Standalone Dump (SADUMP)</li>
  <li><strong>IBM T.J. Watson Research</strong> - Advisory Programmer on MVS Batch Pipes, Automatic Restart Manager, Java Record I/O, and IMS Data Sharing</li>
  <li><strong>Forte Software / Sun Microsystems</strong> - Senior Programmer on Forte 4GL/Conductor/Fusion, Open Enterprise Service Bus, and Glassfish</li>
  <li><strong>Startups</strong> - Individual contributor and management roles including LogiCoy (Open ESB), Likestream (Facebook Clojure App), Guidewire (Platform), Illumio (Network Security Web UI), and Signifyd (Gradle/Docker performance tuning)</li>
</ul>

<p>Areas I’ve worked in: mainframe O/S development, EAI/B2B middleware, platform engineering, build/release engineering, and embedded programming.</p>

<h2 id="programming-languages">Programming Languages</h2>

<p>Over the years, I’ve written production code in:</p>

<table>
  <thead>
    <tr>
      <th>Era</th>
      <th>Languages</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Mainframe</strong></td>
      <td>APL, Assembler (S/370, S/390), IBM PL/S, PL/AS, PL/X, CMS/TSO Pipelines</td>
    </tr>
    <tr>
      <td><strong>Systems</strong></td>
      <td>C, C++</td>
    </tr>
    <tr>
      <td><strong>Enterprise</strong></td>
      <td>Java, Forte 4GL, Guidewire Gosu, Groovy</td>
    </tr>
    <tr>
      <td><strong>Web/Modern</strong></td>
      <td>JavaScript, TypeScript, Go, Clojure, ClojureScript</td>
    </tr>
    <tr>
      <td><strong>Current</strong></td>
      <td>Elisp, JavaScript, Kotlin, Python, Rust, WebAssembly</td>
    </tr>
  </tbody>
</table>

<p>Each language taught me something different about how to think about problems. APL taught me array thinking. Assembler taught me what the machine is actually doing. CMS/TSO Pipelines taught me dataflow composition (an area I plan to revisit in Throwback Thursday posts). Lisp (via Clojure) taught me functional composition. Rust is teaching me ownership and fearless concurrency.</p>

<p>I’m a lifelong learner. When Rust emerged as a modern systems language, I dove in. When AI coding agents became capable enough to be genuine collaborators, I started exploring how they change the way we build software.</p>

<p>This blog and the accompanying YouTube channel document that exploration.</p>

<h2 id="what-this-blog-covers">What This Blog Covers</h2>

<p>Software Wrighter Lab focuses on three main areas:</p>

<h3 id="1-ai-coding-agents">1. AI Coding Agents</h3>

<p>How do tools like Claude Code, Cursor, and other AI assistants actually perform on real projects? I build the same applications with different agents to compare their strengths and weaknesses.</p>

<ul>
  <li><strong>Vibe coding comparisons</strong> (Claude vs GLM, different models)</li>
  <li><strong>Practical workflows</strong> (parallel coding with git worktrees, hooks, custom scripts)</li>
  <li><strong>Tool development</strong> (guardian-cli, proact, ralphy)</li>
</ul>

<h3 id="2-machine-learning-research-implementation">2. Machine Learning Research Implementation</h3>

<p>When interesting ML papers come out, I implement them to understand how they work. The goal isn’t to compete with research labs—it’s to learn by building.</p>

<p>Recent implementations include:</p>

<ul>
  <li><strong>Tiny Recursive Model (TRM)</strong> - Under 1,000 parameters solving mazes</li>
  <li><strong>Hierarchical Reasoning Model (HRM)</strong> - Planner-Doer architecture for abstract reasoning</li>
  <li><strong>MobileLLM</strong> - Running LLMs offline on Android</li>
  <li><strong>Deepseek papers</strong> (mHC, Engram) - Novel architectures for efficient inference</li>
  <li><strong>MIT’s Recursive Language Model</strong> - Implemented in Rust with WASM</li>
</ul>

<h3 id="3-rust-webassembly-and-practical-tools">3. Rust, WebAssembly, and Practical Tools</h3>

<p>Rust is my language of choice for new projects. Combined with WebAssembly, it enables building tools that run anywhere—CLI, browser, or embedded.</p>

<p>Topics include:</p>

<ul>
  <li><strong>Rust/Yew/WASM</strong> web applications</li>
  <li><strong>Visualization</strong> (Three.js, d3.js, pure CSS approaches)</li>
  <li><strong>Video production tools</strong> (TTS, lip sync, explainer generation)</li>
  <li><strong>Developer utilities</strong> (installation scripts, repo assistants, modularizers)</li>
</ul>

<h2 id="why-software-wrighter">Why “Software Wrighter”?</h2>

<p>A “wright” is a craftsperson—someone who builds things. A wheelwright builds wheels. A playwright builds plays.</p>

<p>A <strong>Software Wrighter</strong> builds software, with attention to craft.</p>

<p>The name reflects my belief that good software comes from treating programming as a craft: learning continuously, choosing tools deliberately, and building things that work well and last.</p>

<h2 id="what-to-expect">What to Expect</h2>

<p>Posts on this blog will typically include:</p>

<ul>
  <li><strong>Links to papers, repos, and videos</strong> (above the fold)</li>
  <li><strong>Implementation details</strong> (language, LOC, complexity assessment)</li>
  <li><strong>Working code</strong> you can clone and run</li>
  <li><strong>Honest assessments</strong> of what works and what doesn’t</li>
</ul>

<p>I’m not trying to sell you anything. This is a lab notebook—a record of experiments, some successful, some not.</p>

<h2 id="current-projects">Current Projects</h2>

<p>As of February 2026, I’m actively working on:</p>

<table>
  <thead>
    <tr>
      <th>Project</th>
      <th>Description</th>
      <th>Status</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Small Models, Big Brains</td>
      <td>6-part series on efficient LLMs</td>
      <td>Publishing</td>
    </tr>
    <tr>
      <td>Deepseek papers</td>
      <td>mHC and Engram implementations</td>
      <td>In progress</td>
    </tr>
    <tr>
      <td>Explainer pipeline</td>
      <td>AI-generated video production</td>
      <td>Ongoing</td>
    </tr>
    <tr>
      <td>RLM implementations</td>
      <td>Recursive Language Models in Rust</td>
      <td>Complete</td>
    </tr>
  </tbody>
</table>

<h2 id="technology-stack">Technology Stack</h2>

<p>Most of my current work uses:</p>

<table>
  <thead>
    <tr>
      <th>Layer</th>
      <th>Technology</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Systems</strong></td>
      <td>Rust</td>
    </tr>
    <tr>
      <td><strong>Web</strong></td>
      <td>Yew, WASM, TypeScript</td>
    </tr>
    <tr>
      <td><strong>ML</strong></td>
      <td>Python, PyTorch, HuggingFace</td>
    </tr>
    <tr>
      <td><strong>AI Agents</strong></td>
      <td>Claude Code, Cursor</td>
    </tr>
    <tr>
      <td><strong>Video</strong></td>
      <td>OBS, FFmpeg, TTS tools</td>
    </tr>
  </tbody>
</table>

<h2 id="get-involved">Get Involved</h2>

<p>If any of this resonates with you:</p>

<ul>
  <li><strong>Subscribe</strong> to the <a href="https://www.youtube.com/@SoftwareWrighter">YouTube channel</a> for video content</li>
  <li><strong>Star repos</strong> on <a href="https://github.com/softwarewrighter">GitHub</a> that interest you</li>
  <li><strong>Join</strong> the <a href="https://discord.com/invite/Ctzk5uHggZ">Discord server</a> to discuss</li>
</ul>

<p>I’m always interested in discussing these topics with other engineers exploring similar territory.</p>

<h2 id="whats-next">What’s Next</h2>

<p>The first content series, <strong>Small Models, Big Brains</strong>, starts tomorrow. It’s a 6-part deep dive into small language models that outperform much larger ones on specific tasks:</p>

<ol>
  <li>TRM: 976 parameters beating GPT-4 on mazes</li>
  <li>MobileLLM: AI running offline on your phone</li>
  <li>HRM: 27M parameters beating o3-mini on abstract reasoning</li>
  <li>BDH: A language model with visible, interpretable activations</li>
  <li>Billion-parameter models: The efficiency sweet spot</li>
  <li>The 2-3B efficient frontier: Phi-2, Gemma, SmolLM</li>
</ol>

<p>Each post maps to a YouTube video, a GitHub repo, and working code you can run yourself.</p>

<p>Thanks for reading. Let’s build something interesting.</p>

<hr />

<p><em>Mike Wright</em>
<em>Software Wrighter LLC</em>
<em>San Francisco Bay Area</em></p>

          </div><img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li><li class="post-item" data-index="47"><p class="post-meta">January 29, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">1160 words</span> &bull; <span class="post-read-time">6 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">My first program was a horse race game in APL on an IBM mainframe in 1972. This Throwback Thursday post recreates it using GNU APL, exploring array-oriented programming and the ideas that shaped languages from J to NumPy.</div><h3>
          <a class="post-link" href="/2026/01/29/tbt-apl-horse-race/">
            TBT (1/?): My First Program Was a Horse Race
          </a>
        </h3><nav class="toc" data-toc-id="tbt-apl-horse-race">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content">
          <p><img src="/assets/images/site/post-marker-apl.png" class="post-marker no-invert" alt="" /></p>

<p>My first program was a horse race. Written in APL. On a mainframe. In 1972.</p>

<p>This is the first <strong>Throwback Thursday</strong> post—a series where I revisit the technologies, languages, and ideas that shaped how I think about programming.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/sw-comp-history/apl-horse-race">apl-horse-race</a></td>
      </tr>
      <tr>
        <td><strong>Demo</strong></td>
        <td><a href="https://sw-comp-history.github.io/apl-horse-race/">Live Demo</a></td>
      </tr>
      <tr>
        <td><strong>GNU APL</strong></td>
        <td><a href="https://www.gnu.org/software/apl/">gnu.org/software/apl</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/watch?v=_MkFUwLpnUM">Greek Code, No Lowercase #TBT</a><br /><a href="https://www.youtube.com/watch?v=_MkFUwLpnUM"><img src="https://img.youtube.com/vi/_MkFUwLpnUM/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="apl-a-programming-language">APL: A Programming Language</h2>

<p>APL was created by Kenneth Iverson at IBM in the 1960s. The name literally means “A Programming Language”—Iverson was a mathematician who designed it as a notation for describing algorithms before it became an actual programming language.</p>

<p>What made APL special:</p>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Array-oriented</strong></td>
      <td>Operations work on entire arrays, not single values</td>
    </tr>
    <tr>
      <td><strong>Symbolic notation</strong></td>
      <td>Greek letters and mathematical symbols as operators</td>
    </tr>
    <tr>
      <td><strong>Interactive</strong></td>
      <td>REPL-style development decades before it was common</td>
    </tr>
    <tr>
      <td><strong>Terse</strong></td>
      <td>Complex operations in a few characters</td>
    </tr>
  </tbody>
</table>

<p>APL programs look like nothing else:</p>

<pre><code class="language-apl">POS←POS+?5⍴3
</code></pre>

<p>This single line adds random values (1-3) to all five horse positions simultaneously. No loops. No iteration. The operation just <em>happens</em> across the entire array.</p>

<h2 id="the-ibm-2741-experience">The IBM 2741 Experience</h2>

<p>In 1972, APL\360 ran on IBM mainframes. You accessed it through terminals like the IBM 2741—essentially a modified Selectric typewriter with a special APL typeball.</p>

<figure class="inline-right">
  <img src="/assets/images/posts/2026-01-29-apl/selectric-typeball.png" alt="IBM Selectric APL typeball" width="200" />
  <figcaption>APL typeball for IBM Selectric</figcaption>
</figure>

<p>The typeball had all the APL glyphs: <code class="language-plaintext highlighter-rouge">⍴ ⍳ ∇ ⎕ ← ⌈ ⌊ ⍵ ⍺ ∘ ⊃ ⊂</code> and dozens more. You physically typed these symbols. The keyboard layout was completely different from anything you’d seen before.</p>

<p>When you made an error, there was no backspace in the modern sense. You’d overstrike characters or start the line over. Programs were stored in workspaces, saved to tape or disk.</p>

<p>The terminal printed on paper. Every interaction left a physical record.</p>

<h2 id="the-horse-race-program">The Horse Race Program</h2>

<p>Horse race simulations were popular APL demonstrations. They showed off several things:</p>

<ol>
  <li><strong>Random number generation</strong> (<code class="language-plaintext highlighter-rouge">?</code> roll operator)</li>
  <li><strong>Array operations</strong> (updating all positions at once)</li>
  <li><strong>Character graphics</strong> (crude but effective visualization)</li>
  <li><strong>Interactive output</strong> (watching the race unfold)</li>
</ol>

<p>Here’s the verbose version from the repo:</p>

<pre><code class="language-apl">∇ RACE;HORSES;POS;FINISH;ROUND;_
  HORSES←'LUCKY  ' 'THUNDER' 'SHADOW ' 'COMET  ' 'BLAZE  '
  POS←5⍴0
  FINISH←15
  ROUND←0
  ⎕←'══════════════════════════════════════════'
  ⎕←'           THE RACE IS ON!'
  ⎕←'══════════════════════════════════════════'
LOOP:ROUND←ROUND+1
  ⎕←'--- ROUND ',(⍕ROUND),' ---'
  POS←POS+?5⍴3
  SHOWHORSES
  →DONE×⍳∨/POS≥FINISH
  →LOOP
DONE:⎕←'WINNER: ',((⊃(POS=⌈/POS)/⍳5)⊃HORSES),'!'
∇
</code></pre>

<h3 id="key-apl-idioms">Key APL Idioms</h3>

<p><strong>Array creation:</strong></p>
<pre><code class="language-apl">POS←5⍴0    ⍝ Create array of 5 zeros
</code></pre>

<p>The <code class="language-plaintext highlighter-rouge">⍴</code> (rho) operator reshapes. <code class="language-plaintext highlighter-rouge">5⍴0</code> means “reshape 0 into a 5-element array.”</p>

<p><strong>Random numbers:</strong></p>
<pre><code class="language-apl">?5⍴3       ⍝ Roll 5 dice, each 1-3
</code></pre>

<p>The <code class="language-plaintext highlighter-rouge">?</code> operator is “roll”—like rolling dice. <code class="language-plaintext highlighter-rouge">?5⍴3</code> rolls five 3-sided dice.</p>

<p><strong>Finding the winner:</strong></p>
<pre><code class="language-apl">(⊃(POS=⌈/POS)/⍳5)⊃HORSES
</code></pre>

<p>This reads right-to-left:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">⌈/POS</code> — maximum of all positions</li>
  <li><code class="language-plaintext highlighter-rouge">POS=⌈/POS</code> — boolean mask: which horses are at max?</li>
  <li><code class="language-plaintext highlighter-rouge">/⍳5</code> — compress: keep only those indices</li>
  <li><code class="language-plaintext highlighter-rouge">⊃</code> — take the first one</li>
  <li><code class="language-plaintext highlighter-rouge">⊃HORSES</code> — select that horse’s name</li>
</ul>

<p>One line. No loops. Pure array thinking.</p>

<h2 id="the-idiomatic-version">The Idiomatic Version</h2>

<p>APL programmers pride themselves on terseness. The idiomatic version does the same thing in fewer characters:</p>

<pre><code class="language-apl">HORSES←'LUCKY  ' 'THUNDER' 'SHADOW ' 'COMET  ' 'BLAZE  '

∇ SHOW;I
  I←1
N:⎕←(I⊃HORSES),'│',((I⊃POS)⍴'░'),'▓'
  I←I+1
  →N×⍳I≤5
∇

∇ RACE;POS;_
  POS←5⍴0
  ⎕←'THE RACE IS ON!'
L:_←⎕DL 0.3
  POS←POS+?5⍴3
  SHOW
  ⎕←''
  →L×⍳~∨/POS≥15
  ⎕←'WINNER: ',(⊃(POS=⌈/POS)/⍳5)⊃HORSES
∇
</code></pre>

<p>The entire program fits on a single screen. This was the APL aesthetic: powerful ideas expressed concisely.</p>

<h2 id="running-it-today">Running It Today</h2>

<p>GNU APL implements ISO 13751 (Extended APL) and runs on modern systems:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># macOS</span>
brew <span class="nb">install </span>gnu-apl

<span class="c"># Arch Linux</span>
yay <span class="nt">-S</span> gnu-apl

<span class="c"># Run the race</span>
git clone https://github.com/sw-comp-history/apl-horse-race
<span class="nb">cd </span>apl-horse-race
apl <span class="nt">-f</span> src/race.apl
</code></pre></div></div>

<p>Sample output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>══════════════════════════════════════════
           THE RACE IS ON!
══════════════════════════════════════════

--- ROUND 1 ---
LUCKY   │▓▓▓◄
THUNDER │▓▓◄
SHADOW  │▓◄
COMET   │▓▓▓◄
BLAZE   │▓▓◄
</code></pre></div></div>

<p>The horses advance randomly until one crosses the finish line.</p>

<h2 id="what-apl-taught-me">What APL Taught Me</h2>

<p>APL shaped how I think about programming in ways that persist today:</p>

<p><strong>1. Think in arrays, not loops.</strong></p>

<p>When I see a problem, I ask: can this be expressed as an operation on a whole collection? Languages like NumPy, R, and Julia carry this forward.</p>

<p><strong>2. Notation matters.</strong></p>

<p>Good notation can make complex ideas simple. Bad notation obscures them. APL’s symbols were controversial, but they made array operations <em>visible</em> in ways that verbose syntax doesn’t.</p>

<p><strong>3. The REPL is powerful.</strong></p>

<p>Interactive development—type an expression, see the result immediately—was central to APL decades before it became fashionable again with Jupyter notebooks and modern REPLs.</p>

<p><strong>4. Terseness has value.</strong></p>

<p>Not obfuscation for its own sake, but the ability to see an entire algorithm at once. When your program fits on one screen, you can reason about the whole thing.</p>

<h2 id="apls-legacy">APL’s Legacy</h2>

<p>APL influenced many languages:</p>

<table>
  <thead>
    <tr>
      <th>Language</th>
      <th>Year</th>
      <th>APL Influence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>J</strong></td>
      <td>1990</td>
      <td>Iverson’s ASCII-only redesign</td>
    </tr>
    <tr>
      <td><strong>K/Q</strong></td>
      <td>1993</td>
      <td>Powers financial systems at Kx</td>
    </tr>
    <tr>
      <td><strong>A+</strong></td>
      <td>1988</td>
      <td>Morgan Stanley’s open-source APL</td>
    </tr>
    <tr>
      <td><strong>BQN</strong></td>
      <td>2020</td>
      <td>Modern APL with cleaner semantics</td>
    </tr>
    <tr>
      <td><strong>NumPy</strong></td>
      <td>2006</td>
      <td>Array operations in Python</td>
    </tr>
    <tr>
      <td><strong>R</strong></td>
      <td>1993</td>
      <td>Vector operations for statistics</td>
    </tr>
  </tbody>
</table>

<p>The ideas live on, even if the glyphs don’t.</p>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Primary Language</strong></td>
      <td>APL</td>
    </tr>
    <tr>
      <td><strong>Source Files</strong></td>
      <td>2 <code class="language-plaintext highlighter-rouge">.apl</code> files</td>
    </tr>
    <tr>
      <td><strong>Lines of Code</strong></td>
      <td>~50 lines total</td>
    </tr>
    <tr>
      <td><strong>Runtime</strong></td>
      <td>GNU APL</td>
    </tr>
    <tr>
      <td><strong>Also Includes</strong></td>
      <td>Documentation, PNG samples for Unicode issues</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to understand array programming origins, learn basic APL, or experience what programming felt like in the 1970s.</p>

<p><strong>Complexity:</strong> Low. The program is intentionally simple—a teaching example, not production code. The repo includes extensive documentation explaining every line.</p>

<h2 id="why-throwback-thursday">Why Throwback Thursday?</h2>

<p>Programming didn’t start with Python and JavaScript. Every abstraction we use today was invented by someone solving a real problem.</p>

<p>TBT posts will explore:</p>
<ul>
  <li>Languages that shaped my thinking (APL, Lisp, Forth)</li>
  <li>Technologies that were ahead of their time (CMS/TSO Pipelines, dataflow)</li>
  <li>Ideas worth revisiting with modern tools</li>
</ul>

<p>Understanding where we came from helps us see where we’re going.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://github.com/sw-comp-history/apl-horse-race">apl-horse-race Repository</a></li>
  <li><a href="https://www.gnu.org/software/apl/">GNU APL</a></li>
  <li><a href="https://aplwiki.com/">APL Wiki</a></li>
  <li><a href="https://www.jsoftware.com/papers/tot.htm">Iverson’s Turing Award Lecture: “Notation as a Tool of Thought”</a></li>
  <li><a href="https://www.youtube.com/watch?v=_MkFUwLpnUM">Video: Greek Code, No Lowercase #TBT</a></li>
</ul>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 1 of the Throwback Thursday series. <a href="/series/#throwback-thursday">View all parts</a></td>
      <td><a href="/2026/02/05/tbt-pipelines-os390/">Next: Part 2 →</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Have your own “first program” story? Find me on <a href="https://www.youtube.com/@SoftwareWrighter">YouTube @SoftwareWrighter</a>.</em></p>

          </div>





<div class="youtube-embed-container" id="yt-container-_MkFUwLpnUM">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-_MkFUwLpnUM"
      src="https://www.youtube.com/embed/_MkFUwLpnUM?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-_MkFUwLpnUM';
  const playerId = 'yt-player-_MkFUwLpnUM';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt="">
      </li></ul>

    <div class="pagination-controls" id="pagination-bottom">
      <div>
        <label for="per-page-select-bottom">Posts per page:</label>
        <select id="per-page-select-bottom">
          <option value="5">5</option>
          <option value="10" selected>10</option>
          <option value="25">25</option>
          <option value="50">50</option>
          <option value="100">100</option>
        </select>
      </div>
      <div class="page-numbers" id="page-numbers-bottom"></div>
      <div class="page-info" id="page-info-bottom"></div>
    </div><p class="rss-subscribe">subscribe <a href="/feed.xml">via RSS</a></p></div>

<script>
(function() {
  const posts = document.querySelectorAll('.post-item');
  const totalPosts = posts.length;
  let perPage = 10;
  let currentPage = 1;

  const perPageSelects = [
    document.getElementById('per-page-select'),
    document.getElementById('per-page-select-bottom')
  ];

  function getTotalPages() {
    return Math.ceil(totalPosts / perPage);
  }

  function showPage(page) {
    currentPage = page;
    const start = (page - 1) * perPage;
    const end = start + perPage;

    posts.forEach((post, i) => {
      if (i >= start && i < end) {
        post.classList.add('visible');
      } else {
        post.classList.remove('visible');
      }
    });

    // Hide separator on last visible post
    const visiblePosts = document.querySelectorAll('.post-item.visible');
    visiblePosts.forEach((post, i) => {
      const sep = post.querySelector('.post-separator');
      if (sep) {
        sep.style.display = (i === visiblePosts.length - 1) ? 'none' : '';
      }
    });

    updateControls();
  }

  function updateControls() {
    const totalPages = getTotalPages();

    // Update page info
    const start = (currentPage - 1) * perPage + 1;
    const end = Math.min(currentPage * perPage, totalPosts);
    const infoText = `Showing ${start}-${end} of ${totalPosts}`;

    document.getElementById('page-info-top').textContent = infoText;
    document.getElementById('page-info-bottom').textContent = infoText;

    // Build page numbers
    ['top', 'bottom'].forEach(pos => {
      const container = document.getElementById(`page-numbers-${pos}`);
      container.innerHTML = '';

      // Prev button
      const prevBtn = document.createElement('button');
      prevBtn.textContent = '< Prev';
      prevBtn.disabled = currentPage === 1;
      prevBtn.onclick = () => { showPage(currentPage - 1); scrollToTop(); };
      container.appendChild(prevBtn);

      // Page number buttons
      const maxButtons = 7;
      let startPage = Math.max(1, currentPage - 3);
      let endPage = Math.min(totalPages, startPage + maxButtons - 1);

      if (endPage - startPage < maxButtons - 1) {
        startPage = Math.max(1, endPage - maxButtons + 1);
      }

      if (startPage > 1) {
        const btn = document.createElement('button');
        btn.textContent = '1';
        btn.onclick = () => { showPage(1); scrollToTop(); };
        container.appendChild(btn);

        if (startPage > 2) {
          const ellipsis = document.createElement('span');
          ellipsis.textContent = ' ... ';
          ellipsis.style.padding = '0 0.25em';
          container.appendChild(ellipsis);
        }
      }

      for (let i = startPage; i <= endPage; i++) {
        if (i === 1 && startPage > 1) continue;
        if (i === totalPages && endPage < totalPages) continue;

        const btn = document.createElement('button');
        btn.textContent = i;
        if (i === currentPage) btn.classList.add('active');
        btn.onclick = () => { showPage(i); scrollToTop(); };
        container.appendChild(btn);
      }

      if (endPage < totalPages) {
        if (endPage < totalPages - 1) {
          const ellipsis = document.createElement('span');
          ellipsis.textContent = ' ... ';
          ellipsis.style.padding = '0 0.25em';
          container.appendChild(ellipsis);
        }

        const btn = document.createElement('button');
        btn.textContent = totalPages;
        btn.onclick = () => { showPage(totalPages); scrollToTop(); };
        container.appendChild(btn);
      }

      // Next button
      const nextBtn = document.createElement('button');
      nextBtn.textContent = 'Next >';
      nextBtn.disabled = currentPage === totalPages;
      nextBtn.onclick = () => { showPage(currentPage + 1); scrollToTop(); };
      container.appendChild(nextBtn);
    });
  }

  function scrollToTop() {
    document.getElementById('pagination-top').scrollIntoView({ behavior: 'smooth' });
  }

  function setPerPage(value) {
    perPage = parseInt(value);
    perPageSelects.forEach(s => s.value = value);
    currentPage = 1;
    showPage(1);
  }

  // Event listeners
  perPageSelects.forEach(select => {
    select.addEventListener('change', (e) => setPerPage(e.target.value));
  });

  // Initialize - add class to enable JS-controlled pagination
  document.getElementById('post-list').classList.add('js-pagination');
  showPage(1);
})();
</script>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Software Wrighter Lab Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Mike Wright</li><li><a class="u-email" href="mailto:"></a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/softwarewrighter"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">softwarewrighter</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>AI coding agents, systems programming, and practical machine learning</p>
      </div>
    </div>

    <div class="footer-copyright">
      <p>Copyright &copy; 2026 Michael A. Wright</p>
    </div>

  </div>

  <button id="scroll-to-top" aria-label="Scroll to top" title="Scroll to top">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <polyline points="18 15 12 9 6 15"></polyline>
    </svg>
  </button>

  <style>
  #scroll-to-top {
    position: fixed;
    bottom: 2rem;
    right: 2rem;
    width: 44px;
    height: 44px;
    border-radius: 50%;
    border: none;
    background: var(--brand-color, #2a7ae2);
    color: white;
    cursor: pointer;
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.3s, visibility 0.3s, transform 0.2s;
    z-index: 1000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
  }

  #scroll-to-top:hover {
    transform: scale(1.1);
  }

  #scroll-to-top.visible {
    opacity: 1;
    visibility: visible;
  }

  #scroll-to-top svg {
    width: 24px;
    height: 24px;
  }
  </style>

  <script>
  (function() {
    const btn = document.getElementById('scroll-to-top');
    if (!btn) return;

    window.addEventListener('scroll', function() {
      if (window.scrollY > 300) {
        btn.classList.add('visible');
      } else {
        btn.classList.remove('visible');
      }
    });

    btn.addEventListener('click', function() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    });
  })();
  </script>

</footer>
</body>

</html>
