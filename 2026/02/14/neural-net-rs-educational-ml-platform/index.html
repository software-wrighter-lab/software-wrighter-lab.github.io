<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Neural-Net-RS: An Educational Neural Network Platform | Software Wrighter Lab Blog</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Neural-Net-RS: An Educational Neural Network Platform" />
<meta name="author" content="Software Wrighter" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I wanted a neural network implementation where every step is visible—no framework magic hiding the math. Something I could use to teach the fundamentals, with a CLI for quick experiments and a web UI for visual demonstrations. Claude Code built it. This is Personal Software for education: a complete neural network training platform with multiple interfaces, all from a single Rust codebase. Resource Link Repo neural-net-rs Video Neural-Net-RS Explainer Why Build Your Own Neural Network? Frameworks like PyTorch and TensorFlow are production-ready, but they hide the fundamentals. When teaching or learning, you want to see: How weights and biases actually change during training Why XOR needs a hidden layer when AND doesn’t What backpropagation really computes Neural-Net-RS exposes all of this. No autograd magic—every gradient is computed explicitly. No tensor abstractions—just matrices with clear row-major storage. What Got Built A modular Rust workspace with multiple interfaces to the same core: neural-net-rs/ ├── matrix/ # Linear algebra foundation ├── neural-network/ # Core ML implementation ├── neural-net-cli/ # Command-line interface ├── neural-net-server/ # REST API with SSE streaming └── neural-net-wasm/ # WebAssembly for browser One codebase, three ways to interact: CLI: Train from terminal with progress bars Web UI: Visual training with real-time loss charts WASM: Run entirely in browser, no server needed The Classic Problems The platform includes 8 built-in examples that teach ML concepts progressively: Problem Architecture Key Concept AND, OR 2→2→1 Linear separability XOR 2→3→1 Why hidden layers matter Parity3 3→6→1 Scaling non-linearity Quadrant 2→8→4 Multi-class classification Adder2 4→8→3 Learning arithmetic Iris 4→8→3 Real-world dataset Pattern3x3 9→6→4 Visual pattern recognition The XOR Problem XOR is the canonical neural network problem. AND and OR are linearly separable—a single line can divide the outputs. XOR isn’t. You need a hidden layer. AND: (0,0)→0, (0,1)→0, (1,0)→0, (1,1)→1 ← One line separates XOR: (0,0)→0, (0,1)→1, (1,0)→1, (1,1)→0 ← No line works Watch XOR training and you see why neural networks are powerful: they learn to create intermediate representations that make non-linear problems separable. Implementation Details Feed-Forward with Backpropagation pub struct Network { pub layers: Vec&lt;usize&gt;, // [input, hidden..., output] pub weights: Vec&lt;Matrix&gt;, // Learned connections pub biases: Vec&lt;Matrix&gt;, // Per-neuron offsets pub learning_rate: f64, // Training step size } Forward pass: Each layer computes activation(weights × input + bias) Backward pass: Gradients flow backward using the chain rule, updating weights to reduce error. The sigmoid activation function maps any input to (0, 1): σ(x) = 1 / (1 + e^(-x)) Custom Matrix Library Educational clarity over maximum performance: pub struct Matrix { rows: usize, cols: usize, data: Vec&lt;f64&gt;, // Row-major storage } Operations: dot product, transpose, element-wise multiply, map. Everything visible, nothing hidden. Checkpoint System Training can be interrupted and resumed: # Train for 5000 epochs, save checkpoint neural-net-cli train xor --epochs 5000 --checkpoint model.json # Resume from checkpoint neural-net-cli train xor --epochs 10000 --resume model.json Checkpoints include version metadata to prevent loading incompatible models. CLI Usage # List available examples neural-net-cli examples # Train XOR with progress bar neural-net-cli train xor --epochs 10000 --learning-rate 0.5 # Predict with trained model neural-net-cli predict model.json --input &quot;0,1&quot; # Run web UI neural-net-cli serve --port 8080 The CLI uses indicatif for real-time progress bars: Training XOR [=========&gt; ] 7500/10000 (75%) Loss: 0.0023 Web Interface The server embeds all assets at compile time—one binary serves everything: Training panel: Select problem, set hyperparameters, watch loss decrease Network visualization: See layer structure and connection strengths Prediction panel: Test the trained model interactively Loss chart: Real-time plotting via Server-Sent Events Two training modes: Local (WASM): Runs entirely in browser Remote (API): Server-side with streaming progress Technology Choices Component Purpose Rust Performance, safety, single-binary distribution Axum Lightweight async web framework wasm-bindgen Rust → WebAssembly compilation Indicatif Terminal progress bars Serde JSON serialization for checkpoints The WASM module is ~248KB after optimization. Test Coverage 136+ tests across the workspace: Matrix operations (unit tests) Network training (integration tests) CLI commands (integration tests) Server endpoints (integration tests) WASM bindings (unit tests) Zero clippy warnings. Reproducible results via seeded RNG. References Resource Link Backpropagation Learning representations by back-propagating errors (Rumelhart et al. 1986) Multi-Layer Perceptron Multilayer perceptron (Wikipedia) XOR Problem Perceptrons (Minsky &amp; Papert 1969) Weight Initialization Understanding the Difficulty of Training Deep Feedforward Neural Networks (Glorot &amp; Bengio 2010) Inspired by codemoonsxyz/neural-net-rs The Vibe Coding Process This project grew through iterative conversation with Claude Code: “Build a basic neural network in Rust with backpropagation” “Add a CLI with progress bars” “Add a web UI with real-time training visualization” “Compile to WASM so it runs in the browser” “Add checkpoint save/resume” “Include classic ML examples with educational documentation” Each request built on the previous. The AI handled architecture decisions, chose appropriate crates, and maintained test coverage throughout. When you want to understand how neural networks actually work, sometimes you need to see every weight update. That’s what this platform provides—education through transparency." />
<meta property="og:description" content="I wanted a neural network implementation where every step is visible—no framework magic hiding the math. Something I could use to teach the fundamentals, with a CLI for quick experiments and a web UI for visual demonstrations. Claude Code built it. This is Personal Software for education: a complete neural network training platform with multiple interfaces, all from a single Rust codebase. Resource Link Repo neural-net-rs Video Neural-Net-RS Explainer Why Build Your Own Neural Network? Frameworks like PyTorch and TensorFlow are production-ready, but they hide the fundamentals. When teaching or learning, you want to see: How weights and biases actually change during training Why XOR needs a hidden layer when AND doesn’t What backpropagation really computes Neural-Net-RS exposes all of this. No autograd magic—every gradient is computed explicitly. No tensor abstractions—just matrices with clear row-major storage. What Got Built A modular Rust workspace with multiple interfaces to the same core: neural-net-rs/ ├── matrix/ # Linear algebra foundation ├── neural-network/ # Core ML implementation ├── neural-net-cli/ # Command-line interface ├── neural-net-server/ # REST API with SSE streaming └── neural-net-wasm/ # WebAssembly for browser One codebase, three ways to interact: CLI: Train from terminal with progress bars Web UI: Visual training with real-time loss charts WASM: Run entirely in browser, no server needed The Classic Problems The platform includes 8 built-in examples that teach ML concepts progressively: Problem Architecture Key Concept AND, OR 2→2→1 Linear separability XOR 2→3→1 Why hidden layers matter Parity3 3→6→1 Scaling non-linearity Quadrant 2→8→4 Multi-class classification Adder2 4→8→3 Learning arithmetic Iris 4→8→3 Real-world dataset Pattern3x3 9→6→4 Visual pattern recognition The XOR Problem XOR is the canonical neural network problem. AND and OR are linearly separable—a single line can divide the outputs. XOR isn’t. You need a hidden layer. AND: (0,0)→0, (0,1)→0, (1,0)→0, (1,1)→1 ← One line separates XOR: (0,0)→0, (0,1)→1, (1,0)→1, (1,1)→0 ← No line works Watch XOR training and you see why neural networks are powerful: they learn to create intermediate representations that make non-linear problems separable. Implementation Details Feed-Forward with Backpropagation pub struct Network { pub layers: Vec&lt;usize&gt;, // [input, hidden..., output] pub weights: Vec&lt;Matrix&gt;, // Learned connections pub biases: Vec&lt;Matrix&gt;, // Per-neuron offsets pub learning_rate: f64, // Training step size } Forward pass: Each layer computes activation(weights × input + bias) Backward pass: Gradients flow backward using the chain rule, updating weights to reduce error. The sigmoid activation function maps any input to (0, 1): σ(x) = 1 / (1 + e^(-x)) Custom Matrix Library Educational clarity over maximum performance: pub struct Matrix { rows: usize, cols: usize, data: Vec&lt;f64&gt;, // Row-major storage } Operations: dot product, transpose, element-wise multiply, map. Everything visible, nothing hidden. Checkpoint System Training can be interrupted and resumed: # Train for 5000 epochs, save checkpoint neural-net-cli train xor --epochs 5000 --checkpoint model.json # Resume from checkpoint neural-net-cli train xor --epochs 10000 --resume model.json Checkpoints include version metadata to prevent loading incompatible models. CLI Usage # List available examples neural-net-cli examples # Train XOR with progress bar neural-net-cli train xor --epochs 10000 --learning-rate 0.5 # Predict with trained model neural-net-cli predict model.json --input &quot;0,1&quot; # Run web UI neural-net-cli serve --port 8080 The CLI uses indicatif for real-time progress bars: Training XOR [=========&gt; ] 7500/10000 (75%) Loss: 0.0023 Web Interface The server embeds all assets at compile time—one binary serves everything: Training panel: Select problem, set hyperparameters, watch loss decrease Network visualization: See layer structure and connection strengths Prediction panel: Test the trained model interactively Loss chart: Real-time plotting via Server-Sent Events Two training modes: Local (WASM): Runs entirely in browser Remote (API): Server-side with streaming progress Technology Choices Component Purpose Rust Performance, safety, single-binary distribution Axum Lightweight async web framework wasm-bindgen Rust → WebAssembly compilation Indicatif Terminal progress bars Serde JSON serialization for checkpoints The WASM module is ~248KB after optimization. Test Coverage 136+ tests across the workspace: Matrix operations (unit tests) Network training (integration tests) CLI commands (integration tests) Server endpoints (integration tests) WASM bindings (unit tests) Zero clippy warnings. Reproducible results via seeded RNG. References Resource Link Backpropagation Learning representations by back-propagating errors (Rumelhart et al. 1986) Multi-Layer Perceptron Multilayer perceptron (Wikipedia) XOR Problem Perceptrons (Minsky &amp; Papert 1969) Weight Initialization Understanding the Difficulty of Training Deep Feedforward Neural Networks (Glorot &amp; Bengio 2010) Inspired by codemoonsxyz/neural-net-rs The Vibe Coding Process This project grew through iterative conversation with Claude Code: “Build a basic neural network in Rust with backpropagation” “Add a CLI with progress bars” “Add a web UI with real-time training visualization” “Compile to WASM so it runs in the browser” “Add checkpoint save/resume” “Include classic ML examples with educational documentation” Each request built on the previous. The AI handled architecture decisions, chose appropriate crates, and maintained test coverage throughout. When you want to understand how neural networks actually work, sometimes you need to see every weight update. That’s what this platform provides—education through transparency." />
<link rel="canonical" href="https://software-wrighter-lab.github.io/2026/02/14/neural-net-rs-educational-ml-platform/" />
<meta property="og:url" content="https://software-wrighter-lab.github.io/2026/02/14/neural-net-rs-educational-ml-platform/" />
<meta property="og:site_name" content="Software Wrighter Lab Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-02-14T18:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Neural-Net-RS: An Educational Neural Network Platform" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Software Wrighter"},"dateModified":"2026-02-14T18:00:00-08:00","datePublished":"2026-02-14T18:00:00-08:00","description":"I wanted a neural network implementation where every step is visible—no framework magic hiding the math. Something I could use to teach the fundamentals, with a CLI for quick experiments and a web UI for visual demonstrations. Claude Code built it. This is Personal Software for education: a complete neural network training platform with multiple interfaces, all from a single Rust codebase. Resource Link Repo neural-net-rs Video Neural-Net-RS Explainer Why Build Your Own Neural Network? Frameworks like PyTorch and TensorFlow are production-ready, but they hide the fundamentals. When teaching or learning, you want to see: How weights and biases actually change during training Why XOR needs a hidden layer when AND doesn’t What backpropagation really computes Neural-Net-RS exposes all of this. No autograd magic—every gradient is computed explicitly. No tensor abstractions—just matrices with clear row-major storage. What Got Built A modular Rust workspace with multiple interfaces to the same core: neural-net-rs/ ├── matrix/ # Linear algebra foundation ├── neural-network/ # Core ML implementation ├── neural-net-cli/ # Command-line interface ├── neural-net-server/ # REST API with SSE streaming └── neural-net-wasm/ # WebAssembly for browser One codebase, three ways to interact: CLI: Train from terminal with progress bars Web UI: Visual training with real-time loss charts WASM: Run entirely in browser, no server needed The Classic Problems The platform includes 8 built-in examples that teach ML concepts progressively: Problem Architecture Key Concept AND, OR 2→2→1 Linear separability XOR 2→3→1 Why hidden layers matter Parity3 3→6→1 Scaling non-linearity Quadrant 2→8→4 Multi-class classification Adder2 4→8→3 Learning arithmetic Iris 4→8→3 Real-world dataset Pattern3x3 9→6→4 Visual pattern recognition The XOR Problem XOR is the canonical neural network problem. AND and OR are linearly separable—a single line can divide the outputs. XOR isn’t. You need a hidden layer. AND: (0,0)→0, (0,1)→0, (1,0)→0, (1,1)→1 ← One line separates XOR: (0,0)→0, (0,1)→1, (1,0)→1, (1,1)→0 ← No line works Watch XOR training and you see why neural networks are powerful: they learn to create intermediate representations that make non-linear problems separable. Implementation Details Feed-Forward with Backpropagation pub struct Network { pub layers: Vec&lt;usize&gt;, // [input, hidden..., output] pub weights: Vec&lt;Matrix&gt;, // Learned connections pub biases: Vec&lt;Matrix&gt;, // Per-neuron offsets pub learning_rate: f64, // Training step size } Forward pass: Each layer computes activation(weights × input + bias) Backward pass: Gradients flow backward using the chain rule, updating weights to reduce error. The sigmoid activation function maps any input to (0, 1): σ(x) = 1 / (1 + e^(-x)) Custom Matrix Library Educational clarity over maximum performance: pub struct Matrix { rows: usize, cols: usize, data: Vec&lt;f64&gt;, // Row-major storage } Operations: dot product, transpose, element-wise multiply, map. Everything visible, nothing hidden. Checkpoint System Training can be interrupted and resumed: # Train for 5000 epochs, save checkpoint neural-net-cli train xor --epochs 5000 --checkpoint model.json # Resume from checkpoint neural-net-cli train xor --epochs 10000 --resume model.json Checkpoints include version metadata to prevent loading incompatible models. CLI Usage # List available examples neural-net-cli examples # Train XOR with progress bar neural-net-cli train xor --epochs 10000 --learning-rate 0.5 # Predict with trained model neural-net-cli predict model.json --input &quot;0,1&quot; # Run web UI neural-net-cli serve --port 8080 The CLI uses indicatif for real-time progress bars: Training XOR [=========&gt; ] 7500/10000 (75%) Loss: 0.0023 Web Interface The server embeds all assets at compile time—one binary serves everything: Training panel: Select problem, set hyperparameters, watch loss decrease Network visualization: See layer structure and connection strengths Prediction panel: Test the trained model interactively Loss chart: Real-time plotting via Server-Sent Events Two training modes: Local (WASM): Runs entirely in browser Remote (API): Server-side with streaming progress Technology Choices Component Purpose Rust Performance, safety, single-binary distribution Axum Lightweight async web framework wasm-bindgen Rust → WebAssembly compilation Indicatif Terminal progress bars Serde JSON serialization for checkpoints The WASM module is ~248KB after optimization. Test Coverage 136+ tests across the workspace: Matrix operations (unit tests) Network training (integration tests) CLI commands (integration tests) Server endpoints (integration tests) WASM bindings (unit tests) Zero clippy warnings. Reproducible results via seeded RNG. References Resource Link Backpropagation Learning representations by back-propagating errors (Rumelhart et al. 1986) Multi-Layer Perceptron Multilayer perceptron (Wikipedia) XOR Problem Perceptrons (Minsky &amp; Papert 1969) Weight Initialization Understanding the Difficulty of Training Deep Feedforward Neural Networks (Glorot &amp; Bengio 2010) Inspired by codemoonsxyz/neural-net-rs The Vibe Coding Process This project grew through iterative conversation with Claude Code: “Build a basic neural network in Rust with backpropagation” “Add a CLI with progress bars” “Add a web UI with real-time training visualization” “Compile to WASM so it runs in the browser” “Add checkpoint save/resume” “Include classic ML examples with educational documentation” Each request built on the previous. The AI handled architecture decisions, chose appropriate crates, and maintained test coverage throughout. When you want to understand how neural networks actually work, sometimes you need to see every weight update. That’s what this platform provides—education through transparency.","headline":"Neural-Net-RS: An Educational Neural Network Platform","mainEntityOfPage":{"@type":"WebPage","@id":"https://software-wrighter-lab.github.io/2026/02/14/neural-net-rs-educational-ml-platform/"},"url":"https://software-wrighter-lab.github.io/2026/02/14/neural-net-rs-educational-ml-platform/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://software-wrighter-lab.github.io/feed.xml" title="Software Wrighter Lab Blog" /><!-- Theme toggle script - load early to prevent flash -->
  <script>
    (function() {
      var stored = localStorage.getItem('sw-lab-theme');
      var theme = stored;
      if (!theme) {
        theme = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
      }
      document.documentElement.setAttribute('data-theme', theme);
    })();
  </script>
  <!-- Font size - load early to prevent flash -->
  <script>
    (function() {
      var NEW_DEFAULT = 110;
      var OLD_DEFAULT = 150;
      var stored = localStorage.getItem('sw-lab-font-size');
      var ack = localStorage.getItem('sw-lab-prefs-ack');
      var size = stored ? parseInt(stored, 10) : null;

      // Migration: if on old default and not acknowledged, use new default
      if (size === OLD_DEFAULT && ack !== 'true') {
        size = NEW_DEFAULT;
      } else if (size === null) {
        size = NEW_DEFAULT;
      }

      // Apply to post content when DOM is ready
      document.addEventListener('DOMContentLoaded', function() {
        var pc = document.querySelector('.post-content');
        if (pc) pc.style.fontSize = size + '%';
        var pl = document.querySelector('.post-list');
        if (pl) pl.style.fontSize = size + '%';
      });
    })();
  </script>
  <script src="/assets/js/theme-toggle.js" defer></script>
  <script src="/assets/js/font-size.js" defer></script>
  <script src="/assets/js/preferences.js" defer></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">
      <img src="/assets/images/site/logo.jpg" alt="Software Wrighter Lab Blog" class="site-logo">
      <span class="site-title-text">Software Wrighter Lab Blog</span>
    </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/abstracts/">Abstracts</a><a class="page-link" href="/index-all/">Index</a><a class="page-link" href="/series/">Series</a><a class="page-link" href="/tags/">Tags</a><a class="page-link" href="/categories/">Categories</a><a class="page-link" href="/search/">Search</a><div class="font-size-controls">
  <button class="font-size-btn" id="font-decrease" title="Decrease font size" aria-label="Decrease font size">A-</button>
  <button class="font-size-btn" id="font-reset" title="Reset font size" aria-label="Reset font size">A</button>
  <button class="font-size-btn" id="font-increase" title="Increase font size" aria-label="Increase font size">A+</button>
</div>
<button class="theme-toggle" aria-label="Switch theme" title="Switch theme">
  <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
  </svg>
  <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
  </svg>
</button>
</div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Neural-Net-RS: An Educational Neural Network Platform</h1><p class="post-meta">February 14, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">1048 words</span> &bull; <span class="post-read-time">6 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Personal Software for education: a neural network platform where every step is visible---no framework magic. CLI with progress bars, web UI with real-time loss charts, WASM for browser execution. Built via Vibe Coding to watch XOR training reveal why hidden layers matter.</div><div class="post-taxonomies"><span class="post-categories"><a href="/categories/#rust" class="category">rust</a><a href="/categories/#machine-learning" class="category">machine-learning</a><a href="/categories/#projects" class="category">projects</a><a href="/categories/#vibe-coding" class="category">vibe-coding</a></span><span class="post-tags"><a href="/tags/#rust" class="tag">rust</a><a href="/tags/#neural-networks" class="tag">neural-networks</a><a href="/tags/#backpropagation" class="tag">backpropagation</a><a href="/tags/#wasm" class="tag">wasm</a><a href="/tags/#cli-tools" class="tag">cli-tools</a><a href="/tags/#vibe-coding" class="tag">vibe-coding</a><a href="/tags/#personal-software" class="tag">personal-software</a><a href="/tags/#claude-code" class="tag">claude-code</a><a href="/tags/#education" class="tag">education</a></span></div></header><nav class="toc" data-toc-id="default">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content e-content" itemprop="articleBody">
    <p><img src="/assets/images/posts/block-graph.png" class="post-marker" alt="" /></p>

<p>I wanted a neural network implementation where every step is visible—no framework magic hiding the math. Something I could use to teach the fundamentals, with a CLI for quick experiments and a web UI for visual demonstrations. Claude Code built it.</p>

<p>This is <strong>Personal Software</strong> for education: a complete neural network training platform with multiple interfaces, all from a single Rust codebase.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Repo</strong></td>
        <td><a href="https://github.com/sw-ml-study/neural-net-rs">neural-net-rs</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/watch?v=UNrzf4Wgbv4">Neural-Net-RS Explainer</a><br /><a href="https://www.youtube.com/watch?v=UNrzf4Wgbv4"><img src="https://img.youtube.com/vi/UNrzf4Wgbv4/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="why-build-your-own-neural-network">Why Build Your Own Neural Network?</h2>

<p>Frameworks like PyTorch and TensorFlow are production-ready, but they hide the fundamentals. When teaching or learning, you want to see:</p>

<ul>
  <li>How weights and biases actually change during training</li>
  <li>Why XOR needs a hidden layer when AND doesn’t</li>
  <li>What backpropagation really computes</li>
</ul>

<p>Neural-Net-RS exposes all of this. No autograd magic—every gradient is computed explicitly. No tensor abstractions—just matrices with clear row-major storage.</p>

<h2 id="what-got-built">What Got Built</h2>

<p>A modular Rust workspace with multiple interfaces to the same core:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>neural-net-rs/
├── matrix/              # Linear algebra foundation
├── neural-network/      # Core ML implementation
├── neural-net-cli/      # Command-line interface
├── neural-net-server/   # REST API with SSE streaming
└── neural-net-wasm/     # WebAssembly for browser
</code></pre></div></div>

<p><strong>One codebase, three ways to interact:</strong></p>
<ul>
  <li><strong>CLI</strong>: Train from terminal with progress bars</li>
  <li><strong>Web UI</strong>: Visual training with real-time loss charts</li>
  <li><strong>WASM</strong>: Run entirely in browser, no server needed</li>
</ul>

<h2 id="the-classic-problems">The Classic Problems</h2>

<p>The platform includes 8 built-in examples that teach ML concepts progressively:</p>

<table>
  <thead>
    <tr>
      <th>Problem</th>
      <th>Architecture</th>
      <th>Key Concept</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>AND, OR</td>
      <td>2→2→1</td>
      <td>Linear separability</td>
    </tr>
    <tr>
      <td><strong>XOR</strong></td>
      <td>2→3→1</td>
      <td>Why hidden layers matter</td>
    </tr>
    <tr>
      <td>Parity3</td>
      <td>3→6→1</td>
      <td>Scaling non-linearity</td>
    </tr>
    <tr>
      <td>Quadrant</td>
      <td>2→8→4</td>
      <td>Multi-class classification</td>
    </tr>
    <tr>
      <td>Adder2</td>
      <td>4→8→3</td>
      <td>Learning arithmetic</td>
    </tr>
    <tr>
      <td>Iris</td>
      <td>4→8→3</td>
      <td>Real-world dataset</td>
    </tr>
    <tr>
      <td>Pattern3x3</td>
      <td>9→6→4</td>
      <td>Visual pattern recognition</td>
    </tr>
  </tbody>
</table>

<h3 id="the-xor-problem">The XOR Problem</h3>

<p>XOR is the canonical neural network problem. AND and OR are linearly separable—a single line can divide the outputs. XOR isn’t. You <em>need</em> a hidden layer.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>AND: (0,0)→0, (0,1)→0, (1,0)→0, (1,1)→1  ← One line separates
XOR: (0,0)→0, (0,1)→1, (1,0)→1, (1,1)→0  ← No line works
</code></pre></div></div>

<p>Watch XOR training and you see why neural networks are powerful: they learn to create intermediate representations that make non-linear problems separable.</p>

<h2 id="implementation-details">Implementation Details</h2>

<h3 id="feed-forward-with-backpropagation">Feed-Forward with Backpropagation</h3>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">struct</span> <span class="n">Network</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="n">layers</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span><span class="p">,</span>      <span class="c1">// [input, hidden..., output]</span>
    <span class="k">pub</span> <span class="n">weights</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="n">Matrix</span><span class="o">&gt;</span><span class="p">,</span>    <span class="c1">// Learned connections</span>
    <span class="k">pub</span> <span class="n">biases</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="n">Matrix</span><span class="o">&gt;</span><span class="p">,</span>     <span class="c1">// Per-neuron offsets</span>
    <span class="k">pub</span> <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">f64</span><span class="p">,</span>      <span class="c1">// Training step size</span>
<span class="p">}</span>
</code></pre></div></div>

<p><strong>Forward pass</strong>: Each layer computes <code class="language-plaintext highlighter-rouge">activation(weights × input + bias)</code></p>

<p><strong>Backward pass</strong>: Gradients flow backward using the chain rule, updating weights to reduce error.</p>

<p>The sigmoid activation function maps any input to (0, 1):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>σ(x) = 1 / (1 + e^(-x))
</code></pre></div></div>

<h3 id="custom-matrix-library">Custom Matrix Library</h3>

<p>Educational clarity over maximum performance:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">struct</span> <span class="n">Matrix</span> <span class="p">{</span>
    <span class="n">rows</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
    <span class="n">cols</span><span class="p">:</span> <span class="nb">usize</span><span class="p">,</span>
    <span class="n">data</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f64</span><span class="o">&gt;</span><span class="p">,</span>  <span class="c1">// Row-major storage</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Operations: dot product, transpose, element-wise multiply, map. Everything visible, nothing hidden.</p>

<h3 id="checkpoint-system">Checkpoint System</h3>

<p>Training can be interrupted and resumed:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Train for 5000 epochs, save checkpoint</span>
neural-net-cli train xor <span class="nt">--epochs</span> 5000 <span class="nt">--checkpoint</span> model.json

<span class="c"># Resume from checkpoint</span>
neural-net-cli train xor <span class="nt">--epochs</span> 10000 <span class="nt">--resume</span> model.json
</code></pre></div></div>

<p>Checkpoints include version metadata to prevent loading incompatible models.</p>

<h2 id="cli-usage">CLI Usage</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># List available examples</span>
neural-net-cli examples

<span class="c"># Train XOR with progress bar</span>
neural-net-cli train xor <span class="nt">--epochs</span> 10000 <span class="nt">--learning-rate</span> 0.5

<span class="c"># Predict with trained model</span>
neural-net-cli predict model.json <span class="nt">--input</span> <span class="s2">"0,1"</span>

<span class="c"># Run web UI</span>
neural-net-cli serve <span class="nt">--port</span> 8080
</code></pre></div></div>

<p>The CLI uses <code class="language-plaintext highlighter-rouge">indicatif</code> for real-time progress bars:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training XOR [=========&gt;   ] 7500/10000 (75%) Loss: 0.0023
</code></pre></div></div>

<h2 id="web-interface">Web Interface</h2>

<p>The server embeds all assets at compile time—one binary serves everything:</p>

<ul>
  <li><strong>Training panel</strong>: Select problem, set hyperparameters, watch loss decrease</li>
  <li><strong>Network visualization</strong>: See layer structure and connection strengths</li>
  <li><strong>Prediction panel</strong>: Test the trained model interactively</li>
  <li><strong>Loss chart</strong>: Real-time plotting via Server-Sent Events</li>
</ul>

<p>Two training modes:</p>
<ul>
  <li><strong>Local (WASM)</strong>: Runs entirely in browser</li>
  <li><strong>Remote (API)</strong>: Server-side with streaming progress</li>
</ul>

<h2 id="technology-choices">Technology Choices</h2>

<table>
  <thead>
    <tr>
      <th>Component</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Rust</strong></td>
      <td>Performance, safety, single-binary distribution</td>
    </tr>
    <tr>
      <td><strong>Axum</strong></td>
      <td>Lightweight async web framework</td>
    </tr>
    <tr>
      <td><strong>wasm-bindgen</strong></td>
      <td>Rust → WebAssembly compilation</td>
    </tr>
    <tr>
      <td><strong>Indicatif</strong></td>
      <td>Terminal progress bars</td>
    </tr>
    <tr>
      <td><strong>Serde</strong></td>
      <td>JSON serialization for checkpoints</td>
    </tr>
  </tbody>
</table>

<p>The WASM module is ~248KB after optimization.</p>

<h2 id="test-coverage">Test Coverage</h2>

<p>136+ tests across the workspace:</p>
<ul>
  <li>Matrix operations (unit tests)</li>
  <li>Network training (integration tests)</li>
  <li>CLI commands (integration tests)</li>
  <li>Server endpoints (integration tests)</li>
  <li>WASM bindings (unit tests)</li>
</ul>

<p>Zero clippy warnings. Reproducible results via seeded RNG.</p>

<div class="references-section">

  <h2 id="references">References</h2>

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Backpropagation</strong></td>
        <td><a href="https://www.nature.com/articles/323533a0">Learning representations by back-propagating errors</a> (Rumelhart et al. 1986)</td>
      </tr>
      <tr>
        <td><strong>Multi-Layer Perceptron</strong></td>
        <td><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron">Multilayer perceptron</a> (Wikipedia)</td>
      </tr>
      <tr>
        <td><strong>XOR Problem</strong></td>
        <td><a href="https://mitpress.mit.edu/9780262534772/perceptrons/">Perceptrons</a> (Minsky &amp; Papert 1969)</td>
      </tr>
      <tr>
        <td><strong>Weight Initialization</strong></td>
        <td><a href="https://proceedings.mlr.press/v9/glorot10a.html">Understanding the Difficulty of Training Deep Feedforward Neural Networks</a> (Glorot &amp; Bengio 2010)</td>
      </tr>
      <tr>
        <td><strong>Inspired by</strong></td>
        <td><a href="https://github.com/codemoonsxyz/neural-net-rs">codemoonsxyz/neural-net-rs</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-vibe-coding-process">The Vibe Coding Process</h2>

<p>This project grew through iterative conversation with Claude Code:</p>

<ol>
  <li>“Build a basic neural network in Rust with backpropagation”</li>
  <li>“Add a CLI with progress bars”</li>
  <li>“Add a web UI with real-time training visualization”</li>
  <li>“Compile to WASM so it runs in the browser”</li>
  <li>“Add checkpoint save/resume”</li>
  <li>“Include classic ML examples with educational documentation”</li>
</ol>

<p>Each request built on the previous. The AI handled architecture decisions, chose appropriate crates, and maintained test coverage throughout.</p>

<hr />

<p><em>When you want to understand how neural networks actually work, sometimes you need to see every weight update. That’s what this platform provides—education through transparency.</em></p>


  </div><div class="series-nav">
    <p><em>Part 4 of the Machine Learning series. <a href="/series/#machine-learning">View all parts</a> | <a href="/2026/02/22/icl-revisited-from-mystery-to-engineering/">Next: Part 5 →</a></em></p>
  </div>





<div class="youtube-embed-container" id="yt-container-UNrzf4Wgbv4">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-UNrzf4Wgbv4"
      src="https://www.youtube.com/embed/UNrzf4Wgbv4?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-UNrzf4Wgbv4';
  const playerId = 'yt-player-UNrzf4Wgbv4';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt=""><a class="u-url" href="/2026/02/14/neural-net-rs-educational-ml-platform/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Software Wrighter Lab Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Mike Wright</li><li><a class="u-email" href="mailto:"></a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/softwarewrighter"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">softwarewrighter</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>AI coding agents, systems programming, and practical machine learning</p>
      </div>
    </div>

    <div class="footer-copyright">
      <p>Copyright &copy; 2026 Michael A. Wright</p>
    </div>

  </div>

  <button id="scroll-to-top" aria-label="Scroll to top" title="Scroll to top">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <polyline points="18 15 12 9 6 15"></polyline>
    </svg>
  </button>

  <style>
  #scroll-to-top {
    position: fixed;
    bottom: 2rem;
    right: 2rem;
    width: 44px;
    height: 44px;
    border-radius: 50%;
    border: none;
    background: var(--brand-color, #2a7ae2);
    color: white;
    cursor: pointer;
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.3s, visibility 0.3s, transform 0.2s;
    z-index: 1000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
  }

  #scroll-to-top:hover {
    transform: scale(1.1);
  }

  #scroll-to-top.visible {
    opacity: 1;
    visibility: visible;
  }

  #scroll-to-top svg {
    width: 24px;
    height: 24px;
  }
  </style>

  <script>
  (function() {
    const btn = document.getElementById('scroll-to-top');
    if (!btn) return;

    window.addEventListener('scroll', function() {
      if (window.scrollY > 300) {
        btn.classList.add('visible');
      } else {
        btn.classList.remove('visible');
      }
    });

    btn.addEventListener('click', function() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    });
  })();
  </script>

</footer>
</body>

</html>
