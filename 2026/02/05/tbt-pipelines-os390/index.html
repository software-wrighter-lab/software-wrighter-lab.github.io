<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>TBT (2/?): Pipelines on OS/390 | Software Wrighter Lab Blog</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="TBT (2/?): Pipelines on OS/390" />
<meta name="author" content="Software Wrighter" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Unix invented pipes. Mainframes reinvented them‚Äîfor records, not bytes. This is the second Throwback Thursday post‚Äîrevisiting technologies that shaped how I think about programming. This time: CMS/TSO Pipelines, and a vibe coding project that brings them back to life in Rust for education, fun, and nostalgic reasons. Resource Link Code pipelines-rs Demo Live Demo Video Pipelines on OS/390 #TBT The 1996 Olympics and a Pair of Mainframes In 1996, IBM hosted the Olympics Web Server‚Äîone of the largest public web properties at the time. Many distributed IBM systems in different regions served dynamic web pages. The logs from all of them were funneled to a pair of IBM S/390 mainframes I was in charge of, running OS/390 (formerly MVS). When you‚Äôre processing millions of log records for statistics and forensics, you need tools that think in records, not lines. That‚Äôs where Pipelines for TSO/E came in. Pipelines for TSO/E was the MVS/ESA port of CMS Pipelines, which ran on VM/ESA. Both let you chain stages together to filter, transform, and aggregate record-oriented data‚Äîrecord-oriented pipelines that evolved in parallel with Unix‚Äôs byte-stream pipes. Two Traditions of Piping Unix pipes came first‚ÄîThompson and McIlroy at Bell Labs, 1969‚Äì1974. Byte streams, file descriptors, the | operator. Brutally simple. Explosively powerful. POSIX.1-1988 standardized pipe(2) and shell pipelines, though POSIX work began in the mid-1980s. CMS Pipelines emerged on IBM mainframes in the mid-to-late 1980s. They weren‚Äôt a Unix clone‚Äîthey were convergent evolution under different pressures. Where Unix piped bytes between small programs, CMS piped records through declarative stages. Pipelines for TSO/E followed in the late 1980s and early 1990s, porting CMS concepts to the MVS multi-user environment. Unlike CMS Pipelines (which ships with z/VM), the TSO/E port is typically installed separately on z/OS. Neither tradition was ‚Äúbehind.‚Äù They were optimizing different dimensions: ¬† Unix Pipes CMS/TSO Pipelines Era 1969‚Äì1974 Mid-to-late 1980s Data unit Byte stream Records (fixed or variable length) Stage input stdin (bytes) Record buffer Field access awk, cut (text parsing) Column positions (direct) Execution Typically a process per stage Stages in one address space Topology Linear by default; fan-out/fan-in via tee, FIFOs, or process substitution Multi-stream, fan-out/fan-in built in Philosophy Small tools, ad hoc composition Declarative data transformation Many datasets on mainframes are record-structured. Records can be fixed-length or variable-length. CMS and TSO/E Pipelines treat records as byte arrays‚Äîcharacter-oriented stages assume EBCDIC text, while position/length stages are binary-safe. A fixed-length 80-byte record isn‚Äôt arbitrary text‚Äîcolumns 1-8 are the name, 9-18 are the department, 19-26 are the salary. You don‚Äôt parse. You just read the right columns. Unix won culturally‚Äîcheap hardware, academic distribution, C portability. But IBM‚Äôs record-oriented pipelines were better at structured dataflow, and they anticipate or parallel patterns seen in ETL frameworks like Spark and Beam. CMS Pipelines ships with z/VM and is still used; Pipelines for TSO/E exists for z/OS but isn‚Äôt universally installed. These are not historical curiosities‚Äîmainframes continue to process a significant share of high-value transactions, and pipelines remain an available tool for data transformation on those systems. What a Pipeline Looks Like CMS Pipelines uses a DSL with PIPE as the command, | to chain stages, and ? as a command terminator (it suppresses the console from being used as implicit input): PIPE CONSOLE | FILTER 18,10 = &quot;SALES&quot; | SELECT 0,8,0; 8,10,8 | CONSOLE ? This reads input records, keeps only those where columns 18‚Äì27 equal ‚ÄúSALES‚Äù, extracts the name fields, and writes the result. No regex. No string splitting. Just column positions. Note: pipelines-rs uses 0-based offsets (e.g., SELECT 0,8,0). Historical CMS Pipelines uses 1-based column positions. Compare with the Unix equivalent: cat input.txt | awk &#39;$3 == &quot;SALES&quot; {print $1, $2}&#39; The Unix version looks simpler‚Äîuntil your fields contain spaces, or your records contain non-text bytes, or you need to chain 15 stages without spawning 15 processes. Bringing It Back in Rust (Vibe Coding) pipelines-rs is a nostalgia-driven vibe coding project‚Äîmy attempt to emulate Pipelines for TSO/E in Rust, not because it‚Äôs practical, but because these ideas deserve to be celebrated. It supports a subset of stages and features two execution models: The Two Executors Batched processes all records through one stage before moving to the next: All records ‚Üí Stage 1 ‚Üí All records ‚Üí Stage 2 ‚Üí All records ‚Üí Stage 3 This emulates the correct output and is faster, but doesn‚Äôt demonstrate record-oriented dataflow well. Record-At-a-Time (RAT) sends each record through the entire pipeline before reading the next: Record 1 ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí Stage 3 ‚Üí Output Record 2 ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí Stage 3 ‚Üí Output Record 3 ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí Stage 3 ‚Üí Output RAT is the implementation shown in the video. It‚Äôs a naive approach‚Äîmore buffers, more copying‚Äîbut it shows the dataflow concepts clearly and enables the visual debugger. Both run in linear time (records √ó stages) and produce identical output for all 23 test specifications. A future version will aim for fewer buffers and fewer copy operations. Whether it‚Äôs faster than Batched remains to be seen. The 80-Byte Record The Rust implementation supports fixed-length records only. The fundamental data type is the Record‚Äîexactly 80 bytes, matching historical punch card width. Variable-length input lines are accepted and padded to 80 bytes: pub const RECORD_WIDTH: usize = 80; pub struct Record { data: [u8; RECORD_WIDTH], } Fields are accessed by column position and length. No parsing, no delimiters. The data is always right where you expect it. Supported Stages The current implementation supports 14 stages: Stage Purpose Example FILTER Keep/reject records by field value FILTER 18,10 = &quot;SALES&quot; LOCATE Keep records containing a pattern LOCATE &quot;ERROR&quot; NLOCATE Keep records NOT containing a pattern NLOCATE &quot;DEBUG&quot; SELECT Extract and reposition fields SELECT 0,8,0; 8,10,8 CHANGE Text replacement CHANGE &quot;SALES&quot; &quot;MKTG&quot; COUNT Count records COUNT TAKE Keep first N records TAKE 5 SKIP Skip first N records SKIP 2 DUPLICATE Repeat each record N times DUPLICATE 3 LITERAL Append a literal record LITERAL &quot;--- END ---&quot; UPPER/LOWER Case conversion UPPER REVERSE Reverse record text REVERSE HOLE Discard all input HOLE CONSOLE Driver stage: source or sink depending on position CONSOLE The CLI Both executors have identical CLIs: # Batch executor pipe-run specs/filter-sales.pipe specs/input-fixed-80.data -v # Record-at-a-time executor pipe-run-rat specs/filter-sales.pipe specs/input-fixed-80.data -v Given this input data: SMITH JOHN SALES 00050000 JONES MARY ENGINEER 00075000 DOE JANE SALES 00060000 WILSON ROBERT MARKETING 00055000 CHEN LISA ENGINEER 00080000 GARCIA CARLOS SALES 00045000 TAYLOR SUSAN MARKETING 00065000 BROWN MICHAEL ENGINEER 00090000 And this pipeline: PIPE CONSOLE | FILTER 18,10 = &quot;SALES&quot; | CONSOLE ? The output is: SMITH JOHN SALES 00050000 DOE JANE SALES 00060000 GARCIA CARLOS SALES 00045000 Records: 8 in -&gt; 3 out Exactly what I‚Äôd have gotten on OS/390 in 1996, but with Web Server log data showing client IP address, OS, browser type/version, user cookies, timestamps, URLs, and more, instead of accounting data. üòä The Web UI for Two pipelines-rs Implementations The web interface runs entirely in the browser via WebAssembly. It has three panels: input records with an 80-column ruler, the pipeline editor, and the output. Tutorial Mode The tutorial walks through each stage with examples, running pipelines automatically to show results. You can step through manually or let it auto-advance. The Visual Debugger The debugger is the reason RAT exists. It lets you: Step through execution one pipe point at a time Watch data at specific pipe points between stages Set breakpoints to pause at specific stages See stage state for stateful stages like COUNT You load a pipeline, click Run, then Step to watch each record flow through each stage. The debugger highlights which stages have been reached with a green border. For COUNT and other aggregation stages, you can watch the flush phase where accumulated state becomes output. What‚Äôs Next The current RAT executor is intentionally naive‚Äîit uses a buffer at every pipe point and copies each record between them. A better implementation would minimize buffers and copy operations while preserving the record-at-a-time semantics. Multi-pipe features are also planned‚ÄîCMS Pipelines supported fan-out (one input, multiple output streams) and fan-in (multiple inputs merged), which enabled complex processing topologies beyond simple linear chains. How pipelines-rs Differs from IBM Pipelines ¬† IBM CMS/TSO/E Pipelines pipelines-rs Indexing 1-based column positions 0-based offsets Record format Fixed or variable length, EBCDIC Fixed 80-byte ASCII only (variable-length input padded) Stages Hundreds of built-in stages 14 implemented so far Topology Multi-stream: fan-out, fan-in, multi-pipe Linear only (multi-pipe planned) Environment z/VM, z/OS mainframes CLI (native) and browser (WASM) Character set EBCDIC ASCII/UTF-8 This is a teaching tool and nostalgia project, not a production replacement. Implementation Details Metric Value Language Rust (2024 edition) Web UI Yew framework, compiled to WASM Stages 14 implemented Test Specs 23 pipeline specifications Tests 60+ (including batch/RAT equivalence) License MIT Live Demo sw-comp-history.github.io/pipelines-rs Resources pipelines-rs Repository Live Demo CMS Pipelines Reference (IBM) Credits Role Who Concept &amp; direction Mike Wright Content creation Claude (Anthropic) Editorial review ChatGPT (OpenAI) *Part 2 of the Throwback Thursday series. View all parts Next: TBT (3): Vector Graphics Games* Mainframe ideas, modern tools. Follow for more." />
<meta property="og:description" content="Unix invented pipes. Mainframes reinvented them‚Äîfor records, not bytes. This is the second Throwback Thursday post‚Äîrevisiting technologies that shaped how I think about programming. This time: CMS/TSO Pipelines, and a vibe coding project that brings them back to life in Rust for education, fun, and nostalgic reasons. Resource Link Code pipelines-rs Demo Live Demo Video Pipelines on OS/390 #TBT The 1996 Olympics and a Pair of Mainframes In 1996, IBM hosted the Olympics Web Server‚Äîone of the largest public web properties at the time. Many distributed IBM systems in different regions served dynamic web pages. The logs from all of them were funneled to a pair of IBM S/390 mainframes I was in charge of, running OS/390 (formerly MVS). When you‚Äôre processing millions of log records for statistics and forensics, you need tools that think in records, not lines. That‚Äôs where Pipelines for TSO/E came in. Pipelines for TSO/E was the MVS/ESA port of CMS Pipelines, which ran on VM/ESA. Both let you chain stages together to filter, transform, and aggregate record-oriented data‚Äîrecord-oriented pipelines that evolved in parallel with Unix‚Äôs byte-stream pipes. Two Traditions of Piping Unix pipes came first‚ÄîThompson and McIlroy at Bell Labs, 1969‚Äì1974. Byte streams, file descriptors, the | operator. Brutally simple. Explosively powerful. POSIX.1-1988 standardized pipe(2) and shell pipelines, though POSIX work began in the mid-1980s. CMS Pipelines emerged on IBM mainframes in the mid-to-late 1980s. They weren‚Äôt a Unix clone‚Äîthey were convergent evolution under different pressures. Where Unix piped bytes between small programs, CMS piped records through declarative stages. Pipelines for TSO/E followed in the late 1980s and early 1990s, porting CMS concepts to the MVS multi-user environment. Unlike CMS Pipelines (which ships with z/VM), the TSO/E port is typically installed separately on z/OS. Neither tradition was ‚Äúbehind.‚Äù They were optimizing different dimensions: ¬† Unix Pipes CMS/TSO Pipelines Era 1969‚Äì1974 Mid-to-late 1980s Data unit Byte stream Records (fixed or variable length) Stage input stdin (bytes) Record buffer Field access awk, cut (text parsing) Column positions (direct) Execution Typically a process per stage Stages in one address space Topology Linear by default; fan-out/fan-in via tee, FIFOs, or process substitution Multi-stream, fan-out/fan-in built in Philosophy Small tools, ad hoc composition Declarative data transformation Many datasets on mainframes are record-structured. Records can be fixed-length or variable-length. CMS and TSO/E Pipelines treat records as byte arrays‚Äîcharacter-oriented stages assume EBCDIC text, while position/length stages are binary-safe. A fixed-length 80-byte record isn‚Äôt arbitrary text‚Äîcolumns 1-8 are the name, 9-18 are the department, 19-26 are the salary. You don‚Äôt parse. You just read the right columns. Unix won culturally‚Äîcheap hardware, academic distribution, C portability. But IBM‚Äôs record-oriented pipelines were better at structured dataflow, and they anticipate or parallel patterns seen in ETL frameworks like Spark and Beam. CMS Pipelines ships with z/VM and is still used; Pipelines for TSO/E exists for z/OS but isn‚Äôt universally installed. These are not historical curiosities‚Äîmainframes continue to process a significant share of high-value transactions, and pipelines remain an available tool for data transformation on those systems. What a Pipeline Looks Like CMS Pipelines uses a DSL with PIPE as the command, | to chain stages, and ? as a command terminator (it suppresses the console from being used as implicit input): PIPE CONSOLE | FILTER 18,10 = &quot;SALES&quot; | SELECT 0,8,0; 8,10,8 | CONSOLE ? This reads input records, keeps only those where columns 18‚Äì27 equal ‚ÄúSALES‚Äù, extracts the name fields, and writes the result. No regex. No string splitting. Just column positions. Note: pipelines-rs uses 0-based offsets (e.g., SELECT 0,8,0). Historical CMS Pipelines uses 1-based column positions. Compare with the Unix equivalent: cat input.txt | awk &#39;$3 == &quot;SALES&quot; {print $1, $2}&#39; The Unix version looks simpler‚Äîuntil your fields contain spaces, or your records contain non-text bytes, or you need to chain 15 stages without spawning 15 processes. Bringing It Back in Rust (Vibe Coding) pipelines-rs is a nostalgia-driven vibe coding project‚Äîmy attempt to emulate Pipelines for TSO/E in Rust, not because it‚Äôs practical, but because these ideas deserve to be celebrated. It supports a subset of stages and features two execution models: The Two Executors Batched processes all records through one stage before moving to the next: All records ‚Üí Stage 1 ‚Üí All records ‚Üí Stage 2 ‚Üí All records ‚Üí Stage 3 This emulates the correct output and is faster, but doesn‚Äôt demonstrate record-oriented dataflow well. Record-At-a-Time (RAT) sends each record through the entire pipeline before reading the next: Record 1 ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí Stage 3 ‚Üí Output Record 2 ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí Stage 3 ‚Üí Output Record 3 ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí Stage 3 ‚Üí Output RAT is the implementation shown in the video. It‚Äôs a naive approach‚Äîmore buffers, more copying‚Äîbut it shows the dataflow concepts clearly and enables the visual debugger. Both run in linear time (records √ó stages) and produce identical output for all 23 test specifications. A future version will aim for fewer buffers and fewer copy operations. Whether it‚Äôs faster than Batched remains to be seen. The 80-Byte Record The Rust implementation supports fixed-length records only. The fundamental data type is the Record‚Äîexactly 80 bytes, matching historical punch card width. Variable-length input lines are accepted and padded to 80 bytes: pub const RECORD_WIDTH: usize = 80; pub struct Record { data: [u8; RECORD_WIDTH], } Fields are accessed by column position and length. No parsing, no delimiters. The data is always right where you expect it. Supported Stages The current implementation supports 14 stages: Stage Purpose Example FILTER Keep/reject records by field value FILTER 18,10 = &quot;SALES&quot; LOCATE Keep records containing a pattern LOCATE &quot;ERROR&quot; NLOCATE Keep records NOT containing a pattern NLOCATE &quot;DEBUG&quot; SELECT Extract and reposition fields SELECT 0,8,0; 8,10,8 CHANGE Text replacement CHANGE &quot;SALES&quot; &quot;MKTG&quot; COUNT Count records COUNT TAKE Keep first N records TAKE 5 SKIP Skip first N records SKIP 2 DUPLICATE Repeat each record N times DUPLICATE 3 LITERAL Append a literal record LITERAL &quot;--- END ---&quot; UPPER/LOWER Case conversion UPPER REVERSE Reverse record text REVERSE HOLE Discard all input HOLE CONSOLE Driver stage: source or sink depending on position CONSOLE The CLI Both executors have identical CLIs: # Batch executor pipe-run specs/filter-sales.pipe specs/input-fixed-80.data -v # Record-at-a-time executor pipe-run-rat specs/filter-sales.pipe specs/input-fixed-80.data -v Given this input data: SMITH JOHN SALES 00050000 JONES MARY ENGINEER 00075000 DOE JANE SALES 00060000 WILSON ROBERT MARKETING 00055000 CHEN LISA ENGINEER 00080000 GARCIA CARLOS SALES 00045000 TAYLOR SUSAN MARKETING 00065000 BROWN MICHAEL ENGINEER 00090000 And this pipeline: PIPE CONSOLE | FILTER 18,10 = &quot;SALES&quot; | CONSOLE ? The output is: SMITH JOHN SALES 00050000 DOE JANE SALES 00060000 GARCIA CARLOS SALES 00045000 Records: 8 in -&gt; 3 out Exactly what I‚Äôd have gotten on OS/390 in 1996, but with Web Server log data showing client IP address, OS, browser type/version, user cookies, timestamps, URLs, and more, instead of accounting data. üòä The Web UI for Two pipelines-rs Implementations The web interface runs entirely in the browser via WebAssembly. It has three panels: input records with an 80-column ruler, the pipeline editor, and the output. Tutorial Mode The tutorial walks through each stage with examples, running pipelines automatically to show results. You can step through manually or let it auto-advance. The Visual Debugger The debugger is the reason RAT exists. It lets you: Step through execution one pipe point at a time Watch data at specific pipe points between stages Set breakpoints to pause at specific stages See stage state for stateful stages like COUNT You load a pipeline, click Run, then Step to watch each record flow through each stage. The debugger highlights which stages have been reached with a green border. For COUNT and other aggregation stages, you can watch the flush phase where accumulated state becomes output. What‚Äôs Next The current RAT executor is intentionally naive‚Äîit uses a buffer at every pipe point and copies each record between them. A better implementation would minimize buffers and copy operations while preserving the record-at-a-time semantics. Multi-pipe features are also planned‚ÄîCMS Pipelines supported fan-out (one input, multiple output streams) and fan-in (multiple inputs merged), which enabled complex processing topologies beyond simple linear chains. How pipelines-rs Differs from IBM Pipelines ¬† IBM CMS/TSO/E Pipelines pipelines-rs Indexing 1-based column positions 0-based offsets Record format Fixed or variable length, EBCDIC Fixed 80-byte ASCII only (variable-length input padded) Stages Hundreds of built-in stages 14 implemented so far Topology Multi-stream: fan-out, fan-in, multi-pipe Linear only (multi-pipe planned) Environment z/VM, z/OS mainframes CLI (native) and browser (WASM) Character set EBCDIC ASCII/UTF-8 This is a teaching tool and nostalgia project, not a production replacement. Implementation Details Metric Value Language Rust (2024 edition) Web UI Yew framework, compiled to WASM Stages 14 implemented Test Specs 23 pipeline specifications Tests 60+ (including batch/RAT equivalence) License MIT Live Demo sw-comp-history.github.io/pipelines-rs Resources pipelines-rs Repository Live Demo CMS Pipelines Reference (IBM) Credits Role Who Concept &amp; direction Mike Wright Content creation Claude (Anthropic) Editorial review ChatGPT (OpenAI) *Part 2 of the Throwback Thursday series. View all parts Next: TBT (3): Vector Graphics Games* Mainframe ideas, modern tools. Follow for more." />
<link rel="canonical" href="http://localhost:5907/2026/02/05/tbt-pipelines-os390/" />
<meta property="og:url" content="http://localhost:5907/2026/02/05/tbt-pipelines-os390/" />
<meta property="og:site_name" content="Software Wrighter Lab Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-02-05T12:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="TBT (2/?): Pipelines on OS/390" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Software Wrighter"},"dateModified":"2026-02-05T12:00:00-08:00","datePublished":"2026-02-05T12:00:00-08:00","description":"Unix invented pipes. Mainframes reinvented them‚Äîfor records, not bytes. This is the second Throwback Thursday post‚Äîrevisiting technologies that shaped how I think about programming. This time: CMS/TSO Pipelines, and a vibe coding project that brings them back to life in Rust for education, fun, and nostalgic reasons. Resource Link Code pipelines-rs Demo Live Demo Video Pipelines on OS/390 #TBT The 1996 Olympics and a Pair of Mainframes In 1996, IBM hosted the Olympics Web Server‚Äîone of the largest public web properties at the time. Many distributed IBM systems in different regions served dynamic web pages. The logs from all of them were funneled to a pair of IBM S/390 mainframes I was in charge of, running OS/390 (formerly MVS). When you‚Äôre processing millions of log records for statistics and forensics, you need tools that think in records, not lines. That‚Äôs where Pipelines for TSO/E came in. Pipelines for TSO/E was the MVS/ESA port of CMS Pipelines, which ran on VM/ESA. Both let you chain stages together to filter, transform, and aggregate record-oriented data‚Äîrecord-oriented pipelines that evolved in parallel with Unix‚Äôs byte-stream pipes. Two Traditions of Piping Unix pipes came first‚ÄîThompson and McIlroy at Bell Labs, 1969‚Äì1974. Byte streams, file descriptors, the | operator. Brutally simple. Explosively powerful. POSIX.1-1988 standardized pipe(2) and shell pipelines, though POSIX work began in the mid-1980s. CMS Pipelines emerged on IBM mainframes in the mid-to-late 1980s. They weren‚Äôt a Unix clone‚Äîthey were convergent evolution under different pressures. Where Unix piped bytes between small programs, CMS piped records through declarative stages. Pipelines for TSO/E followed in the late 1980s and early 1990s, porting CMS concepts to the MVS multi-user environment. Unlike CMS Pipelines (which ships with z/VM), the TSO/E port is typically installed separately on z/OS. Neither tradition was ‚Äúbehind.‚Äù They were optimizing different dimensions: ¬† Unix Pipes CMS/TSO Pipelines Era 1969‚Äì1974 Mid-to-late 1980s Data unit Byte stream Records (fixed or variable length) Stage input stdin (bytes) Record buffer Field access awk, cut (text parsing) Column positions (direct) Execution Typically a process per stage Stages in one address space Topology Linear by default; fan-out/fan-in via tee, FIFOs, or process substitution Multi-stream, fan-out/fan-in built in Philosophy Small tools, ad hoc composition Declarative data transformation Many datasets on mainframes are record-structured. Records can be fixed-length or variable-length. CMS and TSO/E Pipelines treat records as byte arrays‚Äîcharacter-oriented stages assume EBCDIC text, while position/length stages are binary-safe. A fixed-length 80-byte record isn‚Äôt arbitrary text‚Äîcolumns 1-8 are the name, 9-18 are the department, 19-26 are the salary. You don‚Äôt parse. You just read the right columns. Unix won culturally‚Äîcheap hardware, academic distribution, C portability. But IBM‚Äôs record-oriented pipelines were better at structured dataflow, and they anticipate or parallel patterns seen in ETL frameworks like Spark and Beam. CMS Pipelines ships with z/VM and is still used; Pipelines for TSO/E exists for z/OS but isn‚Äôt universally installed. These are not historical curiosities‚Äîmainframes continue to process a significant share of high-value transactions, and pipelines remain an available tool for data transformation on those systems. What a Pipeline Looks Like CMS Pipelines uses a DSL with PIPE as the command, | to chain stages, and ? as a command terminator (it suppresses the console from being used as implicit input): PIPE CONSOLE | FILTER 18,10 = &quot;SALES&quot; | SELECT 0,8,0; 8,10,8 | CONSOLE ? This reads input records, keeps only those where columns 18‚Äì27 equal ‚ÄúSALES‚Äù, extracts the name fields, and writes the result. No regex. No string splitting. Just column positions. Note: pipelines-rs uses 0-based offsets (e.g., SELECT 0,8,0). Historical CMS Pipelines uses 1-based column positions. Compare with the Unix equivalent: cat input.txt | awk &#39;$3 == &quot;SALES&quot; {print $1, $2}&#39; The Unix version looks simpler‚Äîuntil your fields contain spaces, or your records contain non-text bytes, or you need to chain 15 stages without spawning 15 processes. Bringing It Back in Rust (Vibe Coding) pipelines-rs is a nostalgia-driven vibe coding project‚Äîmy attempt to emulate Pipelines for TSO/E in Rust, not because it‚Äôs practical, but because these ideas deserve to be celebrated. It supports a subset of stages and features two execution models: The Two Executors Batched processes all records through one stage before moving to the next: All records ‚Üí Stage 1 ‚Üí All records ‚Üí Stage 2 ‚Üí All records ‚Üí Stage 3 This emulates the correct output and is faster, but doesn‚Äôt demonstrate record-oriented dataflow well. Record-At-a-Time (RAT) sends each record through the entire pipeline before reading the next: Record 1 ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí Stage 3 ‚Üí Output Record 2 ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí Stage 3 ‚Üí Output Record 3 ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí Stage 3 ‚Üí Output RAT is the implementation shown in the video. It‚Äôs a naive approach‚Äîmore buffers, more copying‚Äîbut it shows the dataflow concepts clearly and enables the visual debugger. Both run in linear time (records √ó stages) and produce identical output for all 23 test specifications. A future version will aim for fewer buffers and fewer copy operations. Whether it‚Äôs faster than Batched remains to be seen. The 80-Byte Record The Rust implementation supports fixed-length records only. The fundamental data type is the Record‚Äîexactly 80 bytes, matching historical punch card width. Variable-length input lines are accepted and padded to 80 bytes: pub const RECORD_WIDTH: usize = 80; pub struct Record { data: [u8; RECORD_WIDTH], } Fields are accessed by column position and length. No parsing, no delimiters. The data is always right where you expect it. Supported Stages The current implementation supports 14 stages: Stage Purpose Example FILTER Keep/reject records by field value FILTER 18,10 = &quot;SALES&quot; LOCATE Keep records containing a pattern LOCATE &quot;ERROR&quot; NLOCATE Keep records NOT containing a pattern NLOCATE &quot;DEBUG&quot; SELECT Extract and reposition fields SELECT 0,8,0; 8,10,8 CHANGE Text replacement CHANGE &quot;SALES&quot; &quot;MKTG&quot; COUNT Count records COUNT TAKE Keep first N records TAKE 5 SKIP Skip first N records SKIP 2 DUPLICATE Repeat each record N times DUPLICATE 3 LITERAL Append a literal record LITERAL &quot;--- END ---&quot; UPPER/LOWER Case conversion UPPER REVERSE Reverse record text REVERSE HOLE Discard all input HOLE CONSOLE Driver stage: source or sink depending on position CONSOLE The CLI Both executors have identical CLIs: # Batch executor pipe-run specs/filter-sales.pipe specs/input-fixed-80.data -v # Record-at-a-time executor pipe-run-rat specs/filter-sales.pipe specs/input-fixed-80.data -v Given this input data: SMITH JOHN SALES 00050000 JONES MARY ENGINEER 00075000 DOE JANE SALES 00060000 WILSON ROBERT MARKETING 00055000 CHEN LISA ENGINEER 00080000 GARCIA CARLOS SALES 00045000 TAYLOR SUSAN MARKETING 00065000 BROWN MICHAEL ENGINEER 00090000 And this pipeline: PIPE CONSOLE | FILTER 18,10 = &quot;SALES&quot; | CONSOLE ? The output is: SMITH JOHN SALES 00050000 DOE JANE SALES 00060000 GARCIA CARLOS SALES 00045000 Records: 8 in -&gt; 3 out Exactly what I‚Äôd have gotten on OS/390 in 1996, but with Web Server log data showing client IP address, OS, browser type/version, user cookies, timestamps, URLs, and more, instead of accounting data. üòä The Web UI for Two pipelines-rs Implementations The web interface runs entirely in the browser via WebAssembly. It has three panels: input records with an 80-column ruler, the pipeline editor, and the output. Tutorial Mode The tutorial walks through each stage with examples, running pipelines automatically to show results. You can step through manually or let it auto-advance. The Visual Debugger The debugger is the reason RAT exists. It lets you: Step through execution one pipe point at a time Watch data at specific pipe points between stages Set breakpoints to pause at specific stages See stage state for stateful stages like COUNT You load a pipeline, click Run, then Step to watch each record flow through each stage. The debugger highlights which stages have been reached with a green border. For COUNT and other aggregation stages, you can watch the flush phase where accumulated state becomes output. What‚Äôs Next The current RAT executor is intentionally naive‚Äîit uses a buffer at every pipe point and copies each record between them. A better implementation would minimize buffers and copy operations while preserving the record-at-a-time semantics. Multi-pipe features are also planned‚ÄîCMS Pipelines supported fan-out (one input, multiple output streams) and fan-in (multiple inputs merged), which enabled complex processing topologies beyond simple linear chains. How pipelines-rs Differs from IBM Pipelines ¬† IBM CMS/TSO/E Pipelines pipelines-rs Indexing 1-based column positions 0-based offsets Record format Fixed or variable length, EBCDIC Fixed 80-byte ASCII only (variable-length input padded) Stages Hundreds of built-in stages 14 implemented so far Topology Multi-stream: fan-out, fan-in, multi-pipe Linear only (multi-pipe planned) Environment z/VM, z/OS mainframes CLI (native) and browser (WASM) Character set EBCDIC ASCII/UTF-8 This is a teaching tool and nostalgia project, not a production replacement. Implementation Details Metric Value Language Rust (2024 edition) Web UI Yew framework, compiled to WASM Stages 14 implemented Test Specs 23 pipeline specifications Tests 60+ (including batch/RAT equivalence) License MIT Live Demo sw-comp-history.github.io/pipelines-rs Resources pipelines-rs Repository Live Demo CMS Pipelines Reference (IBM) Credits Role Who Concept &amp; direction Mike Wright Content creation Claude (Anthropic) Editorial review ChatGPT (OpenAI) *Part 2 of the Throwback Thursday series. View all parts Next: TBT (3): Vector Graphics Games* Mainframe ideas, modern tools. Follow for more.","headline":"TBT (2/?): Pipelines on OS/390","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:5907/2026/02/05/tbt-pipelines-os390/"},"url":"http://localhost:5907/2026/02/05/tbt-pipelines-os390/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:5907/feed.xml" title="Software Wrighter Lab Blog" /><!-- Theme toggle script - load early to prevent flash -->
  <script>
    (function() {
      var stored = localStorage.getItem('sw-lab-theme');
      var theme = stored;
      if (!theme) {
        theme = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
      }
      document.documentElement.setAttribute('data-theme', theme);
    })();
  </script>
  <!-- Font size - load early to prevent flash -->
  <script>
    (function() {
      var NEW_DEFAULT = 110;
      var OLD_DEFAULT = 150;
      var stored = localStorage.getItem('sw-lab-font-size');
      var ack = localStorage.getItem('sw-lab-prefs-ack');
      var size = stored ? parseInt(stored, 10) : null;

      // Migration: if on old default and not acknowledged, use new default
      if (size === OLD_DEFAULT && ack !== 'true') {
        size = NEW_DEFAULT;
      } else if (size === null) {
        size = NEW_DEFAULT;
      }

      // Apply to post content when DOM is ready
      document.addEventListener('DOMContentLoaded', function() {
        var pc = document.querySelector('.post-content');
        if (pc) pc.style.fontSize = size + '%';
        var pl = document.querySelector('.post-list');
        if (pl) pl.style.fontSize = size + '%';
      });
    })();
  </script>
  <script src="/assets/js/theme-toggle.js" defer></script>
  <script src="/assets/js/font-size.js" defer></script>
  <script src="/assets/js/preferences.js" defer></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">
      <img src="/assets/images/site/logo.jpg" alt="Software Wrighter Lab Blog" class="site-logo">
      <span class="site-title-text">Software Wrighter Lab Blog</span>
    </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/abstracts/">Abstracts</a><a class="page-link" href="/index-all/">Index</a><a class="page-link" href="/series/">Series</a><a class="page-link" href="/tags/">Tags</a><a class="page-link" href="/categories/">Categories</a><a class="page-link" href="/search/">Search</a><div class="font-size-controls">
  <button class="font-size-btn" id="font-decrease" title="Decrease font size" aria-label="Decrease font size">A-</button>
  <button class="font-size-btn" id="font-reset" title="Reset font size" aria-label="Reset font size">A</button>
  <button class="font-size-btn" id="font-increase" title="Increase font size" aria-label="Increase font size">A+</button>
</div>
<button class="theme-toggle" aria-label="Switch theme" title="Switch theme">
  <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
  </svg>
  <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
  </svg>
</button>
</div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">TBT (2/?): Pipelines on OS/390</h1><p class="post-meta">February 5, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">1803 words</span> &bull; <span class="post-read-time">10 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Unix invented pipes. Mainframes reinvented them for records, not bytes. This Throwback Thursday recreates CMS/TSO Pipelines in Rust with a visual debugger, demonstrating record-oriented dataflow from the 1996 Olympics web server era.</div><div class="post-taxonomies"><span class="post-categories"><a href="/categories/#tbt" class="category">tbt</a><a href="/categories/#programming-history" class="category">programming-history</a></span><span class="post-tags"><a href="/tags/#pipelines" class="tag">pipelines</a><a href="/tags/#throwback-thursday" class="tag">throwback-thursday</a><a href="/tags/#retrocomputing" class="tag">retrocomputing</a><a href="/tags/#ibm" class="tag">ibm</a><a href="/tags/#mainframe" class="tag">mainframe</a><a href="/tags/#rust" class="tag">rust</a><a href="/tags/#os390" class="tag">os390</a><a href="/tags/#mvs" class="tag">mvs</a><a href="/tags/#cms" class="tag">cms</a><a href="/tags/#vibe-coding" class="tag">vibe-coding</a></span></div></header><nav class="toc" data-toc-id="default">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content e-content" itemprop="articleBody">
    <p><img src="/assets/images/posts/pipes-bw.png" class="post-marker" alt="" /></p>

<p>Unix invented pipes. Mainframes reinvented them‚Äîfor records, not bytes.</p>

<p>This is the second <strong>Throwback Thursday</strong> post‚Äîrevisiting technologies that shaped how I think about programming. This time: CMS/TSO Pipelines, and a vibe coding project that brings them back to life in Rust for education, fun, and nostalgic reasons.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/sw-comp-history/pipelines-rs">pipelines-rs</a></td>
      </tr>
      <tr>
        <td><strong>Demo</strong></td>
        <td><a href="https://sw-comp-history.github.io/pipelines-rs/">Live Demo</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://youtu.be/872RLMBzC_8">Pipelines on OS/390 #TBT</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-1996-olympics-and-a-pair-of-mainframes">The 1996 Olympics and a Pair of Mainframes</h2>

<p>In 1996, IBM hosted the Olympics Web Server‚Äîone of the largest public web properties at the time. Many distributed IBM systems in different regions served dynamic web pages. The logs from all of them were funneled to a pair of <strong>IBM S/390 mainframes</strong> I was in charge of, running <strong>OS/390</strong> (formerly MVS).</p>

<p>When you‚Äôre processing millions of log records for statistics and forensics, you need tools that think in records, not lines. That‚Äôs where <strong>Pipelines for TSO/E</strong> came in.</p>

<p>Pipelines for TSO/E was the MVS/ESA port of <strong>CMS Pipelines</strong>, which ran on VM/ESA. Both let you chain stages together to filter, transform, and aggregate record-oriented data‚Äîrecord-oriented pipelines that evolved in parallel with Unix‚Äôs byte-stream pipes.</p>

<h2 id="two-traditions-of-piping">Two Traditions of Piping</h2>

<p>Unix pipes came first‚ÄîThompson and McIlroy at Bell Labs, 1969‚Äì1974. Byte streams, file descriptors, the <code class="language-plaintext highlighter-rouge">|</code> operator. Brutally simple. Explosively powerful. POSIX.1-1988 standardized <code class="language-plaintext highlighter-rouge">pipe(2)</code> and shell pipelines, though POSIX work began in the mid-1980s.</p>

<p>CMS Pipelines emerged on IBM mainframes in the mid-to-late 1980s. They weren‚Äôt a Unix clone‚Äîthey were convergent evolution under different pressures. Where Unix piped bytes between small programs, CMS piped <strong>records</strong> through declarative stages. Pipelines for TSO/E followed in the late 1980s and early 1990s, porting CMS concepts to the MVS multi-user environment. Unlike CMS Pipelines (which ships with z/VM), the TSO/E port is typically installed separately on z/OS.</p>

<p>Neither tradition was ‚Äúbehind.‚Äù They were optimizing different dimensions:</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>Unix Pipes</th>
      <th>CMS/TSO Pipelines</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Era</strong></td>
      <td>1969‚Äì1974</td>
      <td>Mid-to-late 1980s</td>
    </tr>
    <tr>
      <td><strong>Data unit</strong></td>
      <td>Byte stream</td>
      <td>Records (fixed or variable length)</td>
    </tr>
    <tr>
      <td><strong>Stage input</strong></td>
      <td>stdin (bytes)</td>
      <td>Record buffer</td>
    </tr>
    <tr>
      <td><strong>Field access</strong></td>
      <td><code class="language-plaintext highlighter-rouge">awk</code>, <code class="language-plaintext highlighter-rouge">cut</code> (text parsing)</td>
      <td>Column positions (direct)</td>
    </tr>
    <tr>
      <td><strong>Execution</strong></td>
      <td>Typically a process per stage</td>
      <td>Stages in one address space</td>
    </tr>
    <tr>
      <td><strong>Topology</strong></td>
      <td>Linear by default; fan-out/fan-in via <code class="language-plaintext highlighter-rouge">tee</code>, FIFOs, or process substitution</td>
      <td>Multi-stream, fan-out/fan-in built in</td>
    </tr>
    <tr>
      <td><strong>Philosophy</strong></td>
      <td>Small tools, ad hoc composition</td>
      <td>Declarative data transformation</td>
    </tr>
  </tbody>
</table>

<p>Many datasets on mainframes are record-structured. Records can be fixed-length or variable-length. CMS and TSO/E Pipelines treat records as byte arrays‚Äîcharacter-oriented stages assume EBCDIC text, while position/length stages are binary-safe. A fixed-length 80-byte record isn‚Äôt arbitrary text‚Äîcolumns 1-8 are the name, 9-18 are the department, 19-26 are the salary. You don‚Äôt parse. You just read the right columns.</p>

<p>Unix won culturally‚Äîcheap hardware, academic distribution, C portability. But IBM‚Äôs record-oriented pipelines were better at structured dataflow, and they anticipate or parallel patterns seen in ETL frameworks like Spark and Beam.</p>

<p>CMS Pipelines ships with z/VM and is still used; Pipelines for TSO/E exists for z/OS but isn‚Äôt universally installed. These are not historical curiosities‚Äîmainframes continue to process a significant share of high-value transactions, and pipelines remain an available tool for data transformation on those systems.</p>

<h2 id="what-a-pipeline-looks-like">What a Pipeline Looks Like</h2>

<p>CMS Pipelines uses a DSL with <code class="language-plaintext highlighter-rouge">PIPE</code> as the command, <code class="language-plaintext highlighter-rouge">|</code> to chain stages, and <code class="language-plaintext highlighter-rouge">?</code> as a command terminator (it suppresses the console from being used as implicit input):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PIPE CONSOLE
| FILTER 18,10 = "SALES"
| SELECT 0,8,0; 8,10,8
| CONSOLE
?
</code></pre></div></div>

<p>This reads input records, keeps only those where columns 18‚Äì27 equal ‚ÄúSALES‚Äù, extracts the name fields, and writes the result. No regex. No string splitting. Just column positions.</p>

<p><em>Note: pipelines-rs uses 0-based offsets (e.g., <code class="language-plaintext highlighter-rouge">SELECT 0,8,0</code>). Historical CMS Pipelines uses 1-based column positions.</em></p>

<p>Compare with the Unix equivalent:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cat </span>input.txt | <span class="nb">awk</span> <span class="s1">'$3 == "SALES" {print $1, $2}'</span>
</code></pre></div></div>

<p>The Unix version looks simpler‚Äîuntil your fields contain spaces, or your records contain non-text bytes, or you need to chain 15 stages without spawning 15 processes.</p>

<h2 id="bringing-it-back-in-rust-vibe-coding">Bringing It Back in Rust (Vibe Coding)</h2>

<p><strong>pipelines-rs</strong> is a nostalgia-driven vibe coding project‚Äîmy attempt to emulate Pipelines for TSO/E in Rust, not because it‚Äôs practical, but because these ideas deserve to be celebrated. It supports a subset of stages and features two execution models:</p>

<h3 id="the-two-executors">The Two Executors</h3>

<p><strong>Batched</strong> processes all records through one stage before moving to the next:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>All records ‚Üí Stage 1 ‚Üí All records ‚Üí Stage 2 ‚Üí All records ‚Üí Stage 3
</code></pre></div></div>

<p>This emulates the correct output and is faster, but doesn‚Äôt demonstrate record-oriented dataflow well.</p>

<p><strong>Record-At-a-Time (RAT)</strong> sends each record through the entire pipeline before reading the next:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Record 1 ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí Stage 3 ‚Üí Output
Record 2 ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí Stage 3 ‚Üí Output
Record 3 ‚Üí Stage 1 ‚Üí Stage 2 ‚Üí Stage 3 ‚Üí Output
</code></pre></div></div>

<p>RAT is the implementation shown in the video. It‚Äôs a naive approach‚Äîmore buffers, more copying‚Äîbut it shows the dataflow concepts clearly and enables the visual debugger. Both run in linear time (records √ó stages) and produce identical output for all 23 test specifications.</p>

<p>A future version will aim for fewer buffers and fewer copy operations. Whether it‚Äôs faster than Batched remains to be seen.</p>

<h3 id="the-80-byte-record">The 80-Byte Record</h3>

<p>The Rust implementation supports fixed-length records only. The fundamental data type is the <code class="language-plaintext highlighter-rouge">Record</code>‚Äîexactly 80 bytes, matching historical punch card width. Variable-length input lines are accepted and padded to 80 bytes:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">const</span> <span class="n">RECORD_WIDTH</span><span class="p">:</span> <span class="nb">usize</span> <span class="o">=</span> <span class="mi">80</span><span class="p">;</span>

<span class="k">pub</span> <span class="k">struct</span> <span class="n">Record</span> <span class="p">{</span>
    <span class="n">data</span><span class="p">:</span> <span class="p">[</span><span class="nb">u8</span><span class="p">;</span> <span class="n">RECORD_WIDTH</span><span class="p">],</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Fields are accessed by column position and length. No parsing, no delimiters. The data is always right where you expect it.</p>

<h3 id="supported-stages">Supported Stages</h3>

<p>The current implementation supports 14 stages:</p>

<table>
  <thead>
    <tr>
      <th>Stage</th>
      <th>Purpose</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>FILTER</strong></td>
      <td>Keep/reject records by field value</td>
      <td><code class="language-plaintext highlighter-rouge">FILTER 18,10 = "SALES"</code></td>
    </tr>
    <tr>
      <td><strong>LOCATE</strong></td>
      <td>Keep records containing a pattern</td>
      <td><code class="language-plaintext highlighter-rouge">LOCATE "ERROR"</code></td>
    </tr>
    <tr>
      <td><strong>NLOCATE</strong></td>
      <td>Keep records NOT containing a pattern</td>
      <td><code class="language-plaintext highlighter-rouge">NLOCATE "DEBUG"</code></td>
    </tr>
    <tr>
      <td><strong>SELECT</strong></td>
      <td>Extract and reposition fields</td>
      <td><code class="language-plaintext highlighter-rouge">SELECT 0,8,0; 8,10,8</code></td>
    </tr>
    <tr>
      <td><strong>CHANGE</strong></td>
      <td>Text replacement</td>
      <td><code class="language-plaintext highlighter-rouge">CHANGE "SALES" "MKTG"</code></td>
    </tr>
    <tr>
      <td><strong>COUNT</strong></td>
      <td>Count records</td>
      <td><code class="language-plaintext highlighter-rouge">COUNT</code></td>
    </tr>
    <tr>
      <td><strong>TAKE</strong></td>
      <td>Keep first N records</td>
      <td><code class="language-plaintext highlighter-rouge">TAKE 5</code></td>
    </tr>
    <tr>
      <td><strong>SKIP</strong></td>
      <td>Skip first N records</td>
      <td><code class="language-plaintext highlighter-rouge">SKIP 2</code></td>
    </tr>
    <tr>
      <td><strong>DUPLICATE</strong></td>
      <td>Repeat each record N times</td>
      <td><code class="language-plaintext highlighter-rouge">DUPLICATE 3</code></td>
    </tr>
    <tr>
      <td><strong>LITERAL</strong></td>
      <td>Append a literal record</td>
      <td><code class="language-plaintext highlighter-rouge">LITERAL "--- END ---"</code></td>
    </tr>
    <tr>
      <td><strong>UPPER/LOWER</strong></td>
      <td>Case conversion</td>
      <td><code class="language-plaintext highlighter-rouge">UPPER</code></td>
    </tr>
    <tr>
      <td><strong>REVERSE</strong></td>
      <td>Reverse record text</td>
      <td><code class="language-plaintext highlighter-rouge">REVERSE</code></td>
    </tr>
    <tr>
      <td><strong>HOLE</strong></td>
      <td>Discard all input</td>
      <td><code class="language-plaintext highlighter-rouge">HOLE</code></td>
    </tr>
    <tr>
      <td><strong>CONSOLE</strong></td>
      <td>Driver stage: source or sink depending on position</td>
      <td><code class="language-plaintext highlighter-rouge">CONSOLE</code></td>
    </tr>
  </tbody>
</table>

<h2 id="the-cli">The CLI</h2>

<p>Both executors have identical CLIs:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Batch executor</span>
pipe-run specs/filter-sales.pipe specs/input-fixed-80.data <span class="nt">-v</span>

<span class="c"># Record-at-a-time executor</span>
pipe-run-rat specs/filter-sales.pipe specs/input-fixed-80.data <span class="nt">-v</span>
</code></pre></div></div>

<p>Given this input data:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SMITH   JOHN      SALES     00050000
JONES   MARY      ENGINEER  00075000
DOE     JANE      SALES     00060000
WILSON  ROBERT    MARKETING 00055000
CHEN    LISA      ENGINEER  00080000
GARCIA  CARLOS    SALES     00045000
TAYLOR  SUSAN     MARKETING 00065000
BROWN   MICHAEL   ENGINEER  00090000
</code></pre></div></div>

<p>And this pipeline:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>PIPE CONSOLE
| FILTER 18,10 = "SALES"
| CONSOLE
?
</code></pre></div></div>

<p>The output is:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SMITH   JOHN      SALES     00050000
DOE     JANE      SALES     00060000
GARCIA  CARLOS    SALES     00045000
Records:  8 in -&gt; 3 out
</code></pre></div></div>

<p>Exactly what I‚Äôd have gotten on OS/390 in 1996, but with Web Server log data showing client IP address, OS, browser type/version, user cookies, timestamps, URLs, and more, instead of accounting data. üòä</p>

<h2 id="the-web-ui-for-two-pipelines-rs-implementations">The Web UI for Two pipelines-rs Implementations</h2>

<p>The web interface runs entirely in the browser via WebAssembly. It has three panels: input records with an 80-column ruler, the pipeline editor, and the output.</p>

<h3 id="tutorial-mode">Tutorial Mode</h3>

<p>The tutorial walks through each stage with examples, running pipelines automatically to show results. You can step through manually or let it auto-advance.</p>

<h3 id="the-visual-debugger">The Visual Debugger</h3>

<p>The debugger is the reason RAT exists. It lets you:</p>

<ul>
  <li><strong>Step</strong> through execution one pipe point at a time</li>
  <li><strong>Watch</strong> data at specific pipe points between stages</li>
  <li><strong>Set breakpoints</strong> to pause at specific stages</li>
  <li><strong>See stage state</strong> for stateful stages like COUNT</li>
</ul>

<p>You load a pipeline, click Run, then Step to watch each record flow through each stage. The debugger highlights which stages have been reached with a green border. For COUNT and other aggregation stages, you can watch the flush phase where accumulated state becomes output.</p>

<h2 id="whats-next">What‚Äôs Next</h2>

<p>The current RAT executor is intentionally naive‚Äîit uses a buffer at every pipe point and copies each record between them. A better implementation would minimize buffers and copy operations while preserving the record-at-a-time semantics.</p>

<p>Multi-pipe features are also planned‚ÄîCMS Pipelines supported fan-out (one input, multiple output streams) and fan-in (multiple inputs merged), which enabled complex processing topologies beyond simple linear chains.</p>

<h2 id="how-pipelines-rs-differs-from-ibm-pipelines">How pipelines-rs Differs from IBM Pipelines</h2>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>IBM CMS/TSO/E Pipelines</th>
      <th>pipelines-rs</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Indexing</strong></td>
      <td>1-based column positions</td>
      <td>0-based offsets</td>
    </tr>
    <tr>
      <td><strong>Record format</strong></td>
      <td>Fixed or variable length, EBCDIC</td>
      <td>Fixed 80-byte ASCII only (variable-length input padded)</td>
    </tr>
    <tr>
      <td><strong>Stages</strong></td>
      <td>Hundreds of built-in stages</td>
      <td>14 implemented so far</td>
    </tr>
    <tr>
      <td><strong>Topology</strong></td>
      <td>Multi-stream: fan-out, fan-in, multi-pipe</td>
      <td>Linear only (multi-pipe planned)</td>
    </tr>
    <tr>
      <td><strong>Environment</strong></td>
      <td>z/VM, z/OS mainframes</td>
      <td>CLI (native) and browser (WASM)</td>
    </tr>
    <tr>
      <td><strong>Character set</strong></td>
      <td>EBCDIC</td>
      <td>ASCII/UTF-8</td>
    </tr>
  </tbody>
</table>

<p>This is a teaching tool and nostalgia project, not a production replacement.</p>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Language</strong></td>
      <td>Rust (2024 edition)</td>
    </tr>
    <tr>
      <td><strong>Web UI</strong></td>
      <td>Yew framework, compiled to WASM</td>
    </tr>
    <tr>
      <td><strong>Stages</strong></td>
      <td>14 implemented</td>
    </tr>
    <tr>
      <td><strong>Test Specs</strong></td>
      <td>23 pipeline specifications</td>
    </tr>
    <tr>
      <td><strong>Tests</strong></td>
      <td>60+ (including batch/RAT equivalence)</td>
    </tr>
    <tr>
      <td><strong>License</strong></td>
      <td>MIT</td>
    </tr>
    <tr>
      <td><strong>Live Demo</strong></td>
      <td><a href="https://sw-comp-history.github.io/pipelines-rs/">sw-comp-history.github.io/pipelines-rs</a></td>
    </tr>
  </tbody>
</table>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://github.com/sw-comp-history/pipelines-rs">pipelines-rs Repository</a></li>
  <li><a href="https://sw-comp-history.github.io/pipelines-rs/">Live Demo</a></li>
  <li><a href="https://www.ibm.com/docs/en/zvm/7.3?topic=reference-cms-pipelines">CMS Pipelines Reference (IBM)</a></li>
</ul>

<h2 id="credits">Credits</h2>

<table>
  <thead>
    <tr>
      <th>Role</th>
      <th>Who</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Concept &amp; direction</strong></td>
      <td>Mike Wright</td>
    </tr>
    <tr>
      <td><strong>Content creation</strong></td>
      <td>Claude (Anthropic)</td>
    </tr>
    <tr>
      <td><strong>Editorial review</strong></td>
      <td>ChatGPT (OpenAI)</td>
    </tr>
  </tbody>
</table>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 2 of the Throwback Thursday series. <a href="/series/#throwback-thursday">View all parts</a></td>
      <td>Next: <a href="/2026/02/13/tbt-vector-graphics-games/">TBT (3): Vector Graphics Games</a>*</td>
    </tr>
  </tbody>
</table>

<p><em>Mainframe ideas, modern tools. Follow for more.</em></p>

  </div><div class="series-nav">
    <p><em>Part 2 of the Throwback Thursday series. <a href="/series/#throwback-thursday">View all parts</a> | <a href="/2026/02/12/tbt-vector-graphics-games/">Next: Part 3 ‚Üí</a></em></p>
  </div>





<div class="youtube-embed-container" id="yt-container-872RLMBzC_8">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-872RLMBzC_8"
      src="https://www.youtube.com/embed/872RLMBzC_8?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-872RLMBzC_8';
  const playerId = 'yt-player-872RLMBzC_8';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt=""><a class="u-url" href="/2026/02/05/tbt-pipelines-os390/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Software Wrighter Lab Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Mike Wright</li><li><a class="u-email" href="mailto:"></a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/softwarewrighter"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">softwarewrighter</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>AI coding agents, systems programming, and practical machine learning</p>
      </div>
    </div>

    <div class="footer-copyright">
      <p>Copyright &copy; 2026 Michael A. Wright</p>
    </div>

  </div>

  <button id="scroll-to-top" aria-label="Scroll to top" title="Scroll to top">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <polyline points="18 15 12 9 6 15"></polyline>
    </svg>
  </button>

  <style>
  #scroll-to-top {
    position: fixed;
    bottom: 2rem;
    right: 2rem;
    width: 44px;
    height: 44px;
    border-radius: 50%;
    border: none;
    background: var(--brand-color, #2a7ae2);
    color: white;
    cursor: pointer;
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.3s, visibility 0.3s, transform 0.2s;
    z-index: 1000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
  }

  #scroll-to-top:hover {
    transform: scale(1.1);
  }

  #scroll-to-top.visible {
    opacity: 1;
    visibility: visible;
  }

  #scroll-to-top svg {
    width: 24px;
    height: 24px;
  }
  </style>

  <script>
  (function() {
    const btn = document.getElementById('scroll-to-top');
    if (!btn) return;

    window.addEventListener('scroll', function() {
      if (window.scrollY > 300) {
        btn.classList.add('visible');
      } else {
        btn.classList.remove('visible');
      }
    });

    btn.addEventListener('click', function() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    });
  })();
  </script>

</footer>
</body>

</html>
