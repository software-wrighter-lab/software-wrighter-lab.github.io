<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Solving Sparse Rewards with Many Eyes | Software Wrighter Lab Blog</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Solving Sparse Rewards with Many Eyes" />
<meta name="author" content="Software Wrighter" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Single explorer: 0% success. Five explorers: 60% success. Learning often fails not because models are slow, but because they see too little. In sparse-reward environments, a single explorer is likely to miss the rare feedback entirely. The solution? Put many eyes on the problem. Resource Link Related Intrinsic Rewards and Diversity Papers IRPO · Reagent Code many-eyes-learning ELI5 eli5.md Video Given enough eyeballs… The Problem: Sparse Rewards Create Blindness As IRPO formalizes: in sparse-reward RL, the true policy gradient is basically uninformative most of the time. No reward signal → no gradient signal. A 7x7 grid with a single goal demonstrates this perfectly: Random agent success rate: ~9% With limited training (75 episodes), a single learner exploring alone never finds the goal This isn’t a compute problem. It’s an information problem. Challenge Effect Paper Connection Rare rewards Weak gradient signal IRPO’s core problem statement Single explorer Limited coverage Why multiple scouts help Random exploration Misses valuable states Why intrinsic rewards matter No feedback structure Can’t distinguish “almost right” from “nonsense” Reagent’s motivation The Solution: Many Eyes Instead of one explorer, use multiple scouts—independent exploratory agents that gather diverse information. ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ Scout 1 │ │ Scout 2 │ │ Scout N │ │ (strategy A)│ │ (strategy B)│ │ (strategy N)│ └──────┬──────┘ └──────┬──────┘ └──────┬──────┘ │ │ │ v v v ┌─────────────────────────────────────────────────┐ │ Experience Buffer │ └─────────────────────────────────────────────────┘ │ v ┌─────────────────────────────────────────────────┐ │ Shared Learner │ └─────────────────────────────────────────────────┘ Each scout explores with its own strategy. Their discoveries are aggregated to improve a shared learner. Results On a 7x7 sparse grid with 75 training episodes: Method Success Rate Random baseline 9% Single scout 0% Many eyes (3 scouts) 40% Many eyes (5 scouts) 60% Same total environment steps. Dramatically better outcomes. Why It Works Single Scout Fails Because: In IRPO terms: sparse reward → sparse gradient signal → no learning. Random exploration rarely reaches the goal (~9%) Insufficient successful trajectories DQN can’t learn from sparse positive examples The policy gradient has near-zero magnitude Many Eyes Succeeds Because: IRPO’s key insight: multiple exploratory policies manufacture signal. More coverage: Different scouts explore different regions (intrinsic rewards drive novelty-seeking) More discoveries: Higher probability of reaching goal (scouts find extrinsic reward) Signal routing: Scout discoveries update the shared learner (surrogate gradient in IRPO, experience pooling in many-eyes) Better gradients: Aggregated experience provides meaningful learning signal Scout Strategies (Intrinsic Rewards) IRPO uses intrinsic rewards to drive exploration. The many-eyes-learning project implements several strategies: Strategy Intrinsic Motivation IRPO Connection Epsilon-greedy Random action with probability ε Simple exploration noise Curious Bonus for novel states: 1/√(count+1) Count-based intrinsic reward Optimistic High initial Q-values Optimism under uncertainty Random Pure random baseline Maximum entropy exploration # CuriousScout intrinsic reward (simplified) def intrinsic_reward(self, state): count = self.state_counts[state] return self.bonus_scale / sqrt(count + 1) Scouts can be homogeneous (same strategy, different seeds) or heterogeneous (different strategies). IRPO supports swapping intrinsic reward functions—many-eyes makes this concrete with pluggable scout types. Running the Demo git clone https://github.com/softwarewrighter/many-eyes-learning cd many-eyes-learning # Setup uv venv .venv source .venv/bin/activate uv pip install -e &quot;.[dev]&quot; # Interactive CLI demo python experiments/cli_demo.py # Full experiment python experiments/run_experiment.py --episodes 75 --scouts 1 3 5 # Generate plots python experiments/plot_results.py Results appear in ~5-10 minutes on a laptop. Diversity Experiment Does diversity of strategies matter, or just number of scouts? Configuration Success Rate 5 random scouts 20% 5 epsilon-greedy scouts 40% 5 diverse scouts (mixed strategies) 40% Finding: In simple environments, strategy quality matters more than diversity. Epsilon-greedy beats random regardless of diversity. Key Insight The problem isn’t that learning is slow. The problem is that learning is blind. Many eyes make learning better. Implementation Details Metric Value Primary Language Python Source Files ~12 .py files Estimated Size ~1.5 KLOC Framework PyTorch, NumPy Platform CPU (no GPU required) Good for you if: You want to understand exploration in RL, experiment with sparse-reward environments, or see a clean implementation of scout-based learning. Complexity: Low-Moderate. Clean codebase with CLI demos. Runs on a laptop in minutes. Design Philosophy The project prioritizes clarity over performance: Single-file implementations where practical Minimal dependencies Sequential mode is first-class (parallel optional) Reproducible experiments with fixed seeds Simplifications from IRPO Full IRPO computes Jacobians to route gradients from exploratory policies back to the base policy. Many-eyes-learning simplifies this: IRPO Many-Eyes-Learning Jacobian chain rule Experience pooling Surrogate gradient Standard DQN updates Learned intrinsic rewards Hand-designed strategies The core insight remains: scouts explore with intrinsic motivation, discoveries benefit the shared learner. The math is simpler, the demo runs on a laptop, and the concept is clear. Key Takeaways Sparse rewards create information bottlenecks. Learning fails not from lack of compute, but lack of signal. More eyes = more information. Multiple scouts increase coverage and discovery rate. Diversity helps, but quality matters more. In simple environments, good exploration strategy beats diversity. Same compute, better outcomes. Many-eyes improves sample efficiency, not wall-clock speed. The Papers Behind Many-Eyes This project builds on two recent papers that attack the same fundamental problem: sparse rewards starve learning of signal. IRPO: Intrinsic Reward Policy Optimization IRPO (Cho &amp; Tran, UIUC) formalizes the scouts concept mathematically. The core insight: In sparse-reward RL, the true policy gradient is basically uninformative most of the time. No reward signal → no gradient signal. Learning stalls. IRPO’s solution: ┌─────────────────────────────────────────────────┐ │ 1. Train exploratory policies (scouts) │ │ using INTRINSIC rewards │ ├─────────────────────────────────────────────────┤ │ 2. Scouts discover EXTRINSIC rewards │ │ through exploration │ ├─────────────────────────────────────────────────┤ │ 3. Route extrinsic signal back to base policy │ │ via surrogate gradient (Jacobian chain) │ └─────────────────────────────────────────────────┘ IRPO Concept What It Means Intrinsic rewards “Explore what’s new” - reward novelty Exploratory policies Scouts driven by intrinsic motivation Surrogate gradient Trade bias for signal - approximate gradient that actually has magnitude Base policy The learner that benefits from scout discoveries How many-eyes-learning demonstrates this: Scouts implement intrinsic motivation (CuriousScout uses count-based novelty bonuses) Multiple exploration strategies create diverse coverage Aggregated experience routes discoveries to the shared DQN learner Simplified gradient routing - we pool experiences rather than compute full Jacobians Reagent: Reasoning Reward Models for Agents Reagent (Fan et al., CUHK/Meituan) takes a different approach: make feedback richer and more structured. The problem with sparse rewards: They can’t distinguish “almost right, failed at the end” from “complete nonsense.” Both get the same zero reward. Reagent’s solution: Build a Reasoning Reward Model that emits: Signal Purpose &lt;think&gt; Explicit reasoning trace &lt;critique&gt; Targeted natural-language feedback &lt;score&gt; Overall scalar reward This provides dense-ish supervision without hand-labeling every step. How many-eyes-learning relates: Both papers recognize sparse rewards as an information problem Reagent enriches the reward signal; IRPO multiplies the exploration Many-eyes takes the IRPO path: more explorers finding the sparse signal Future work could combine both: scouts + richer feedback per trajectory The Shared Meta-Lesson Both papers are saying the same thing: Sparse signals are a tragedy. Let’s smuggle in richer ones. IRPO: via intrinsic-reward exploration gradients Reagent: via language-based reward feedback Many-eyes-learning demonstrates the IRPO intuition in a simple, visual, reproducible way. Resources IRPO Paper (arXiv:2601.21391) Reagent Paper (arXiv:2601.22154) many-eyes-learning Repository Results Documentation Architecture Documentation Sparse rewards are an information problem. Many eyes provide the solution." />
<meta property="og:description" content="Single explorer: 0% success. Five explorers: 60% success. Learning often fails not because models are slow, but because they see too little. In sparse-reward environments, a single explorer is likely to miss the rare feedback entirely. The solution? Put many eyes on the problem. Resource Link Related Intrinsic Rewards and Diversity Papers IRPO · Reagent Code many-eyes-learning ELI5 eli5.md Video Given enough eyeballs… The Problem: Sparse Rewards Create Blindness As IRPO formalizes: in sparse-reward RL, the true policy gradient is basically uninformative most of the time. No reward signal → no gradient signal. A 7x7 grid with a single goal demonstrates this perfectly: Random agent success rate: ~9% With limited training (75 episodes), a single learner exploring alone never finds the goal This isn’t a compute problem. It’s an information problem. Challenge Effect Paper Connection Rare rewards Weak gradient signal IRPO’s core problem statement Single explorer Limited coverage Why multiple scouts help Random exploration Misses valuable states Why intrinsic rewards matter No feedback structure Can’t distinguish “almost right” from “nonsense” Reagent’s motivation The Solution: Many Eyes Instead of one explorer, use multiple scouts—independent exploratory agents that gather diverse information. ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ Scout 1 │ │ Scout 2 │ │ Scout N │ │ (strategy A)│ │ (strategy B)│ │ (strategy N)│ └──────┬──────┘ └──────┬──────┘ └──────┬──────┘ │ │ │ v v v ┌─────────────────────────────────────────────────┐ │ Experience Buffer │ └─────────────────────────────────────────────────┘ │ v ┌─────────────────────────────────────────────────┐ │ Shared Learner │ └─────────────────────────────────────────────────┘ Each scout explores with its own strategy. Their discoveries are aggregated to improve a shared learner. Results On a 7x7 sparse grid with 75 training episodes: Method Success Rate Random baseline 9% Single scout 0% Many eyes (3 scouts) 40% Many eyes (5 scouts) 60% Same total environment steps. Dramatically better outcomes. Why It Works Single Scout Fails Because: In IRPO terms: sparse reward → sparse gradient signal → no learning. Random exploration rarely reaches the goal (~9%) Insufficient successful trajectories DQN can’t learn from sparse positive examples The policy gradient has near-zero magnitude Many Eyes Succeeds Because: IRPO’s key insight: multiple exploratory policies manufacture signal. More coverage: Different scouts explore different regions (intrinsic rewards drive novelty-seeking) More discoveries: Higher probability of reaching goal (scouts find extrinsic reward) Signal routing: Scout discoveries update the shared learner (surrogate gradient in IRPO, experience pooling in many-eyes) Better gradients: Aggregated experience provides meaningful learning signal Scout Strategies (Intrinsic Rewards) IRPO uses intrinsic rewards to drive exploration. The many-eyes-learning project implements several strategies: Strategy Intrinsic Motivation IRPO Connection Epsilon-greedy Random action with probability ε Simple exploration noise Curious Bonus for novel states: 1/√(count+1) Count-based intrinsic reward Optimistic High initial Q-values Optimism under uncertainty Random Pure random baseline Maximum entropy exploration # CuriousScout intrinsic reward (simplified) def intrinsic_reward(self, state): count = self.state_counts[state] return self.bonus_scale / sqrt(count + 1) Scouts can be homogeneous (same strategy, different seeds) or heterogeneous (different strategies). IRPO supports swapping intrinsic reward functions—many-eyes makes this concrete with pluggable scout types. Running the Demo git clone https://github.com/softwarewrighter/many-eyes-learning cd many-eyes-learning # Setup uv venv .venv source .venv/bin/activate uv pip install -e &quot;.[dev]&quot; # Interactive CLI demo python experiments/cli_demo.py # Full experiment python experiments/run_experiment.py --episodes 75 --scouts 1 3 5 # Generate plots python experiments/plot_results.py Results appear in ~5-10 minutes on a laptop. Diversity Experiment Does diversity of strategies matter, or just number of scouts? Configuration Success Rate 5 random scouts 20% 5 epsilon-greedy scouts 40% 5 diverse scouts (mixed strategies) 40% Finding: In simple environments, strategy quality matters more than diversity. Epsilon-greedy beats random regardless of diversity. Key Insight The problem isn’t that learning is slow. The problem is that learning is blind. Many eyes make learning better. Implementation Details Metric Value Primary Language Python Source Files ~12 .py files Estimated Size ~1.5 KLOC Framework PyTorch, NumPy Platform CPU (no GPU required) Good for you if: You want to understand exploration in RL, experiment with sparse-reward environments, or see a clean implementation of scout-based learning. Complexity: Low-Moderate. Clean codebase with CLI demos. Runs on a laptop in minutes. Design Philosophy The project prioritizes clarity over performance: Single-file implementations where practical Minimal dependencies Sequential mode is first-class (parallel optional) Reproducible experiments with fixed seeds Simplifications from IRPO Full IRPO computes Jacobians to route gradients from exploratory policies back to the base policy. Many-eyes-learning simplifies this: IRPO Many-Eyes-Learning Jacobian chain rule Experience pooling Surrogate gradient Standard DQN updates Learned intrinsic rewards Hand-designed strategies The core insight remains: scouts explore with intrinsic motivation, discoveries benefit the shared learner. The math is simpler, the demo runs on a laptop, and the concept is clear. Key Takeaways Sparse rewards create information bottlenecks. Learning fails not from lack of compute, but lack of signal. More eyes = more information. Multiple scouts increase coverage and discovery rate. Diversity helps, but quality matters more. In simple environments, good exploration strategy beats diversity. Same compute, better outcomes. Many-eyes improves sample efficiency, not wall-clock speed. The Papers Behind Many-Eyes This project builds on two recent papers that attack the same fundamental problem: sparse rewards starve learning of signal. IRPO: Intrinsic Reward Policy Optimization IRPO (Cho &amp; Tran, UIUC) formalizes the scouts concept mathematically. The core insight: In sparse-reward RL, the true policy gradient is basically uninformative most of the time. No reward signal → no gradient signal. Learning stalls. IRPO’s solution: ┌─────────────────────────────────────────────────┐ │ 1. Train exploratory policies (scouts) │ │ using INTRINSIC rewards │ ├─────────────────────────────────────────────────┤ │ 2. Scouts discover EXTRINSIC rewards │ │ through exploration │ ├─────────────────────────────────────────────────┤ │ 3. Route extrinsic signal back to base policy │ │ via surrogate gradient (Jacobian chain) │ └─────────────────────────────────────────────────┘ IRPO Concept What It Means Intrinsic rewards “Explore what’s new” - reward novelty Exploratory policies Scouts driven by intrinsic motivation Surrogate gradient Trade bias for signal - approximate gradient that actually has magnitude Base policy The learner that benefits from scout discoveries How many-eyes-learning demonstrates this: Scouts implement intrinsic motivation (CuriousScout uses count-based novelty bonuses) Multiple exploration strategies create diverse coverage Aggregated experience routes discoveries to the shared DQN learner Simplified gradient routing - we pool experiences rather than compute full Jacobians Reagent: Reasoning Reward Models for Agents Reagent (Fan et al., CUHK/Meituan) takes a different approach: make feedback richer and more structured. The problem with sparse rewards: They can’t distinguish “almost right, failed at the end” from “complete nonsense.” Both get the same zero reward. Reagent’s solution: Build a Reasoning Reward Model that emits: Signal Purpose &lt;think&gt; Explicit reasoning trace &lt;critique&gt; Targeted natural-language feedback &lt;score&gt; Overall scalar reward This provides dense-ish supervision without hand-labeling every step. How many-eyes-learning relates: Both papers recognize sparse rewards as an information problem Reagent enriches the reward signal; IRPO multiplies the exploration Many-eyes takes the IRPO path: more explorers finding the sparse signal Future work could combine both: scouts + richer feedback per trajectory The Shared Meta-Lesson Both papers are saying the same thing: Sparse signals are a tragedy. Let’s smuggle in richer ones. IRPO: via intrinsic-reward exploration gradients Reagent: via language-based reward feedback Many-eyes-learning demonstrates the IRPO intuition in a simple, visual, reproducible way. Resources IRPO Paper (arXiv:2601.21391) Reagent Paper (arXiv:2601.22154) many-eyes-learning Repository Results Documentation Architecture Documentation Sparse rewards are an information problem. Many eyes provide the solution." />
<link rel="canonical" href="https://software-wrighter-lab.github.io/2026/02/03/many-eyes-learning/" />
<meta property="og:url" content="https://software-wrighter-lab.github.io/2026/02/03/many-eyes-learning/" />
<meta property="og:site_name" content="Software Wrighter Lab Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-02-03T09:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Solving Sparse Rewards with Many Eyes" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Software Wrighter"},"dateModified":"2026-02-03T09:00:00-08:00","datePublished":"2026-02-03T09:00:00-08:00","description":"Single explorer: 0% success. Five explorers: 60% success. Learning often fails not because models are slow, but because they see too little. In sparse-reward environments, a single explorer is likely to miss the rare feedback entirely. The solution? Put many eyes on the problem. Resource Link Related Intrinsic Rewards and Diversity Papers IRPO · Reagent Code many-eyes-learning ELI5 eli5.md Video Given enough eyeballs… The Problem: Sparse Rewards Create Blindness As IRPO formalizes: in sparse-reward RL, the true policy gradient is basically uninformative most of the time. No reward signal → no gradient signal. A 7x7 grid with a single goal demonstrates this perfectly: Random agent success rate: ~9% With limited training (75 episodes), a single learner exploring alone never finds the goal This isn’t a compute problem. It’s an information problem. Challenge Effect Paper Connection Rare rewards Weak gradient signal IRPO’s core problem statement Single explorer Limited coverage Why multiple scouts help Random exploration Misses valuable states Why intrinsic rewards matter No feedback structure Can’t distinguish “almost right” from “nonsense” Reagent’s motivation The Solution: Many Eyes Instead of one explorer, use multiple scouts—independent exploratory agents that gather diverse information. ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ Scout 1 │ │ Scout 2 │ │ Scout N │ │ (strategy A)│ │ (strategy B)│ │ (strategy N)│ └──────┬──────┘ └──────┬──────┘ └──────┬──────┘ │ │ │ v v v ┌─────────────────────────────────────────────────┐ │ Experience Buffer │ └─────────────────────────────────────────────────┘ │ v ┌─────────────────────────────────────────────────┐ │ Shared Learner │ └─────────────────────────────────────────────────┘ Each scout explores with its own strategy. Their discoveries are aggregated to improve a shared learner. Results On a 7x7 sparse grid with 75 training episodes: Method Success Rate Random baseline 9% Single scout 0% Many eyes (3 scouts) 40% Many eyes (5 scouts) 60% Same total environment steps. Dramatically better outcomes. Why It Works Single Scout Fails Because: In IRPO terms: sparse reward → sparse gradient signal → no learning. Random exploration rarely reaches the goal (~9%) Insufficient successful trajectories DQN can’t learn from sparse positive examples The policy gradient has near-zero magnitude Many Eyes Succeeds Because: IRPO’s key insight: multiple exploratory policies manufacture signal. More coverage: Different scouts explore different regions (intrinsic rewards drive novelty-seeking) More discoveries: Higher probability of reaching goal (scouts find extrinsic reward) Signal routing: Scout discoveries update the shared learner (surrogate gradient in IRPO, experience pooling in many-eyes) Better gradients: Aggregated experience provides meaningful learning signal Scout Strategies (Intrinsic Rewards) IRPO uses intrinsic rewards to drive exploration. The many-eyes-learning project implements several strategies: Strategy Intrinsic Motivation IRPO Connection Epsilon-greedy Random action with probability ε Simple exploration noise Curious Bonus for novel states: 1/√(count+1) Count-based intrinsic reward Optimistic High initial Q-values Optimism under uncertainty Random Pure random baseline Maximum entropy exploration # CuriousScout intrinsic reward (simplified) def intrinsic_reward(self, state): count = self.state_counts[state] return self.bonus_scale / sqrt(count + 1) Scouts can be homogeneous (same strategy, different seeds) or heterogeneous (different strategies). IRPO supports swapping intrinsic reward functions—many-eyes makes this concrete with pluggable scout types. Running the Demo git clone https://github.com/softwarewrighter/many-eyes-learning cd many-eyes-learning # Setup uv venv .venv source .venv/bin/activate uv pip install -e &quot;.[dev]&quot; # Interactive CLI demo python experiments/cli_demo.py # Full experiment python experiments/run_experiment.py --episodes 75 --scouts 1 3 5 # Generate plots python experiments/plot_results.py Results appear in ~5-10 minutes on a laptop. Diversity Experiment Does diversity of strategies matter, or just number of scouts? Configuration Success Rate 5 random scouts 20% 5 epsilon-greedy scouts 40% 5 diverse scouts (mixed strategies) 40% Finding: In simple environments, strategy quality matters more than diversity. Epsilon-greedy beats random regardless of diversity. Key Insight The problem isn’t that learning is slow. The problem is that learning is blind. Many eyes make learning better. Implementation Details Metric Value Primary Language Python Source Files ~12 .py files Estimated Size ~1.5 KLOC Framework PyTorch, NumPy Platform CPU (no GPU required) Good for you if: You want to understand exploration in RL, experiment with sparse-reward environments, or see a clean implementation of scout-based learning. Complexity: Low-Moderate. Clean codebase with CLI demos. Runs on a laptop in minutes. Design Philosophy The project prioritizes clarity over performance: Single-file implementations where practical Minimal dependencies Sequential mode is first-class (parallel optional) Reproducible experiments with fixed seeds Simplifications from IRPO Full IRPO computes Jacobians to route gradients from exploratory policies back to the base policy. Many-eyes-learning simplifies this: IRPO Many-Eyes-Learning Jacobian chain rule Experience pooling Surrogate gradient Standard DQN updates Learned intrinsic rewards Hand-designed strategies The core insight remains: scouts explore with intrinsic motivation, discoveries benefit the shared learner. The math is simpler, the demo runs on a laptop, and the concept is clear. Key Takeaways Sparse rewards create information bottlenecks. Learning fails not from lack of compute, but lack of signal. More eyes = more information. Multiple scouts increase coverage and discovery rate. Diversity helps, but quality matters more. In simple environments, good exploration strategy beats diversity. Same compute, better outcomes. Many-eyes improves sample efficiency, not wall-clock speed. The Papers Behind Many-Eyes This project builds on two recent papers that attack the same fundamental problem: sparse rewards starve learning of signal. IRPO: Intrinsic Reward Policy Optimization IRPO (Cho &amp; Tran, UIUC) formalizes the scouts concept mathematically. The core insight: In sparse-reward RL, the true policy gradient is basically uninformative most of the time. No reward signal → no gradient signal. Learning stalls. IRPO’s solution: ┌─────────────────────────────────────────────────┐ │ 1. Train exploratory policies (scouts) │ │ using INTRINSIC rewards │ ├─────────────────────────────────────────────────┤ │ 2. Scouts discover EXTRINSIC rewards │ │ through exploration │ ├─────────────────────────────────────────────────┤ │ 3. Route extrinsic signal back to base policy │ │ via surrogate gradient (Jacobian chain) │ └─────────────────────────────────────────────────┘ IRPO Concept What It Means Intrinsic rewards “Explore what’s new” - reward novelty Exploratory policies Scouts driven by intrinsic motivation Surrogate gradient Trade bias for signal - approximate gradient that actually has magnitude Base policy The learner that benefits from scout discoveries How many-eyes-learning demonstrates this: Scouts implement intrinsic motivation (CuriousScout uses count-based novelty bonuses) Multiple exploration strategies create diverse coverage Aggregated experience routes discoveries to the shared DQN learner Simplified gradient routing - we pool experiences rather than compute full Jacobians Reagent: Reasoning Reward Models for Agents Reagent (Fan et al., CUHK/Meituan) takes a different approach: make feedback richer and more structured. The problem with sparse rewards: They can’t distinguish “almost right, failed at the end” from “complete nonsense.” Both get the same zero reward. Reagent’s solution: Build a Reasoning Reward Model that emits: Signal Purpose &lt;think&gt; Explicit reasoning trace &lt;critique&gt; Targeted natural-language feedback &lt;score&gt; Overall scalar reward This provides dense-ish supervision without hand-labeling every step. How many-eyes-learning relates: Both papers recognize sparse rewards as an information problem Reagent enriches the reward signal; IRPO multiplies the exploration Many-eyes takes the IRPO path: more explorers finding the sparse signal Future work could combine both: scouts + richer feedback per trajectory The Shared Meta-Lesson Both papers are saying the same thing: Sparse signals are a tragedy. Let’s smuggle in richer ones. IRPO: via intrinsic-reward exploration gradients Reagent: via language-based reward feedback Many-eyes-learning demonstrates the IRPO intuition in a simple, visual, reproducible way. Resources IRPO Paper (arXiv:2601.21391) Reagent Paper (arXiv:2601.22154) many-eyes-learning Repository Results Documentation Architecture Documentation Sparse rewards are an information problem. Many eyes provide the solution.","headline":"Solving Sparse Rewards with Many Eyes","mainEntityOfPage":{"@type":"WebPage","@id":"https://software-wrighter-lab.github.io/2026/02/03/many-eyes-learning/"},"url":"https://software-wrighter-lab.github.io/2026/02/03/many-eyes-learning/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://software-wrighter-lab.github.io/feed.xml" title="Software Wrighter Lab Blog" /><!-- Theme toggle script - load early to prevent flash -->
  <script>
    (function() {
      var stored = localStorage.getItem('sw-lab-theme');
      var theme = stored;
      if (!theme) {
        theme = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
      }
      document.documentElement.setAttribute('data-theme', theme);
    })();
  </script>
  <!-- Font size - load early to prevent flash -->
  <script>
    (function() {
      var NEW_DEFAULT = 110;
      var OLD_DEFAULT = 150;
      var stored = localStorage.getItem('sw-lab-font-size');
      var ack = localStorage.getItem('sw-lab-prefs-ack');
      var size = stored ? parseInt(stored, 10) : null;

      // Migration: if on old default and not acknowledged, use new default
      if (size === OLD_DEFAULT && ack !== 'true') {
        size = NEW_DEFAULT;
      } else if (size === null) {
        size = NEW_DEFAULT;
      }

      // Apply to post content when DOM is ready
      document.addEventListener('DOMContentLoaded', function() {
        var pc = document.querySelector('.post-content');
        if (pc) pc.style.fontSize = size + '%';
        var pl = document.querySelector('.post-list');
        if (pl) pl.style.fontSize = size + '%';
      });
    })();
  </script>
  <script src="/assets/js/theme-toggle.js" defer></script>
  <script src="/assets/js/font-size.js" defer></script>
  <script src="/assets/js/preferences.js" defer></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">
      <img src="/assets/images/site/logo.jpg" alt="Software Wrighter Lab Blog" class="site-logo">
      <span class="site-title-text">Software Wrighter Lab Blog</span>
    </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/abstracts/">Abstracts</a><a class="page-link" href="/index-all/">Index</a><a class="page-link" href="/series/">Series</a><a class="page-link" href="/tags/">Tags</a><a class="page-link" href="/categories/">Categories</a><a class="page-link" href="/search/">Search</a><div class="font-size-controls">
  <button class="font-size-btn" id="font-decrease" title="Decrease font size" aria-label="Decrease font size">A-</button>
  <button class="font-size-btn" id="font-reset" title="Reset font size" aria-label="Reset font size">A</button>
  <button class="font-size-btn" id="font-increase" title="Increase font size" aria-label="Increase font size">A+</button>
</div>
<button class="theme-toggle" aria-label="Switch theme" title="Switch theme">
  <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
  </svg>
  <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
  </svg>
</button>
</div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Solving Sparse Rewards with Many Eyes</h1><p class="post-meta">February 3, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">1473 words</span> &bull; <span class="post-read-time">8 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Single explorer: 0% success. Five explorers: 60% success. Sparse rewards are an information problem, not a compute problem. Using multiple scouts with different exploration strategies, we gather diverse discoveries that benefit a shared learner.</div><div class="post-taxonomies"><span class="post-categories"><a href="/categories/#llm" class="category">llm</a><a href="/categories/#machine-learning" class="category">machine-learning</a><a href="/categories/#research" class="category">research</a></span><span class="post-tags"><a href="/tags/#reinforcement-learning" class="tag">reinforcement-learning</a><a href="/tags/#exploration" class="tag">exploration</a><a href="/tags/#sparse-rewards" class="tag">sparse-rewards</a><a href="/tags/#scouts" class="tag">scouts</a><a href="/tags/#dqn" class="tag">dqn</a></span></div></header><nav class="toc" data-toc-id="default">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content e-content" itemprop="articleBody">
    <p><img src="/assets/images/posts/many-eyes-monster.gif" class="post-marker" alt="" /></p>

<p>Single explorer: 0% success. Five explorers: 60% success.</p>

<p>Learning often fails not because models are slow, but because they <strong>see too little</strong>. In sparse-reward environments, a single explorer is likely to miss the rare feedback entirely. The solution? <strong>Put many eyes on the problem.</strong></p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Related</strong></td>
        <td><a href="/2026/02/25/many-eyes-learning-part2-intrinsic-rewards/">Intrinsic Rewards and Diversity</a></td>
      </tr>
      <tr>
        <td><strong>Papers</strong></td>
        <td><a href="https://arxiv.org/abs/2601.21391">IRPO</a> · <a href="https://arxiv.org/abs/2601.22154">Reagent</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/many-eyes-learning">many-eyes-learning</a></td>
      </tr>
      <tr>
        <td><strong>ELI5</strong></td>
        <td><a href="https://github.com/softwarewrighter/many-eyes-learning/blob/main/docs/eli5.md">eli5.md</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/KgtiBy4rdm4">Given enough eyeballs…</a><br /><a href="https://www.youtube.com/shorts/KgtiBy4rdm4"><img src="https://img.youtube.com/vi/KgtiBy4rdm4/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-problem-sparse-rewards-create-blindness">The Problem: Sparse Rewards Create Blindness</h2>

<p>As IRPO formalizes: in sparse-reward RL, <strong>the true policy gradient is basically uninformative most of the time</strong>. No reward signal → no gradient signal.</p>

<p>A 7x7 grid with a single goal demonstrates this perfectly:</p>

<ul>
  <li>Random agent success rate: ~9%</li>
  <li>With limited training (75 episodes), a single learner exploring alone <strong>never finds the goal</strong></li>
</ul>

<p>This isn’t a compute problem. It’s an <strong>information problem</strong>.</p>

<div class="references-float">

  <table>
    <thead>
      <tr>
        <th>Challenge</th>
        <th>Effect</th>
        <th>Paper Connection</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>Rare rewards</td>
        <td>Weak gradient signal</td>
        <td>IRPO’s core problem statement</td>
      </tr>
      <tr>
        <td>Single explorer</td>
        <td>Limited coverage</td>
        <td>Why multiple scouts help</td>
      </tr>
      <tr>
        <td>Random exploration</td>
        <td>Misses valuable states</td>
        <td>Why intrinsic rewards matter</td>
      </tr>
      <tr>
        <td>No feedback structure</td>
        <td>Can’t distinguish “almost right” from “nonsense”</td>
        <td>Reagent’s motivation</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-solution-many-eyes">The Solution: Many Eyes</h2>

<p>Instead of one explorer, use multiple <strong>scouts</strong>—independent exploratory agents that gather diverse information.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│   Scout 1   │  │   Scout 2   │  │   Scout N   │
│ (strategy A)│  │ (strategy B)│  │ (strategy N)│
└──────┬──────┘  └──────┬──────┘  └──────┬──────┘
       │                │                │
       v                v                v
┌─────────────────────────────────────────────────┐
│              Experience Buffer                   │
└─────────────────────────────────────────────────┘
                       │
                       v
┌─────────────────────────────────────────────────┐
│               Shared Learner                     │
└─────────────────────────────────────────────────┘
</code></pre></div></div>

<p>Each scout explores with its own strategy. Their discoveries are aggregated to improve a shared learner.</p>

<h2 id="results">Results</h2>

<p>On a 7x7 sparse grid with 75 training episodes:</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>Success Rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Random baseline</td>
      <td>9%</td>
    </tr>
    <tr>
      <td><strong>Single scout</strong></td>
      <td><strong>0%</strong></td>
    </tr>
    <tr>
      <td>Many eyes (3 scouts)</td>
      <td>40%</td>
    </tr>
    <tr>
      <td><strong>Many eyes (5 scouts)</strong></td>
      <td><strong>60%</strong></td>
    </tr>
  </tbody>
</table>

<p>Same total environment steps. Dramatically better outcomes.</p>

<h2 id="why-it-works">Why It Works</h2>

<h3 id="single-scout-fails-because">Single Scout Fails Because:</h3>

<p>In IRPO terms: <strong>sparse reward → sparse gradient signal → no learning</strong>.</p>

<ol>
  <li>Random exploration rarely reaches the goal (~9%)</li>
  <li>Insufficient successful trajectories</li>
  <li>DQN can’t learn from sparse positive examples</li>
  <li>The policy gradient has near-zero magnitude</li>
</ol>

<h3 id="many-eyes-succeeds-because">Many Eyes Succeeds Because:</h3>

<p>IRPO’s key insight: <strong>multiple exploratory policies manufacture signal</strong>.</p>

<ol>
  <li><strong>More coverage</strong>: Different scouts explore different regions (intrinsic rewards drive novelty-seeking)</li>
  <li><strong>More discoveries</strong>: Higher probability of reaching goal (scouts find extrinsic reward)</li>
  <li><strong>Signal routing</strong>: Scout discoveries update the shared learner (surrogate gradient in IRPO, experience pooling in many-eyes)</li>
  <li><strong>Better gradients</strong>: Aggregated experience provides meaningful learning signal</li>
</ol>

<h2 id="scout-strategies-intrinsic-rewards">Scout Strategies (Intrinsic Rewards)</h2>

<p>IRPO uses <strong>intrinsic rewards</strong> to drive exploration. The many-eyes-learning project implements several strategies:</p>

<table>
  <thead>
    <tr>
      <th>Strategy</th>
      <th>Intrinsic Motivation</th>
      <th>IRPO Connection</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Epsilon-greedy</strong></td>
      <td>Random action with probability ε</td>
      <td>Simple exploration noise</td>
    </tr>
    <tr>
      <td><strong>Curious</strong></td>
      <td>Bonus for novel states: <code class="language-plaintext highlighter-rouge">1/√(count+1)</code></td>
      <td>Count-based intrinsic reward</td>
    </tr>
    <tr>
      <td><strong>Optimistic</strong></td>
      <td>High initial Q-values</td>
      <td>Optimism under uncertainty</td>
    </tr>
    <tr>
      <td><strong>Random</strong></td>
      <td>Pure random baseline</td>
      <td>Maximum entropy exploration</td>
    </tr>
  </tbody>
</table>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># CuriousScout intrinsic reward (simplified)
</span><span class="k">def</span> <span class="nf">intrinsic_reward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">state_counts</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">bonus_scale</span> <span class="o">/</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">count</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Scouts can be <strong>homogeneous</strong> (same strategy, different seeds) or <strong>heterogeneous</strong> (different strategies). IRPO supports swapping intrinsic reward functions—many-eyes makes this concrete with pluggable scout types.</p>

<h2 id="running-the-demo">Running the Demo</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/softwarewrighter/many-eyes-learning
<span class="nb">cd </span>many-eyes-learning

<span class="c"># Setup</span>
uv venv .venv
<span class="nb">source</span> .venv/bin/activate
uv pip <span class="nb">install</span> <span class="nt">-e</span> <span class="s2">".[dev]"</span>

<span class="c"># Interactive CLI demo</span>
python experiments/cli_demo.py

<span class="c"># Full experiment</span>
python experiments/run_experiment.py <span class="nt">--episodes</span> 75 <span class="nt">--scouts</span> 1 3 5

<span class="c"># Generate plots</span>
python experiments/plot_results.py
</code></pre></div></div>

<p>Results appear in ~5-10 minutes on a laptop.</p>

<h2 id="diversity-experiment">Diversity Experiment</h2>

<p>Does <strong>diversity of strategies</strong> matter, or just <strong>number of scouts</strong>?</p>

<table>
  <thead>
    <tr>
      <th>Configuration</th>
      <th>Success Rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>5 random scouts</td>
      <td>20%</td>
    </tr>
    <tr>
      <td>5 epsilon-greedy scouts</td>
      <td>40%</td>
    </tr>
    <tr>
      <td>5 diverse scouts (mixed strategies)</td>
      <td>40%</td>
    </tr>
  </tbody>
</table>

<p><strong>Finding</strong>: In simple environments, strategy quality matters more than diversity. Epsilon-greedy beats random regardless of diversity.</p>

<h2 id="key-insight">Key Insight</h2>

<blockquote>
  <p><em>The problem isn’t that learning is slow. The problem is that learning is blind.</em></p>
</blockquote>

<p>Many eyes make learning better.</p>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Primary Language</strong></td>
      <td>Python</td>
    </tr>
    <tr>
      <td><strong>Source Files</strong></td>
      <td>~12 <code class="language-plaintext highlighter-rouge">.py</code> files</td>
    </tr>
    <tr>
      <td><strong>Estimated Size</strong></td>
      <td>~1.5 KLOC</td>
    </tr>
    <tr>
      <td><strong>Framework</strong></td>
      <td>PyTorch, NumPy</td>
    </tr>
    <tr>
      <td><strong>Platform</strong></td>
      <td>CPU (no GPU required)</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to understand exploration in RL, experiment with sparse-reward environments, or see a clean implementation of scout-based learning.</p>

<p><strong>Complexity:</strong> Low-Moderate. Clean codebase with CLI demos. Runs on a laptop in minutes.</p>

<h2 id="design-philosophy">Design Philosophy</h2>

<p>The project prioritizes <strong>clarity over performance</strong>:</p>

<ul>
  <li>Single-file implementations where practical</li>
  <li>Minimal dependencies</li>
  <li>Sequential mode is first-class (parallel optional)</li>
  <li>Reproducible experiments with fixed seeds</li>
</ul>

<h3 id="simplifications-from-irpo">Simplifications from IRPO</h3>

<p>Full IRPO computes Jacobians to route gradients from exploratory policies back to the base policy. Many-eyes-learning simplifies this:</p>

<table>
  <thead>
    <tr>
      <th>IRPO</th>
      <th>Many-Eyes-Learning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Jacobian chain rule</td>
      <td>Experience pooling</td>
    </tr>
    <tr>
      <td>Surrogate gradient</td>
      <td>Standard DQN updates</td>
    </tr>
    <tr>
      <td>Learned intrinsic rewards</td>
      <td>Hand-designed strategies</td>
    </tr>
  </tbody>
</table>

<p>The core insight remains: <strong>scouts explore with intrinsic motivation, discoveries benefit the shared learner</strong>. The math is simpler, the demo runs on a laptop, and the concept is clear.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>Sparse rewards create information bottlenecks.</strong> Learning fails not from lack of compute, but lack of signal.</p>
  </li>
  <li>
    <p><strong>More eyes = more information.</strong> Multiple scouts increase coverage and discovery rate.</p>
  </li>
  <li>
    <p><strong>Diversity helps, but quality matters more.</strong> In simple environments, good exploration strategy beats diversity.</p>
  </li>
  <li>
    <p><strong>Same compute, better outcomes.</strong> Many-eyes improves sample efficiency, not wall-clock speed.</p>
  </li>
</ol>

<h2 id="the-papers-behind-many-eyes">The Papers Behind Many-Eyes</h2>

<p>This project builds on two recent papers that attack the same fundamental problem: <strong>sparse rewards starve learning of signal</strong>.</p>

<h3 id="irpo-intrinsic-reward-policy-optimization">IRPO: Intrinsic Reward Policy Optimization</h3>

<p><a href="https://arxiv.org/abs/2601.21391">IRPO</a> (Cho &amp; Tran, UIUC) formalizes the scouts concept mathematically.</p>

<p><strong>The core insight:</strong> In sparse-reward RL, the true policy gradient is basically uninformative most of the time. No reward signal → no gradient signal. Learning stalls.</p>

<p><strong>IRPO’s solution:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────────────────────────────────────────┐
│  1. Train exploratory policies (scouts)         │
│     using INTRINSIC rewards                     │
├─────────────────────────────────────────────────┤
│  2. Scouts discover EXTRINSIC rewards           │
│     through exploration                         │
├─────────────────────────────────────────────────┤
│  3. Route extrinsic signal back to base policy  │
│     via surrogate gradient (Jacobian chain)     │
└─────────────────────────────────────────────────┘
</code></pre></div></div>

<table>
  <thead>
    <tr>
      <th>IRPO Concept</th>
      <th>What It Means</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Intrinsic rewards</strong></td>
      <td>“Explore what’s new” - reward novelty</td>
    </tr>
    <tr>
      <td><strong>Exploratory policies</strong></td>
      <td>Scouts driven by intrinsic motivation</td>
    </tr>
    <tr>
      <td><strong>Surrogate gradient</strong></td>
      <td>Trade bias for signal - approximate gradient that actually has magnitude</td>
    </tr>
    <tr>
      <td><strong>Base policy</strong></td>
      <td>The learner that benefits from scout discoveries</td>
    </tr>
  </tbody>
</table>

<p><strong>How many-eyes-learning demonstrates this:</strong></p>

<ul>
  <li><strong>Scouts</strong> implement intrinsic motivation (CuriousScout uses count-based novelty bonuses)</li>
  <li><strong>Multiple exploration strategies</strong> create diverse coverage</li>
  <li><strong>Aggregated experience</strong> routes discoveries to the shared DQN learner</li>
  <li><strong>Simplified gradient routing</strong> - we pool experiences rather than compute full Jacobians</li>
</ul>

<h3 id="reagent-reasoning-reward-models-for-agents">Reagent: Reasoning Reward Models for Agents</h3>

<p><a href="https://arxiv.org/abs/2601.22154">Reagent</a> (Fan et al., CUHK/Meituan) takes a different approach: make feedback <strong>richer and more structured</strong>.</p>

<p><strong>The problem with sparse rewards:</strong> They can’t distinguish “almost right, failed at the end” from “complete nonsense.” Both get the same zero reward.</p>

<p><strong>Reagent’s solution:</strong> Build a Reasoning Reward Model that emits:</p>

<table>
  <thead>
    <tr>
      <th>Signal</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">&lt;think&gt;</code></td>
      <td>Explicit reasoning trace</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">&lt;critique&gt;</code></td>
      <td>Targeted natural-language feedback</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">&lt;score&gt;</code></td>
      <td>Overall scalar reward</td>
    </tr>
  </tbody>
</table>

<p>This provides <strong>dense-ish supervision without hand-labeling every step</strong>.</p>

<p><strong>How many-eyes-learning relates:</strong></p>

<ul>
  <li>Both papers recognize sparse rewards as an <strong>information problem</strong></li>
  <li>Reagent enriches the reward signal; IRPO multiplies the exploration</li>
  <li>Many-eyes takes the IRPO path: <strong>more explorers finding the sparse signal</strong></li>
  <li>Future work could combine both: scouts + richer feedback per trajectory</li>
</ul>

<h3 id="the-shared-meta-lesson">The Shared Meta-Lesson</h3>

<p>Both papers are saying the same thing:</p>

<blockquote>
  <p><strong>Sparse signals are a tragedy. Let’s smuggle in richer ones.</strong></p>
</blockquote>

<ul>
  <li>IRPO: via intrinsic-reward exploration gradients</li>
  <li>Reagent: via language-based reward feedback</li>
</ul>

<p>Many-eyes-learning demonstrates the IRPO intuition in a simple, visual, reproducible way.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2601.21391">IRPO Paper (arXiv:2601.21391)</a></li>
  <li><a href="https://arxiv.org/abs/2601.22154">Reagent Paper (arXiv:2601.22154)</a></li>
  <li><a href="https://github.com/softwarewrighter/many-eyes-learning">many-eyes-learning Repository</a></li>
  <li><a href="https://github.com/softwarewrighter/many-eyes-learning/blob/main/docs/results.md">Results Documentation</a></li>
  <li><a href="https://github.com/softwarewrighter/many-eyes-learning/blob/main/docs/architecture.md">Architecture Documentation</a></li>
</ul>

<hr />

<p><em>Sparse rewards are an information problem. Many eyes provide the solution.</em></p>

  </div><div class="series-nav">
    <p><em>Part 1 of the Machine Learning series. <a href="/series/#machine-learning">View all parts</a> | <a href="/2026/02/12/dytopo-rs-dynamic-topology-multi-agent/">Next: Part 2 →</a></em></p>
  </div>





<div class="youtube-embed-container" id="yt-container-KgtiBy4rdm4">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-KgtiBy4rdm4"
      src="https://www.youtube.com/embed/KgtiBy4rdm4?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-KgtiBy4rdm4';
  const playerId = 'yt-player-KgtiBy4rdm4';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt=""><a class="u-url" href="/2026/02/03/many-eyes-learning/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Software Wrighter Lab Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Mike Wright</li><li><a class="u-email" href="mailto:"></a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/softwarewrighter"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">softwarewrighter</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>AI coding agents, systems programming, and practical machine learning</p>
      </div>
    </div>

    <div class="footer-copyright">
      <p>Copyright &copy; 2026 Michael A. Wright</p>
    </div>

  </div>

  <button id="scroll-to-top" aria-label="Scroll to top" title="Scroll to top">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <polyline points="18 15 12 9 6 15"></polyline>
    </svg>
  </button>

  <style>
  #scroll-to-top {
    position: fixed;
    bottom: 2rem;
    right: 2rem;
    width: 44px;
    height: 44px;
    border-radius: 50%;
    border: none;
    background: var(--brand-color, #2a7ae2);
    color: white;
    cursor: pointer;
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.3s, visibility 0.3s, transform 0.2s;
    z-index: 1000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
  }

  #scroll-to-top:hover {
    transform: scale(1.1);
  }

  #scroll-to-top.visible {
    opacity: 1;
    visibility: visible;
  }

  #scroll-to-top svg {
    width: 24px;
    height: 24px;
  }
  </style>

  <script>
  (function() {
    const btn = document.getElementById('scroll-to-top');
    if (!btn) return;

    window.addEventListener('scroll', function() {
      if (window.scrollY > 300) {
        btn.classList.add('visible');
      } else {
        btn.classList.remove('visible');
      }
    });

    btn.addEventListener('click', function() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    });
  })();
  </script>

</footer>
</body>

</html>
