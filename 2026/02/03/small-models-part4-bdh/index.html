<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Small Models (4/6): This AI Has a Visible Brain | Software Wrighter Lab Blog</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Small Models (4/6): This AI Has a Visible Brain" />
<meta name="author" content="Software Wrighter" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="LLMs are black boxes. Baby Dragon Hatchling (BDH) is different—a brain-inspired language model with sparse, interpretable activations. Train it on Shakespeare and actually see what’s happening inside. This is Part 4 of the Small Models, Big Brains series, exploring interpretability through sparsity. Resource Link Paper Pathway (Sparse Coding) Original Code pathwaycom/bdh Fork (with tools) softwarewrighter/bdh Video This AI Has a Visible Brain The Black Box Problem Modern neural networks are opaque: Billions of parameters Dense activations everywhere No clear mapping from neurons to concepts “It works, but we don’t know why” This isn’t just an academic concern. We’re deploying AI systems we don’t understand. Baby Dragon Hatchling: A Different Approach BDH takes inspiration from biological brains, which use sparse coding: Biological Brains Dense Neural Networks ~1-5% neurons active ~100% neurons active Energy efficient Computationally expensive Interpretable patterns Distributed, opaque Robust to noise Brittle Sparse Activations BDH enforces 80% sparsity—only 20% of neurons are active for any given token. Dense Network: [████████████████████] 100% active BDH: [████░░░░░░░░░░░░░░░░] 20% active This constraint forces the network to learn meaningful, localized representations. Training on Shakespeare The demo trains BDH on Shakespeare’s works: Training Progress: Epoch 1: Loss 0.86 Epoch 50: Loss 0.54 Epoch 100: Loss 0.38 Epoch 200: Loss 0.22 Loss drops from 0.86 to 0.22—the architecture works. Seeing Inside the Model With sparse activations, you can actually inspect what neurons mean: # Which neurons fire for &quot;love&quot;? activations = model.forward(&quot;love&quot;) active_neurons = activations.nonzero() # Neuron 47: fires for emotional words # Neuron 112: fires for abstract nouns # Neuron 203: fires for relationship terms When only 20% of neurons fire, each one carries interpretable meaning. Running the Code The bdh repository is a fork of Pathway’s original with added inspection tools: git clone https://github.com/softwarewrighter/bdh cd bdh pip install -r requirements.txt # Train on Shakespeare python train.py --dataset shakespeare --sparsity 0.8 # Inspect activations python inspect.py --model checkpoint.pt --text &quot;To be or not to be&quot; GPU recommended (Nvidia or Apple Silicon) for reasonable training times. Why Sparsity Enables Interpretability Dense Networks Every neuron participates in every computation. The “meaning” of any single neuron is distributed across all inputs it ever sees. Input: &quot;cat&quot; → All neurons contribute → Output Input: &quot;dog&quot; → All neurons contribute → Output Input: &quot;love&quot; → All neurons contribute → Output Trying to understand one neuron means understanding everything. Sparse Networks Only a small subset of neurons fire for each input. Neurons develop specialization. Input: &quot;cat&quot; → Neurons [12, 47, 89] fire → Output Input: &quot;dog&quot; → Neurons [12, 52, 89] fire → Output Input: &quot;love&quot; → Neurons [47, 112, 203] fire → Output Neuron 12 might mean “animal.” Neuron 47 might mean “emotional/living.” You can actually trace meaning. Comparison with Other Sparse Architectures Model Sparsity Type Purpose Mixture of Experts Routing sparsity Efficiency Top-k attention Attention sparsity Memory BDH Activation sparsity Interpretability BDH’s sparsity is specifically designed for understanding, not just efficiency. Implementation Details Metric Value Primary Language Python Source Files 9 .py files Estimated Size ~1.5 KLOC Framework PyTorch Build System pip / requirements.txt GPU Support CUDA, MPS (Apple Silicon) Good for you if: You want to experiment with sparse neural architectures, study interpretability techniques, or train small language models with visible internals. Complexity: Low-Moderate. Standard PyTorch project structure. The sparse activation mechanism is well-documented. Fork includes additional inspection tools not in the original. Key Takeaways Sparsity enables interpretability. When fewer neurons fire, each one means more. Brain-inspired design works. Biological neural coding principles transfer to AI. Interpretability doesn’t require sacrifice. BDH learns effectively despite constraints. We can build AI we understand. Black boxes aren’t inevitable. Current Limitations Early research stage Smaller scale than production models Training requires more epochs Not yet competitive with dense models on benchmarks But the principle is sound: constraint breeds clarity. What’s Next Part 5 dives into the 1B parameter sweet spot—comparing TinyLlama, Llama 3.2, StableLM, and Pythia. Resources Pathway Paper Original Pathway Code bdh Repository (with inspection tools) Video: This AI Has a Visible Brain *Part 4 of 6 in the Small Models, Big Brains series. View all parts Next: Part 5 →*" />
<meta property="og:description" content="LLMs are black boxes. Baby Dragon Hatchling (BDH) is different—a brain-inspired language model with sparse, interpretable activations. Train it on Shakespeare and actually see what’s happening inside. This is Part 4 of the Small Models, Big Brains series, exploring interpretability through sparsity. Resource Link Paper Pathway (Sparse Coding) Original Code pathwaycom/bdh Fork (with tools) softwarewrighter/bdh Video This AI Has a Visible Brain The Black Box Problem Modern neural networks are opaque: Billions of parameters Dense activations everywhere No clear mapping from neurons to concepts “It works, but we don’t know why” This isn’t just an academic concern. We’re deploying AI systems we don’t understand. Baby Dragon Hatchling: A Different Approach BDH takes inspiration from biological brains, which use sparse coding: Biological Brains Dense Neural Networks ~1-5% neurons active ~100% neurons active Energy efficient Computationally expensive Interpretable patterns Distributed, opaque Robust to noise Brittle Sparse Activations BDH enforces 80% sparsity—only 20% of neurons are active for any given token. Dense Network: [████████████████████] 100% active BDH: [████░░░░░░░░░░░░░░░░] 20% active This constraint forces the network to learn meaningful, localized representations. Training on Shakespeare The demo trains BDH on Shakespeare’s works: Training Progress: Epoch 1: Loss 0.86 Epoch 50: Loss 0.54 Epoch 100: Loss 0.38 Epoch 200: Loss 0.22 Loss drops from 0.86 to 0.22—the architecture works. Seeing Inside the Model With sparse activations, you can actually inspect what neurons mean: # Which neurons fire for &quot;love&quot;? activations = model.forward(&quot;love&quot;) active_neurons = activations.nonzero() # Neuron 47: fires for emotional words # Neuron 112: fires for abstract nouns # Neuron 203: fires for relationship terms When only 20% of neurons fire, each one carries interpretable meaning. Running the Code The bdh repository is a fork of Pathway’s original with added inspection tools: git clone https://github.com/softwarewrighter/bdh cd bdh pip install -r requirements.txt # Train on Shakespeare python train.py --dataset shakespeare --sparsity 0.8 # Inspect activations python inspect.py --model checkpoint.pt --text &quot;To be or not to be&quot; GPU recommended (Nvidia or Apple Silicon) for reasonable training times. Why Sparsity Enables Interpretability Dense Networks Every neuron participates in every computation. The “meaning” of any single neuron is distributed across all inputs it ever sees. Input: &quot;cat&quot; → All neurons contribute → Output Input: &quot;dog&quot; → All neurons contribute → Output Input: &quot;love&quot; → All neurons contribute → Output Trying to understand one neuron means understanding everything. Sparse Networks Only a small subset of neurons fire for each input. Neurons develop specialization. Input: &quot;cat&quot; → Neurons [12, 47, 89] fire → Output Input: &quot;dog&quot; → Neurons [12, 52, 89] fire → Output Input: &quot;love&quot; → Neurons [47, 112, 203] fire → Output Neuron 12 might mean “animal.” Neuron 47 might mean “emotional/living.” You can actually trace meaning. Comparison with Other Sparse Architectures Model Sparsity Type Purpose Mixture of Experts Routing sparsity Efficiency Top-k attention Attention sparsity Memory BDH Activation sparsity Interpretability BDH’s sparsity is specifically designed for understanding, not just efficiency. Implementation Details Metric Value Primary Language Python Source Files 9 .py files Estimated Size ~1.5 KLOC Framework PyTorch Build System pip / requirements.txt GPU Support CUDA, MPS (Apple Silicon) Good for you if: You want to experiment with sparse neural architectures, study interpretability techniques, or train small language models with visible internals. Complexity: Low-Moderate. Standard PyTorch project structure. The sparse activation mechanism is well-documented. Fork includes additional inspection tools not in the original. Key Takeaways Sparsity enables interpretability. When fewer neurons fire, each one means more. Brain-inspired design works. Biological neural coding principles transfer to AI. Interpretability doesn’t require sacrifice. BDH learns effectively despite constraints. We can build AI we understand. Black boxes aren’t inevitable. Current Limitations Early research stage Smaller scale than production models Training requires more epochs Not yet competitive with dense models on benchmarks But the principle is sound: constraint breeds clarity. What’s Next Part 5 dives into the 1B parameter sweet spot—comparing TinyLlama, Llama 3.2, StableLM, and Pythia. Resources Pathway Paper Original Pathway Code bdh Repository (with inspection tools) Video: This AI Has a Visible Brain *Part 4 of 6 in the Small Models, Big Brains series. View all parts Next: Part 5 →*" />
<link rel="canonical" href="https://software-wrighter-lab.github.io/2026/02/03/small-models-part4-bdh/" />
<meta property="og:url" content="https://software-wrighter-lab.github.io/2026/02/03/small-models-part4-bdh/" />
<meta property="og:site_name" content="Software Wrighter Lab Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-02-03T09:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Small Models (4/6): This AI Has a Visible Brain" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Software Wrighter"},"dateModified":"2026-02-03T09:00:00-08:00","datePublished":"2026-02-03T09:00:00-08:00","description":"LLMs are black boxes. Baby Dragon Hatchling (BDH) is different—a brain-inspired language model with sparse, interpretable activations. Train it on Shakespeare and actually see what’s happening inside. This is Part 4 of the Small Models, Big Brains series, exploring interpretability through sparsity. Resource Link Paper Pathway (Sparse Coding) Original Code pathwaycom/bdh Fork (with tools) softwarewrighter/bdh Video This AI Has a Visible Brain The Black Box Problem Modern neural networks are opaque: Billions of parameters Dense activations everywhere No clear mapping from neurons to concepts “It works, but we don’t know why” This isn’t just an academic concern. We’re deploying AI systems we don’t understand. Baby Dragon Hatchling: A Different Approach BDH takes inspiration from biological brains, which use sparse coding: Biological Brains Dense Neural Networks ~1-5% neurons active ~100% neurons active Energy efficient Computationally expensive Interpretable patterns Distributed, opaque Robust to noise Brittle Sparse Activations BDH enforces 80% sparsity—only 20% of neurons are active for any given token. Dense Network: [████████████████████] 100% active BDH: [████░░░░░░░░░░░░░░░░] 20% active This constraint forces the network to learn meaningful, localized representations. Training on Shakespeare The demo trains BDH on Shakespeare’s works: Training Progress: Epoch 1: Loss 0.86 Epoch 50: Loss 0.54 Epoch 100: Loss 0.38 Epoch 200: Loss 0.22 Loss drops from 0.86 to 0.22—the architecture works. Seeing Inside the Model With sparse activations, you can actually inspect what neurons mean: # Which neurons fire for &quot;love&quot;? activations = model.forward(&quot;love&quot;) active_neurons = activations.nonzero() # Neuron 47: fires for emotional words # Neuron 112: fires for abstract nouns # Neuron 203: fires for relationship terms When only 20% of neurons fire, each one carries interpretable meaning. Running the Code The bdh repository is a fork of Pathway’s original with added inspection tools: git clone https://github.com/softwarewrighter/bdh cd bdh pip install -r requirements.txt # Train on Shakespeare python train.py --dataset shakespeare --sparsity 0.8 # Inspect activations python inspect.py --model checkpoint.pt --text &quot;To be or not to be&quot; GPU recommended (Nvidia or Apple Silicon) for reasonable training times. Why Sparsity Enables Interpretability Dense Networks Every neuron participates in every computation. The “meaning” of any single neuron is distributed across all inputs it ever sees. Input: &quot;cat&quot; → All neurons contribute → Output Input: &quot;dog&quot; → All neurons contribute → Output Input: &quot;love&quot; → All neurons contribute → Output Trying to understand one neuron means understanding everything. Sparse Networks Only a small subset of neurons fire for each input. Neurons develop specialization. Input: &quot;cat&quot; → Neurons [12, 47, 89] fire → Output Input: &quot;dog&quot; → Neurons [12, 52, 89] fire → Output Input: &quot;love&quot; → Neurons [47, 112, 203] fire → Output Neuron 12 might mean “animal.” Neuron 47 might mean “emotional/living.” You can actually trace meaning. Comparison with Other Sparse Architectures Model Sparsity Type Purpose Mixture of Experts Routing sparsity Efficiency Top-k attention Attention sparsity Memory BDH Activation sparsity Interpretability BDH’s sparsity is specifically designed for understanding, not just efficiency. Implementation Details Metric Value Primary Language Python Source Files 9 .py files Estimated Size ~1.5 KLOC Framework PyTorch Build System pip / requirements.txt GPU Support CUDA, MPS (Apple Silicon) Good for you if: You want to experiment with sparse neural architectures, study interpretability techniques, or train small language models with visible internals. Complexity: Low-Moderate. Standard PyTorch project structure. The sparse activation mechanism is well-documented. Fork includes additional inspection tools not in the original. Key Takeaways Sparsity enables interpretability. When fewer neurons fire, each one means more. Brain-inspired design works. Biological neural coding principles transfer to AI. Interpretability doesn’t require sacrifice. BDH learns effectively despite constraints. We can build AI we understand. Black boxes aren’t inevitable. Current Limitations Early research stage Smaller scale than production models Training requires more epochs Not yet competitive with dense models on benchmarks But the principle is sound: constraint breeds clarity. What’s Next Part 5 dives into the 1B parameter sweet spot—comparing TinyLlama, Llama 3.2, StableLM, and Pythia. Resources Pathway Paper Original Pathway Code bdh Repository (with inspection tools) Video: This AI Has a Visible Brain *Part 4 of 6 in the Small Models, Big Brains series. View all parts Next: Part 5 →*","headline":"Small Models (4/6): This AI Has a Visible Brain","mainEntityOfPage":{"@type":"WebPage","@id":"https://software-wrighter-lab.github.io/2026/02/03/small-models-part4-bdh/"},"url":"https://software-wrighter-lab.github.io/2026/02/03/small-models-part4-bdh/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://software-wrighter-lab.github.io/feed.xml" title="Software Wrighter Lab Blog" /><!-- Theme toggle script - load early to prevent flash -->
  <script>
    (function() {
      var stored = localStorage.getItem('sw-lab-theme');
      var theme = stored;
      if (!theme) {
        theme = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
      }
      document.documentElement.setAttribute('data-theme', theme);
    })();
  </script>
  <!-- Font size - load early to prevent flash -->
  <script>
    (function() {
      var NEW_DEFAULT = 110;
      var OLD_DEFAULT = 150;
      var stored = localStorage.getItem('sw-lab-font-size');
      var ack = localStorage.getItem('sw-lab-prefs-ack');
      var size = stored ? parseInt(stored, 10) : null;

      // Migration: if on old default and not acknowledged, use new default
      if (size === OLD_DEFAULT && ack !== 'true') {
        size = NEW_DEFAULT;
      } else if (size === null) {
        size = NEW_DEFAULT;
      }

      // Apply to post content when DOM is ready
      document.addEventListener('DOMContentLoaded', function() {
        var pc = document.querySelector('.post-content');
        if (pc) pc.style.fontSize = size + '%';
        var pl = document.querySelector('.post-list');
        if (pl) pl.style.fontSize = size + '%';
      });
    })();
  </script>
  <script src="/assets/js/theme-toggle.js" defer></script>
  <script src="/assets/js/font-size.js" defer></script>
  <script src="/assets/js/preferences.js" defer></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">
      <img src="/assets/images/site/logo.jpg" alt="Software Wrighter Lab Blog" class="site-logo">
      <span class="site-title-text">Software Wrighter Lab Blog</span>
    </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/abstracts/">Abstracts</a><a class="page-link" href="/index-all/">Index</a><a class="page-link" href="/series/">Series</a><a class="page-link" href="/tags/">Tags</a><a class="page-link" href="/categories/">Categories</a><a class="page-link" href="/search/">Search</a><div class="font-size-controls">
  <button class="font-size-btn" id="font-decrease" title="Decrease font size" aria-label="Decrease font size">A-</button>
  <button class="font-size-btn" id="font-reset" title="Reset font size" aria-label="Reset font size">A</button>
  <button class="font-size-btn" id="font-increase" title="Increase font size" aria-label="Increase font size">A+</button>
</div>
<button class="theme-toggle" aria-label="Switch theme" title="Switch theme">
  <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
  </svg>
  <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
  </svg>
</button>
</div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Small Models (4/6): This AI Has a Visible Brain</h1><p class="post-meta">February 3, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">870 words</span> &bull; <span class="post-read-time">5 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">LLMs are black boxes. Baby Dragon Hatchling uses brain-inspired sparse coding with 80% sparsity, making only 20% of neurons active per token. When fewer neurons fire, each one carries interpretable meaning. Train it on Shakespeare and actually see what's happening inside.</div><div class="post-taxonomies"><span class="post-categories"><a href="/categories/#llm" class="category">llm</a><a href="/categories/#machine-learning" class="category">machine-learning</a><a href="/categories/#interpretability" class="category">interpretability</a></span><span class="post-tags"><a href="/tags/#bdh" class="tag">bdh</a><a href="/tags/#baby-dragon-hatchling" class="tag">baby-dragon-hatchling</a><a href="/tags/#sparse-activations" class="tag">sparse-activations</a><a href="/tags/#interpretable-ai" class="tag">interpretable-ai</a></span></div></header><nav class="toc" data-toc-id="default">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content e-content" itemprop="articleBody">
    <p><img src="/assets/images/posts/veggies.png" class="post-marker" alt="" /></p>

<p>LLMs are black boxes. Baby Dragon Hatchling (BDH) is different—a brain-inspired language model with sparse, interpretable activations.</p>

<p>Train it on Shakespeare and actually <em>see</em> what’s happening inside.</p>

<p>This is Part 4 of the <strong>Small Models, Big Brains</strong> series, exploring interpretability through sparsity.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2410.16179">Pathway (Sparse Coding)</a></td>
      </tr>
      <tr>
        <td><strong>Original Code</strong></td>
        <td><a href="https://github.com/pathwaycom/bdh">pathwaycom/bdh</a></td>
      </tr>
      <tr>
        <td><strong>Fork (with tools)</strong></td>
        <td><a href="https://github.com/softwarewrighter/bdh">softwarewrighter/bdh</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/pldadqycEQs">This AI Has a Visible Brain</a><br /><a href="https://www.youtube.com/shorts/pldadqycEQs"><img src="https://img.youtube.com/vi/pldadqycEQs/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="the-black-box-problem">The Black Box Problem</h2>

<p>Modern neural networks are opaque:</p>

<ul>
  <li>Billions of parameters</li>
  <li>Dense activations everywhere</li>
  <li>No clear mapping from neurons to concepts</li>
  <li>“It works, but we don’t know why”</li>
</ul>

<p>This isn’t just an academic concern. We’re deploying AI systems we don’t understand.</p>

<h2 id="baby-dragon-hatchling-a-different-approach">Baby Dragon Hatchling: A Different Approach</h2>

<p>BDH takes inspiration from biological brains, which use <strong>sparse coding</strong>:</p>

<table>
  <thead>
    <tr>
      <th>Biological Brains</th>
      <th>Dense Neural Networks</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>~1-5% neurons active</td>
      <td>~100% neurons active</td>
    </tr>
    <tr>
      <td>Energy efficient</td>
      <td>Computationally expensive</td>
    </tr>
    <tr>
      <td>Interpretable patterns</td>
      <td>Distributed, opaque</td>
    </tr>
    <tr>
      <td>Robust to noise</td>
      <td>Brittle</td>
    </tr>
  </tbody>
</table>

<h3 id="sparse-activations">Sparse Activations</h3>

<p>BDH enforces <strong>80% sparsity</strong>—only 20% of neurons are active for any given token.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dense Network:    [████████████████████] 100% active
BDH:              [████░░░░░░░░░░░░░░░░]  20% active
</code></pre></div></div>

<p>This constraint forces the network to learn meaningful, localized representations.</p>

<h2 id="training-on-shakespeare">Training on Shakespeare</h2>

<p>The demo trains BDH on Shakespeare’s works:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training Progress:
Epoch 1:   Loss 0.86
Epoch 50:  Loss 0.54
Epoch 100: Loss 0.38
Epoch 200: Loss 0.22
</code></pre></div></div>

<p>Loss drops from 0.86 to 0.22—the architecture works.</p>

<h2 id="seeing-inside-the-model">Seeing Inside the Model</h2>

<p>With sparse activations, you can actually inspect what neurons mean:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Which neurons fire for "love"?
</span><span class="n">activations</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">forward</span><span class="p">(</span><span class="sh">"</span><span class="s">love</span><span class="sh">"</span><span class="p">)</span>
<span class="n">active_neurons</span> <span class="o">=</span> <span class="n">activations</span><span class="p">.</span><span class="nf">nonzero</span><span class="p">()</span>

<span class="c1"># Neuron 47: fires for emotional words
# Neuron 112: fires for abstract nouns
# Neuron 203: fires for relationship terms
</span></code></pre></div></div>

<p>When only 20% of neurons fire, each one carries interpretable meaning.</p>

<h2 id="running-the-code">Running the Code</h2>

<p>The <a href="https://github.com/softwarewrighter/bdh">bdh</a> repository is a fork of Pathway’s original with added inspection tools:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/softwarewrighter/bdh
<span class="nb">cd </span>bdh
pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt

<span class="c"># Train on Shakespeare</span>
python train.py <span class="nt">--dataset</span> shakespeare <span class="nt">--sparsity</span> 0.8

<span class="c"># Inspect activations</span>
python inspect.py <span class="nt">--model</span> checkpoint.pt <span class="nt">--text</span> <span class="s2">"To be or not to be"</span>
</code></pre></div></div>

<p>GPU recommended (Nvidia or Apple Silicon) for reasonable training times.</p>

<h2 id="why-sparsity-enables-interpretability">Why Sparsity Enables Interpretability</h2>

<h3 id="dense-networks">Dense Networks</h3>

<p>Every neuron participates in every computation. The “meaning” of any single neuron is distributed across all inputs it ever sees.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: "cat"  → All neurons contribute → Output
Input: "dog"  → All neurons contribute → Output
Input: "love" → All neurons contribute → Output
</code></pre></div></div>

<p>Trying to understand one neuron means understanding <em>everything</em>.</p>

<h3 id="sparse-networks">Sparse Networks</h3>

<p>Only a small subset of neurons fire for each input. Neurons develop <em>specialization</em>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input: "cat"  → Neurons [12, 47, 89] fire → Output
Input: "dog"  → Neurons [12, 52, 89] fire → Output
Input: "love" → Neurons [47, 112, 203] fire → Output
</code></pre></div></div>

<p>Neuron 12 might mean “animal.” Neuron 47 might mean “emotional/living.” You can actually trace meaning.</p>

<h2 id="comparison-with-other-sparse-architectures">Comparison with Other Sparse Architectures</h2>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Sparsity Type</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Mixture of Experts</td>
      <td>Routing sparsity</td>
      <td>Efficiency</td>
    </tr>
    <tr>
      <td>Top-k attention</td>
      <td>Attention sparsity</td>
      <td>Memory</td>
    </tr>
    <tr>
      <td><strong>BDH</strong></td>
      <td>Activation sparsity</td>
      <td><strong>Interpretability</strong></td>
    </tr>
  </tbody>
</table>

<p>BDH’s sparsity is specifically designed for understanding, not just efficiency.</p>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Primary Language</strong></td>
      <td>Python</td>
    </tr>
    <tr>
      <td><strong>Source Files</strong></td>
      <td>9 <code class="language-plaintext highlighter-rouge">.py</code> files</td>
    </tr>
    <tr>
      <td><strong>Estimated Size</strong></td>
      <td>~1.5 KLOC</td>
    </tr>
    <tr>
      <td><strong>Framework</strong></td>
      <td>PyTorch</td>
    </tr>
    <tr>
      <td><strong>Build System</strong></td>
      <td>pip / requirements.txt</td>
    </tr>
    <tr>
      <td><strong>GPU Support</strong></td>
      <td>CUDA, MPS (Apple Silicon)</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to experiment with sparse neural architectures, study interpretability techniques, or train small language models with visible internals.</p>

<p><strong>Complexity:</strong> Low-Moderate. Standard PyTorch project structure. The sparse activation mechanism is well-documented. Fork includes additional inspection tools not in the original.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>Sparsity enables interpretability.</strong> When fewer neurons fire, each one means more.</p>
  </li>
  <li>
    <p><strong>Brain-inspired design works.</strong> Biological neural coding principles transfer to AI.</p>
  </li>
  <li>
    <p><strong>Interpretability doesn’t require sacrifice.</strong> BDH learns effectively despite constraints.</p>
  </li>
  <li>
    <p><strong>We can build AI we understand.</strong> Black boxes aren’t inevitable.</p>
  </li>
</ol>

<h2 id="current-limitations">Current Limitations</h2>

<ul>
  <li>Early research stage</li>
  <li>Smaller scale than production models</li>
  <li>Training requires more epochs</li>
  <li>Not yet competitive with dense models on benchmarks</li>
</ul>

<p>But the principle is sound: <strong>constraint breeds clarity</strong>.</p>

<h2 id="whats-next">What’s Next</h2>

<p>Part 5 dives into the <strong>1B parameter sweet spot</strong>—comparing TinyLlama, Llama 3.2, StableLM, and Pythia.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2410.16179">Pathway Paper</a></li>
  <li><a href="https://github.com/pathwaycom/bdh">Original Pathway Code</a></li>
  <li><a href="https://github.com/softwarewrighter/bdh">bdh Repository (with inspection tools)</a></li>
  <li><a href="https://youtube.com/shorts/VIDEO_ID">Video: This AI Has a Visible Brain</a></li>
</ul>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 4 of 6 in the Small Models, Big Brains series. <a href="/series/#small-models-big-brains">View all parts</a></td>
      <td><a href="/2026/02/04/small-models-part5-billion-llm/">Next: Part 5 →</a>*</td>
    </tr>
  </tbody>
</table>

  </div><div class="series-nav">
    <p><em>Part 4 of the Small Models, Big Brains series. <a href="/series/#small-models-big-brains">View all parts</a> | <a href="/2026/02/04/small-models-part5-billion-llm/">Next: Part 5 →</a></em></p>
  </div>





<div class="youtube-embed-container" id="yt-container-pldadqycEQs">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-pldadqycEQs"
      src="https://www.youtube.com/embed/pldadqycEQs?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-pldadqycEQs';
  const playerId = 'yt-player-pldadqycEQs';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt=""><a class="u-url" href="/2026/02/03/small-models-part4-bdh/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Software Wrighter Lab Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Mike Wright</li><li><a class="u-email" href="mailto:"></a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/softwarewrighter"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">softwarewrighter</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>AI coding agents, systems programming, and practical machine learning</p>
      </div>
    </div>

    <div class="footer-copyright">
      <p>Copyright &copy; 2026 Michael A. Wright</p>
    </div>

  </div>

  <button id="scroll-to-top" aria-label="Scroll to top" title="Scroll to top">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <polyline points="18 15 12 9 6 15"></polyline>
    </svg>
  </button>

  <style>
  #scroll-to-top {
    position: fixed;
    bottom: 2rem;
    right: 2rem;
    width: 44px;
    height: 44px;
    border-radius: 50%;
    border: none;
    background: var(--brand-color, #2a7ae2);
    color: white;
    cursor: pointer;
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.3s, visibility 0.3s, transform 0.2s;
    z-index: 1000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
  }

  #scroll-to-top:hover {
    transform: scale(1.1);
  }

  #scroll-to-top.visible {
    opacity: 1;
    visibility: visible;
  }

  #scroll-to-top svg {
    width: 24px;
    height: 24px;
  }
  </style>

  <script>
  (function() {
    const btn = document.getElementById('scroll-to-top');
    if (!btn) return;

    window.addEventListener('scroll', function() {
      if (window.scrollY > 300) {
        btn.classList.add('visible');
      } else {
        btn.classList.remove('visible');
      }
    });

    btn.addEventListener('click', function() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    });
  })();
  </script>

</footer>
</body>

</html>
