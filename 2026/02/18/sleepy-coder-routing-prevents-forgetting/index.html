<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Towards Continuous LLM Learning (2): Routing Prevents Forgetting | Software Wrighter Lab Blog</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Towards Continuous LLM Learning (2): Routing Prevents Forgetting" />
<meta name="author" content="Software Wrighter" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In Part 1, naive LoRA fine-tuning caused catastrophic forgetting. Now we’re implementing the Share algorithm properly—and we’re about 60% of the way to verifying the paper’s claims. Resource Link Code sleepy-coder Part 1 When Fine-Tuning Fails ELI5 eli5.md Share Paper arXiv:2602.06043 Paper Claims vs Implementation Status We’re systematically verifying the claims from the Share and UWSH papers: Paper Claim Infrastructure Demonstrated? Shared basis via SVD Complete Yes ~100x parameter reduction Complete (76x) Yes Task routing beats averaging Tested (Exp 1b) Partial Prevents catastrophic forgetting Tested (Exp 1b) Partial Sequential learning Not tested No UWSH subspace stability Not tested No Overall: ~60% complete. Infrastructure is solid. Routing tested. Sequential learning remains. What We Built The full Share algorithm implementation: Phase 1: SVD-based subspace extraction from 51 LoRA adapters (60% variance threshold) Phase 2: Coefficient-only training with frozen basis (83K params vs 1.6M full LoRA) Phase 3: Basis merging and updates Routing: Error pattern classification for coefficient selection Bug Fixes That Unlocked Progress Two critical bugs blocked proper Phase 2 training: Bug 1: Zero-Gradient Saddle Point Both coefficient matrices initialized to zero: eps_beta = 0, eps_alpha = 0 → delta_W = 0 @ 0 = 0 → zero gradients, no learning Fix: Dual small-random initialization. Bug 2: Half-Parameter Training LoRA-style initialization only trained one coefficient set: Before: 112/224 parameters getting gradients After: 224/224 parameters getting gradients Fix: Both coefficient matrices need random initialization. Experiment 1b: Routing Works With gradient-trained v4 coefficients and proper routing: Strategy Pass Rate BC RH TB Regressions Baseline (no LoRA) 46.7% 70% 40% 30% – Averaged 50.0% 70% 40% 40% 1 Routed 50.0% 70% 50% 30% 0 Result handling improved 40% → 50%. Zero regressions. This is the first positive transfer from Share coefficients. The Forgetting Heatmap We applied each coefficient individually to all 30 koans: Koan BL mut_bc dbl_mt ret_lr mis_cl mis_hs mis_or opt_ok res_me ROUTED bc_001-009 P P P P P P P P P P bc_003,5,10. . . . . . . . . . rh_002 . . +GAIN . . +GAIN +GAIN +GAIN +GAIN +GAIN rh_008 P -LOST -LOST -LOST -LOST -LOST -LOST -LOST -LOST P tb_005 P P P P P -LOST P P P P Key finding: rh_008 regresses under every coefficient applied globally. But routing saves it by falling back to the base model when no pattern matches. This is exactly what the Share paper predicts: task-specific coefficients improve targeted patterns without interfering with unrelated ones. What the Papers Claim vs What We’ve Verified Verified Shared basis via SVD — We extract principal components from 51 adapters. Works. 76x parameter reduction — 83K coefficient parameters vs 1.6M full LoRA. Verified. Routing prevents forgetting — Zero regressions with routed inference. The fragile rh_008 koan survives because it falls back to base model. Positive transfer possible — Result handling improved 40% → 50% with routed coefficients. Not Yet Verified Sequential learning — The core continual learning claim. Train task 1 → eval → train task 2 → eval (verify task 1 still passes). This is next. UWSH subspace stability — Do different adapter subsets converge to similar subspaces? Grassmann distance measurement needed. Next Experiments Priority Experiment Target High Sequential learning curve No degradation on prior tasks High Fix k_alpha=32 (paper recommends) Match paper exactly Medium UWSH verification &gt;70% subspace overlap Medium Add rank update vectors Full algorithm The Architecture Day: Agent attempts to fix Rust errors ↓ Successes and failures logged ↓ Night: Train coefficients (frozen basis) ↓ 83K params per task ↓ Eval: Route to appropriate coefficients ↓ Pattern-matched inference ↓ (repeat) The key insight: train small, route smart. The shared basis captures common structure. Per-task coefficients specialize without interference. Resources sleepy-coder Repository Part 1: When Fine-Tuning Fails Paper Checklists Share Paper (arXiv:2602.06043) UWSH Paper (arXiv:2512.05117) Part 2 of the Towards Continuous LLM Learning series. View all parts 60% of the way to verifying the papers. Sequential learning is next." />
<meta property="og:description" content="In Part 1, naive LoRA fine-tuning caused catastrophic forgetting. Now we’re implementing the Share algorithm properly—and we’re about 60% of the way to verifying the paper’s claims. Resource Link Code sleepy-coder Part 1 When Fine-Tuning Fails ELI5 eli5.md Share Paper arXiv:2602.06043 Paper Claims vs Implementation Status We’re systematically verifying the claims from the Share and UWSH papers: Paper Claim Infrastructure Demonstrated? Shared basis via SVD Complete Yes ~100x parameter reduction Complete (76x) Yes Task routing beats averaging Tested (Exp 1b) Partial Prevents catastrophic forgetting Tested (Exp 1b) Partial Sequential learning Not tested No UWSH subspace stability Not tested No Overall: ~60% complete. Infrastructure is solid. Routing tested. Sequential learning remains. What We Built The full Share algorithm implementation: Phase 1: SVD-based subspace extraction from 51 LoRA adapters (60% variance threshold) Phase 2: Coefficient-only training with frozen basis (83K params vs 1.6M full LoRA) Phase 3: Basis merging and updates Routing: Error pattern classification for coefficient selection Bug Fixes That Unlocked Progress Two critical bugs blocked proper Phase 2 training: Bug 1: Zero-Gradient Saddle Point Both coefficient matrices initialized to zero: eps_beta = 0, eps_alpha = 0 → delta_W = 0 @ 0 = 0 → zero gradients, no learning Fix: Dual small-random initialization. Bug 2: Half-Parameter Training LoRA-style initialization only trained one coefficient set: Before: 112/224 parameters getting gradients After: 224/224 parameters getting gradients Fix: Both coefficient matrices need random initialization. Experiment 1b: Routing Works With gradient-trained v4 coefficients and proper routing: Strategy Pass Rate BC RH TB Regressions Baseline (no LoRA) 46.7% 70% 40% 30% – Averaged 50.0% 70% 40% 40% 1 Routed 50.0% 70% 50% 30% 0 Result handling improved 40% → 50%. Zero regressions. This is the first positive transfer from Share coefficients. The Forgetting Heatmap We applied each coefficient individually to all 30 koans: Koan BL mut_bc dbl_mt ret_lr mis_cl mis_hs mis_or opt_ok res_me ROUTED bc_001-009 P P P P P P P P P P bc_003,5,10. . . . . . . . . . rh_002 . . +GAIN . . +GAIN +GAIN +GAIN +GAIN +GAIN rh_008 P -LOST -LOST -LOST -LOST -LOST -LOST -LOST -LOST P tb_005 P P P P P -LOST P P P P Key finding: rh_008 regresses under every coefficient applied globally. But routing saves it by falling back to the base model when no pattern matches. This is exactly what the Share paper predicts: task-specific coefficients improve targeted patterns without interfering with unrelated ones. What the Papers Claim vs What We’ve Verified Verified Shared basis via SVD — We extract principal components from 51 adapters. Works. 76x parameter reduction — 83K coefficient parameters vs 1.6M full LoRA. Verified. Routing prevents forgetting — Zero regressions with routed inference. The fragile rh_008 koan survives because it falls back to base model. Positive transfer possible — Result handling improved 40% → 50% with routed coefficients. Not Yet Verified Sequential learning — The core continual learning claim. Train task 1 → eval → train task 2 → eval (verify task 1 still passes). This is next. UWSH subspace stability — Do different adapter subsets converge to similar subspaces? Grassmann distance measurement needed. Next Experiments Priority Experiment Target High Sequential learning curve No degradation on prior tasks High Fix k_alpha=32 (paper recommends) Match paper exactly Medium UWSH verification &gt;70% subspace overlap Medium Add rank update vectors Full algorithm The Architecture Day: Agent attempts to fix Rust errors ↓ Successes and failures logged ↓ Night: Train coefficients (frozen basis) ↓ 83K params per task ↓ Eval: Route to appropriate coefficients ↓ Pattern-matched inference ↓ (repeat) The key insight: train small, route smart. The shared basis captures common structure. Per-task coefficients specialize without interference. Resources sleepy-coder Repository Part 1: When Fine-Tuning Fails Paper Checklists Share Paper (arXiv:2602.06043) UWSH Paper (arXiv:2512.05117) Part 2 of the Towards Continuous LLM Learning series. View all parts 60% of the way to verifying the papers. Sequential learning is next." />
<link rel="canonical" href="https://software-wrighter-lab.github.io/2026/02/18/sleepy-coder-routing-prevents-forgetting/" />
<meta property="og:url" content="https://software-wrighter-lab.github.io/2026/02/18/sleepy-coder-routing-prevents-forgetting/" />
<meta property="og:site_name" content="Software Wrighter Lab Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-02-18T12:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Towards Continuous LLM Learning (2): Routing Prevents Forgetting" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Software Wrighter"},"dateModified":"2026-02-18T12:00:00-08:00","datePublished":"2026-02-18T12:00:00-08:00","description":"In Part 1, naive LoRA fine-tuning caused catastrophic forgetting. Now we’re implementing the Share algorithm properly—and we’re about 60% of the way to verifying the paper’s claims. Resource Link Code sleepy-coder Part 1 When Fine-Tuning Fails ELI5 eli5.md Share Paper arXiv:2602.06043 Paper Claims vs Implementation Status We’re systematically verifying the claims from the Share and UWSH papers: Paper Claim Infrastructure Demonstrated? Shared basis via SVD Complete Yes ~100x parameter reduction Complete (76x) Yes Task routing beats averaging Tested (Exp 1b) Partial Prevents catastrophic forgetting Tested (Exp 1b) Partial Sequential learning Not tested No UWSH subspace stability Not tested No Overall: ~60% complete. Infrastructure is solid. Routing tested. Sequential learning remains. What We Built The full Share algorithm implementation: Phase 1: SVD-based subspace extraction from 51 LoRA adapters (60% variance threshold) Phase 2: Coefficient-only training with frozen basis (83K params vs 1.6M full LoRA) Phase 3: Basis merging and updates Routing: Error pattern classification for coefficient selection Bug Fixes That Unlocked Progress Two critical bugs blocked proper Phase 2 training: Bug 1: Zero-Gradient Saddle Point Both coefficient matrices initialized to zero: eps_beta = 0, eps_alpha = 0 → delta_W = 0 @ 0 = 0 → zero gradients, no learning Fix: Dual small-random initialization. Bug 2: Half-Parameter Training LoRA-style initialization only trained one coefficient set: Before: 112/224 parameters getting gradients After: 224/224 parameters getting gradients Fix: Both coefficient matrices need random initialization. Experiment 1b: Routing Works With gradient-trained v4 coefficients and proper routing: Strategy Pass Rate BC RH TB Regressions Baseline (no LoRA) 46.7% 70% 40% 30% – Averaged 50.0% 70% 40% 40% 1 Routed 50.0% 70% 50% 30% 0 Result handling improved 40% → 50%. Zero regressions. This is the first positive transfer from Share coefficients. The Forgetting Heatmap We applied each coefficient individually to all 30 koans: Koan BL mut_bc dbl_mt ret_lr mis_cl mis_hs mis_or opt_ok res_me ROUTED bc_001-009 P P P P P P P P P P bc_003,5,10. . . . . . . . . . rh_002 . . +GAIN . . +GAIN +GAIN +GAIN +GAIN +GAIN rh_008 P -LOST -LOST -LOST -LOST -LOST -LOST -LOST -LOST P tb_005 P P P P P -LOST P P P P Key finding: rh_008 regresses under every coefficient applied globally. But routing saves it by falling back to the base model when no pattern matches. This is exactly what the Share paper predicts: task-specific coefficients improve targeted patterns without interfering with unrelated ones. What the Papers Claim vs What We’ve Verified Verified Shared basis via SVD — We extract principal components from 51 adapters. Works. 76x parameter reduction — 83K coefficient parameters vs 1.6M full LoRA. Verified. Routing prevents forgetting — Zero regressions with routed inference. The fragile rh_008 koan survives because it falls back to base model. Positive transfer possible — Result handling improved 40% → 50% with routed coefficients. Not Yet Verified Sequential learning — The core continual learning claim. Train task 1 → eval → train task 2 → eval (verify task 1 still passes). This is next. UWSH subspace stability — Do different adapter subsets converge to similar subspaces? Grassmann distance measurement needed. Next Experiments Priority Experiment Target High Sequential learning curve No degradation on prior tasks High Fix k_alpha=32 (paper recommends) Match paper exactly Medium UWSH verification &gt;70% subspace overlap Medium Add rank update vectors Full algorithm The Architecture Day: Agent attempts to fix Rust errors ↓ Successes and failures logged ↓ Night: Train coefficients (frozen basis) ↓ 83K params per task ↓ Eval: Route to appropriate coefficients ↓ Pattern-matched inference ↓ (repeat) The key insight: train small, route smart. The shared basis captures common structure. Per-task coefficients specialize without interference. Resources sleepy-coder Repository Part 1: When Fine-Tuning Fails Paper Checklists Share Paper (arXiv:2602.06043) UWSH Paper (arXiv:2512.05117) Part 2 of the Towards Continuous LLM Learning series. View all parts 60% of the way to verifying the papers. Sequential learning is next.","headline":"Towards Continuous LLM Learning (2): Routing Prevents Forgetting","mainEntityOfPage":{"@type":"WebPage","@id":"https://software-wrighter-lab.github.io/2026/02/18/sleepy-coder-routing-prevents-forgetting/"},"url":"https://software-wrighter-lab.github.io/2026/02/18/sleepy-coder-routing-prevents-forgetting/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://software-wrighter-lab.github.io/feed.xml" title="Software Wrighter Lab Blog" /><!-- Theme toggle script - load early to prevent flash -->
  <script>
    (function() {
      var stored = localStorage.getItem('sw-lab-theme');
      var theme = stored;
      if (!theme) {
        theme = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
      }
      document.documentElement.setAttribute('data-theme', theme);
    })();
  </script>
  <!-- Font size - load early to prevent flash -->
  <script>
    (function() {
      var NEW_DEFAULT = 110;
      var OLD_DEFAULT = 150;
      var stored = localStorage.getItem('sw-lab-font-size');
      var ack = localStorage.getItem('sw-lab-prefs-ack');
      var size = stored ? parseInt(stored, 10) : null;

      // Migration: if on old default and not acknowledged, use new default
      if (size === OLD_DEFAULT && ack !== 'true') {
        size = NEW_DEFAULT;
      } else if (size === null) {
        size = NEW_DEFAULT;
      }

      // Apply to post content when DOM is ready
      document.addEventListener('DOMContentLoaded', function() {
        var pc = document.querySelector('.post-content');
        if (pc) pc.style.fontSize = size + '%';
        var pl = document.querySelector('.post-list');
        if (pl) pl.style.fontSize = size + '%';
      });
    })();
  </script>
  <script src="/assets/js/theme-toggle.js" defer></script>
  <script src="/assets/js/font-size.js" defer></script>
  <script src="/assets/js/preferences.js" defer></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">
      <img src="/assets/images/site/logo.jpg" alt="Software Wrighter Lab Blog" class="site-logo">
      <span class="site-title-text">Software Wrighter Lab Blog</span>
    </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/abstracts/">Abstracts</a><a class="page-link" href="/index-all/">Index</a><a class="page-link" href="/series/">Series</a><a class="page-link" href="/tags/">Tags</a><a class="page-link" href="/categories/">Categories</a><a class="page-link" href="/search/">Search</a><div class="font-size-controls">
  <button class="font-size-btn" id="font-decrease" title="Decrease font size" aria-label="Decrease font size">A-</button>
  <button class="font-size-btn" id="font-reset" title="Reset font size" aria-label="Reset font size">A</button>
  <button class="font-size-btn" id="font-increase" title="Increase font size" aria-label="Increase font size">A+</button>
</div>
<button class="theme-toggle" aria-label="Switch theme" title="Switch theme">
  <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
  </svg>
  <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
  </svg>
</button>
</div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Towards Continuous LLM Learning (2): Routing Prevents Forgetting</h1><p class="post-meta">February 18, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">788 words</span> &bull; <span class="post-read-time">4 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">Part 2 of implementing the Share algorithm: after fixing critical bugs (zero-gradient saddle point, half-parameter training), routing-based coefficient selection achieves zero regressions. Result handling improved 40% to 50%. We're 60% through verifying the paper's claims.</div><div class="post-taxonomies"><span class="post-categories"><a href="/categories/#llm" class="category">llm</a><a href="/categories/#machine-learning" class="category">machine-learning</a><a href="/categories/#research" class="category">research</a></span><span class="post-tags"><a href="/tags/#share-algorithm" class="tag">share-algorithm</a><a href="/tags/#continual-learning" class="tag">continual-learning</a><a href="/tags/#rust" class="tag">rust</a><a href="/tags/#lora" class="tag">lora</a><a href="/tags/#sleepy-coder" class="tag">sleepy-coder</a><a href="/tags/#svd" class="tag">svd</a></span></div></header><nav class="toc" data-toc-id="default">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content e-content" itemprop="articleBody">
    <p><img src="/assets/images/posts/block-ink-bottle.png" class="post-marker" alt="" /></p>

<p>In <a href="/2026/02/12/sleepy-coder-when-fine-tuning-fails/">Part 1</a>, naive LoRA fine-tuning caused catastrophic forgetting. Now we’re implementing the Share algorithm properly—and we’re about 60% of the way to verifying the paper’s claims.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/sleepy-coder">sleepy-coder</a></td>
      </tr>
      <tr>
        <td><strong>Part 1</strong></td>
        <td><a href="/2026/02/12/sleepy-coder-when-fine-tuning-fails/">When Fine-Tuning Fails</a></td>
      </tr>
      <tr>
        <td><strong>ELI5</strong></td>
        <td><a href="https://github.com/softwarewrighter/sleepy-coder/blob/main/docs/eli5.md">eli5.md</a></td>
      </tr>
      <tr>
        <td><strong>Share Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2602.06043">arXiv:2602.06043</a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="paper-claims-vs-implementation-status">Paper Claims vs Implementation Status</h2>

<p>We’re systematically verifying the claims from the Share and UWSH papers:</p>

<table>
  <thead>
    <tr>
      <th>Paper Claim</th>
      <th>Infrastructure</th>
      <th>Demonstrated?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Shared basis via SVD</td>
      <td>Complete</td>
      <td><strong>Yes</strong></td>
    </tr>
    <tr>
      <td>~100x parameter reduction</td>
      <td>Complete (76x)</td>
      <td><strong>Yes</strong></td>
    </tr>
    <tr>
      <td>Task routing beats averaging</td>
      <td>Tested (Exp 1b)</td>
      <td><strong>Partial</strong></td>
    </tr>
    <tr>
      <td>Prevents catastrophic forgetting</td>
      <td>Tested (Exp 1b)</td>
      <td><strong>Partial</strong></td>
    </tr>
    <tr>
      <td>Sequential learning</td>
      <td>Not tested</td>
      <td><strong>No</strong></td>
    </tr>
    <tr>
      <td>UWSH subspace stability</td>
      <td>Not tested</td>
      <td><strong>No</strong></td>
    </tr>
  </tbody>
</table>

<p><strong>Overall: ~60% complete.</strong> Infrastructure is solid. Routing tested. Sequential learning remains.</p>

<h2 id="what-we-built">What We Built</h2>

<p>The full Share algorithm implementation:</p>

<ul>
  <li><strong>Phase 1</strong>: SVD-based subspace extraction from 51 LoRA adapters (60% variance threshold)</li>
  <li><strong>Phase 2</strong>: Coefficient-only training with frozen basis (83K params vs 1.6M full LoRA)</li>
  <li><strong>Phase 3</strong>: Basis merging and updates</li>
  <li><strong>Routing</strong>: Error pattern classification for coefficient selection</li>
</ul>

<h2 id="bug-fixes-that-unlocked-progress">Bug Fixes That Unlocked Progress</h2>

<p>Two critical bugs blocked proper Phase 2 training:</p>

<h3 id="bug-1-zero-gradient-saddle-point">Bug 1: Zero-Gradient Saddle Point</h3>

<p>Both coefficient matrices initialized to zero:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>eps_beta = 0, eps_alpha = 0
→ delta_W = 0 @ 0 = 0
→ zero gradients, no learning
</code></pre></div></div>

<p><strong>Fix</strong>: Dual small-random initialization.</p>

<h3 id="bug-2-half-parameter-training">Bug 2: Half-Parameter Training</h3>

<p>LoRA-style initialization only trained one coefficient set:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Before: 112/224 parameters getting gradients
After:  224/224 parameters getting gradients
</code></pre></div></div>

<p><strong>Fix</strong>: Both coefficient matrices need random initialization.</p>

<h2 id="experiment-1b-routing-works">Experiment 1b: Routing Works</h2>

<p>With gradient-trained v4 coefficients and proper routing:</p>

<table>
  <thead>
    <tr>
      <th>Strategy</th>
      <th>Pass Rate</th>
      <th>BC</th>
      <th>RH</th>
      <th>TB</th>
      <th>Regressions</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Baseline (no LoRA)</td>
      <td>46.7%</td>
      <td>70%</td>
      <td>40%</td>
      <td>30%</td>
      <td>–</td>
    </tr>
    <tr>
      <td>Averaged</td>
      <td>50.0%</td>
      <td>70%</td>
      <td>40%</td>
      <td>40%</td>
      <td>1</td>
    </tr>
    <tr>
      <td><strong>Routed</strong></td>
      <td><strong>50.0%</strong></td>
      <td><strong>70%</strong></td>
      <td><strong>50%</strong></td>
      <td><strong>30%</strong></td>
      <td><strong>0</strong></td>
    </tr>
  </tbody>
</table>

<p><strong>Result handling improved 40% → 50%.</strong> Zero regressions. This is the first positive transfer from Share coefficients.</p>

<h2 id="the-forgetting-heatmap">The Forgetting Heatmap</h2>

<p>We applied each coefficient <strong>individually to all 30 koans</strong>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Koan       BL  mut_bc dbl_mt ret_lr mis_cl mis_hs mis_or opt_ok res_me ROUTED
bc_001-009 P   P      P      P      P      P      P      P      P      P
bc_003,5,10.   .      .      .      .      .      .      .      .      .
rh_002     .   .     +GAIN   .      .     +GAIN  +GAIN  +GAIN  +GAIN  +GAIN
rh_008     P  -LOST  -LOST  -LOST  -LOST  -LOST  -LOST  -LOST  -LOST   P
tb_005     P   P      P      P      P     -LOST   P      P      P      P
</code></pre></div></div>

<p><strong>Key finding</strong>: rh_008 regresses under <em>every</em> coefficient applied globally. But routing <strong>saves it</strong> by falling back to the base model when no pattern matches.</p>

<p>This is exactly what the Share paper predicts: task-specific coefficients improve targeted patterns without interfering with unrelated ones.</p>

<h2 id="what-the-papers-claim-vs-what-weve-verified">What the Papers Claim vs What We’ve Verified</h2>

<h3 id="verified">Verified</h3>

<ol>
  <li>
    <p><strong>Shared basis via SVD</strong> — We extract principal components from 51 adapters. Works.</p>
  </li>
  <li>
    <p><strong>76x parameter reduction</strong> — 83K coefficient parameters vs 1.6M full LoRA. Verified.</p>
  </li>
  <li>
    <p><strong>Routing prevents forgetting</strong> — Zero regressions with routed inference. The fragile rh_008 koan survives because it falls back to base model.</p>
  </li>
  <li>
    <p><strong>Positive transfer possible</strong> — Result handling improved 40% → 50% with routed coefficients.</p>
  </li>
</ol>

<h3 id="not-yet-verified">Not Yet Verified</h3>

<ol>
  <li>
    <p><strong>Sequential learning</strong> — The core continual learning claim. Train task 1 → eval → train task 2 → eval (verify task 1 still passes). This is next.</p>
  </li>
  <li>
    <p><strong>UWSH subspace stability</strong> — Do different adapter subsets converge to similar subspaces? Grassmann distance measurement needed.</p>
  </li>
</ol>

<h2 id="next-experiments">Next Experiments</h2>

<table>
  <thead>
    <tr>
      <th>Priority</th>
      <th>Experiment</th>
      <th>Target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>High</strong></td>
      <td>Sequential learning curve</td>
      <td>No degradation on prior tasks</td>
    </tr>
    <tr>
      <td><strong>High</strong></td>
      <td>Fix k_alpha=32 (paper recommends)</td>
      <td>Match paper exactly</td>
    </tr>
    <tr>
      <td>Medium</td>
      <td>UWSH verification</td>
      <td>&gt;70% subspace overlap</td>
    </tr>
    <tr>
      <td>Medium</td>
      <td>Add rank update vectors</td>
      <td>Full algorithm</td>
    </tr>
  </tbody>
</table>

<h2 id="the-architecture">The Architecture</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Day:   Agent attempts to fix Rust errors
       ↓
       Successes and failures logged
       ↓
Night: Train coefficients (frozen basis)
       ↓
       83K params per task
       ↓
Eval:  Route to appropriate coefficients
       ↓
       Pattern-matched inference
       ↓
(repeat)
</code></pre></div></div>

<p>The key insight: <strong>train small, route smart</strong>. The shared basis captures common structure. Per-task coefficients specialize without interference.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://github.com/softwarewrighter/sleepy-coder">sleepy-coder Repository</a></li>
  <li><a href="/2026/02/12/sleepy-coder-when-fine-tuning-fails/">Part 1: When Fine-Tuning Fails</a></li>
  <li><a href="https://github.com/softwarewrighter/sleepy-coder/blob/main/docs/paper-checklists.md">Paper Checklists</a></li>
  <li><a href="https://arxiv.org/abs/2602.06043">Share Paper (arXiv:2602.06043)</a></li>
  <li><a href="https://arxiv.org/abs/2512.05117">UWSH Paper (arXiv:2512.05117)</a></li>
</ul>

<hr />

<p><em>Part 2 of the Towards Continuous LLM Learning series. <a href="/series/#towards-continuous-llm-learning">View all parts</a></em></p>

<p><em>60% of the way to verifying the papers. Sequential learning is next.</em></p>


  </div><img src="/assets/images/site/post-separator.png" class="post-separator" alt=""><a class="u-url" href="/2026/02/18/sleepy-coder-routing-prevents-forgetting/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Software Wrighter Lab Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Mike Wright</li><li><a class="u-email" href="mailto:"></a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/softwarewrighter"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">softwarewrighter</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>AI coding agents, systems programming, and practical machine learning</p>
      </div>
    </div>

    <div class="footer-copyright">
      <p>Copyright &copy; 2026 Michael A. Wright</p>
    </div>

  </div>

  <button id="scroll-to-top" aria-label="Scroll to top" title="Scroll to top">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <polyline points="18 15 12 9 6 15"></polyline>
    </svg>
  </button>

  <style>
  #scroll-to-top {
    position: fixed;
    bottom: 2rem;
    right: 2rem;
    width: 44px;
    height: 44px;
    border-radius: 50%;
    border: none;
    background: var(--brand-color, #2a7ae2);
    color: white;
    cursor: pointer;
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.3s, visibility 0.3s, transform 0.2s;
    z-index: 1000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
  }

  #scroll-to-top:hover {
    transform: scale(1.1);
  }

  #scroll-to-top.visible {
    opacity: 1;
    visibility: visible;
  }

  #scroll-to-top svg {
    width: 24px;
    height: 24px;
  }
  </style>

  <script>
  (function() {
    const btn = document.getElementById('scroll-to-top');
    if (!btn) return;

    window.addEventListener('scroll', function() {
      if (window.scrollY > 300) {
        btn.classList.add('visible');
      } else {
        btn.classList.remove('visible');
      }
    });

    btn.addEventListener('click', function() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    });
  })();
  </script>

</footer>
</body>

</html>
