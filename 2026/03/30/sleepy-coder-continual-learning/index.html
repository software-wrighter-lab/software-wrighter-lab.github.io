<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Sleepy Coder: Teaching AI Agents to Learn from Their Mistakes | Software Wrighter Lab Blog</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Sleepy Coder: Teaching AI Agents to Learn from Their Mistakes" />
<meta name="author" content="Software Wrighter" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="What if your AI coding assistant could learn from its mistakes—not just for one session, but across training cycles? Resource Link Paper Share: Shared LoRA Subspaces Code sleepy-coder Video Coming soon References Concept Reference Share Shared LoRA Subspaces for almost Strict Continual Learning (Kaushik et al., Johns Hopkins 2026) UWSH Universal Weight Subspace Hypothesis (Kaushik et al., Johns Hopkins 2025) LoRA Low-Rank Adaptation of Large Language Models (Hu et al., Microsoft 2021) The Problem AI coding agents have a memory problem. They fix a bug today, then make the same mistake next week. Every session starts from the same frozen model. Nothing carries forward. Current agents are stateless learners—they can use tools, follow instructions, and generate code, but they don’t improve from experience. The model that struggles with borrow checker errors on Monday is equally confused by Friday. Sleep Learning The idea is simple: learn while you sleep. Day phase: The agent works on coding tasks. We log its failures—the error messages, the broken code, and the fixes that eventually worked. Sleep phase: Overnight, we fine-tune the model on those failures using parameter-efficient methods (LoRA). Eval phase: We test for improvement on new tasks and regression on old ones. Only if tests pass do we promote the new checkpoint. Each morning, a new model wakes up a little better than before. ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ DAY LOOP │ │ SLEEP LOOP │ │ EVAL LOOP │ │ (Rust Agent)│ --&gt; │(Python Train)│ --&gt; │ (Rust Eval) │ └─────────────┘ └─────────────┘ └─────────────┘ | | | v v v ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │episodes.sqlite│ │ adapter/ │ │metrics.jsonl│ │ (capture) │ │(LoRA weights)│ │ (results) │ └─────────────┘ └─────────────┘ └─────────────┘ Why This Might Work Two recent papers from Johns Hopkins University (Kaushik, Chaudhari, Vaidya, Chellappa, Yuille, 2025-2026) suggest this approach is grounded in solid theory: Share: Shared LoRA Subspaces The Share paper shows that continual LoRA training can work with a single evolving shared low-rank basis. Instead of keeping separate adapters for each task, you: Build a shared basis from past task adapters (via SVD) Store only tiny per-task coefficients Expand the basis when new tasks don’t fit This dramatically reduces storage and enables knowledge transfer between related tasks. UWSH: Universal Weight Subspace Hypothesis The UWSH paper goes further: models of the same architecture naturally converge to similar low-dimensional spectral subspaces. These subspaces are stable across tasks, initializations, and even modalities. The implication: continual learning might mostly be coefficient updates in a frozen basis, not constant retraining. The Architecture Sleepy Coder uses a dual-language design: Rust handles the agent runtime, episode capture, and evaluation. Rust gives us determinism and reproducibility—critical for measuring improvement. Python handles training, embeddings, and visualization. The HuggingFace/PEFT ecosystem makes parameter-efficient fine-tuning straightforward. Task System We use “Rust Koans”—small buggy code snippets that exercise common error patterns: Error Family Example Borrow checker Moved values, dangling borrows Lifetimes Missing annotations Trait bounds Missing implementations Result handling Unwrap/? misuse Type mismatch Iterator types, generics Each task has buggy code, expected error, and correct fix—enabling automated evaluation. Episode Capture Every agent interaction is logged: pub struct Episode { pub task_id: String, pub error_signature: String, // normalized pub diff_unified: String, // the fix pub passed: bool, pub steps_to_green: u32, // ... } Error signatures are normalized to enable clustering and repeat detection. Gated Promotion Not every training run produces a better model. We gate deployment on: Repeat error rate: Does the agent make the same mistakes? Steps to green: How many attempts to solve tasks? Regression check: Does a frozen suite still pass? Only models that improve without regressing get promoted. Current Status This project is work in progress. The infrastructure is largely built, but we don’t have conclusive results yet. Done: Agent runtime (Rust) Episode capture (SQLite) Task generator (Rust Koans) LoRA training pipeline (Python) Evaluation harness (Rust) In Progress: Running multi-day experiments Measuring repeat error rates Testing Share-style consolidation Open Questions: How many sleep cycles before measurable improvement? Does the shared basis approach scale? What’s the optimal task curriculum? Why This Matters If sleep learning works, it changes how we think about AI agents: Agents could specialize to your codebase, your error patterns, your style Mistakes become training data, not just frustration Small models could outperform large ones on familiar tasks Continual improvement replaces periodic retraining This is what “learning from experience” should mean for AI. What’s Next We’re running experiments now. Follow for updates as we collect results. The code is open source—contributions welcome. Sleepy Coder: Parameter-efficient continual learning for AI coding agents." />
<meta property="og:description" content="What if your AI coding assistant could learn from its mistakes—not just for one session, but across training cycles? Resource Link Paper Share: Shared LoRA Subspaces Code sleepy-coder Video Coming soon References Concept Reference Share Shared LoRA Subspaces for almost Strict Continual Learning (Kaushik et al., Johns Hopkins 2026) UWSH Universal Weight Subspace Hypothesis (Kaushik et al., Johns Hopkins 2025) LoRA Low-Rank Adaptation of Large Language Models (Hu et al., Microsoft 2021) The Problem AI coding agents have a memory problem. They fix a bug today, then make the same mistake next week. Every session starts from the same frozen model. Nothing carries forward. Current agents are stateless learners—they can use tools, follow instructions, and generate code, but they don’t improve from experience. The model that struggles with borrow checker errors on Monday is equally confused by Friday. Sleep Learning The idea is simple: learn while you sleep. Day phase: The agent works on coding tasks. We log its failures—the error messages, the broken code, and the fixes that eventually worked. Sleep phase: Overnight, we fine-tune the model on those failures using parameter-efficient methods (LoRA). Eval phase: We test for improvement on new tasks and regression on old ones. Only if tests pass do we promote the new checkpoint. Each morning, a new model wakes up a little better than before. ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ DAY LOOP │ │ SLEEP LOOP │ │ EVAL LOOP │ │ (Rust Agent)│ --&gt; │(Python Train)│ --&gt; │ (Rust Eval) │ └─────────────┘ └─────────────┘ └─────────────┘ | | | v v v ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │episodes.sqlite│ │ adapter/ │ │metrics.jsonl│ │ (capture) │ │(LoRA weights)│ │ (results) │ └─────────────┘ └─────────────┘ └─────────────┘ Why This Might Work Two recent papers from Johns Hopkins University (Kaushik, Chaudhari, Vaidya, Chellappa, Yuille, 2025-2026) suggest this approach is grounded in solid theory: Share: Shared LoRA Subspaces The Share paper shows that continual LoRA training can work with a single evolving shared low-rank basis. Instead of keeping separate adapters for each task, you: Build a shared basis from past task adapters (via SVD) Store only tiny per-task coefficients Expand the basis when new tasks don’t fit This dramatically reduces storage and enables knowledge transfer between related tasks. UWSH: Universal Weight Subspace Hypothesis The UWSH paper goes further: models of the same architecture naturally converge to similar low-dimensional spectral subspaces. These subspaces are stable across tasks, initializations, and even modalities. The implication: continual learning might mostly be coefficient updates in a frozen basis, not constant retraining. The Architecture Sleepy Coder uses a dual-language design: Rust handles the agent runtime, episode capture, and evaluation. Rust gives us determinism and reproducibility—critical for measuring improvement. Python handles training, embeddings, and visualization. The HuggingFace/PEFT ecosystem makes parameter-efficient fine-tuning straightforward. Task System We use “Rust Koans”—small buggy code snippets that exercise common error patterns: Error Family Example Borrow checker Moved values, dangling borrows Lifetimes Missing annotations Trait bounds Missing implementations Result handling Unwrap/? misuse Type mismatch Iterator types, generics Each task has buggy code, expected error, and correct fix—enabling automated evaluation. Episode Capture Every agent interaction is logged: pub struct Episode { pub task_id: String, pub error_signature: String, // normalized pub diff_unified: String, // the fix pub passed: bool, pub steps_to_green: u32, // ... } Error signatures are normalized to enable clustering and repeat detection. Gated Promotion Not every training run produces a better model. We gate deployment on: Repeat error rate: Does the agent make the same mistakes? Steps to green: How many attempts to solve tasks? Regression check: Does a frozen suite still pass? Only models that improve without regressing get promoted. Current Status This project is work in progress. The infrastructure is largely built, but we don’t have conclusive results yet. Done: Agent runtime (Rust) Episode capture (SQLite) Task generator (Rust Koans) LoRA training pipeline (Python) Evaluation harness (Rust) In Progress: Running multi-day experiments Measuring repeat error rates Testing Share-style consolidation Open Questions: How many sleep cycles before measurable improvement? Does the shared basis approach scale? What’s the optimal task curriculum? Why This Matters If sleep learning works, it changes how we think about AI agents: Agents could specialize to your codebase, your error patterns, your style Mistakes become training data, not just frustration Small models could outperform large ones on familiar tasks Continual improvement replaces periodic retraining This is what “learning from experience” should mean for AI. What’s Next We’re running experiments now. Follow for updates as we collect results. The code is open source—contributions welcome. Sleepy Coder: Parameter-efficient continual learning for AI coding agents." />
<link rel="canonical" href="http://localhost:5907/2026/03/30/sleepy-coder-continual-learning/" />
<meta property="og:url" content="http://localhost:5907/2026/03/30/sleepy-coder-continual-learning/" />
<meta property="og:site_name" content="Software Wrighter Lab Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-03-30T01:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Sleepy Coder: Teaching AI Agents to Learn from Their Mistakes" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Software Wrighter"},"dateModified":"2026-03-30T01:00:00-07:00","datePublished":"2026-03-30T01:00:00-07:00","description":"What if your AI coding assistant could learn from its mistakes—not just for one session, but across training cycles? Resource Link Paper Share: Shared LoRA Subspaces Code sleepy-coder Video Coming soon References Concept Reference Share Shared LoRA Subspaces for almost Strict Continual Learning (Kaushik et al., Johns Hopkins 2026) UWSH Universal Weight Subspace Hypothesis (Kaushik et al., Johns Hopkins 2025) LoRA Low-Rank Adaptation of Large Language Models (Hu et al., Microsoft 2021) The Problem AI coding agents have a memory problem. They fix a bug today, then make the same mistake next week. Every session starts from the same frozen model. Nothing carries forward. Current agents are stateless learners—they can use tools, follow instructions, and generate code, but they don’t improve from experience. The model that struggles with borrow checker errors on Monday is equally confused by Friday. Sleep Learning The idea is simple: learn while you sleep. Day phase: The agent works on coding tasks. We log its failures—the error messages, the broken code, and the fixes that eventually worked. Sleep phase: Overnight, we fine-tune the model on those failures using parameter-efficient methods (LoRA). Eval phase: We test for improvement on new tasks and regression on old ones. Only if tests pass do we promote the new checkpoint. Each morning, a new model wakes up a little better than before. ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │ DAY LOOP │ │ SLEEP LOOP │ │ EVAL LOOP │ │ (Rust Agent)│ --&gt; │(Python Train)│ --&gt; │ (Rust Eval) │ └─────────────┘ └─────────────┘ └─────────────┘ | | | v v v ┌─────────────┐ ┌─────────────┐ ┌─────────────┐ │episodes.sqlite│ │ adapter/ │ │metrics.jsonl│ │ (capture) │ │(LoRA weights)│ │ (results) │ └─────────────┘ └─────────────┘ └─────────────┘ Why This Might Work Two recent papers from Johns Hopkins University (Kaushik, Chaudhari, Vaidya, Chellappa, Yuille, 2025-2026) suggest this approach is grounded in solid theory: Share: Shared LoRA Subspaces The Share paper shows that continual LoRA training can work with a single evolving shared low-rank basis. Instead of keeping separate adapters for each task, you: Build a shared basis from past task adapters (via SVD) Store only tiny per-task coefficients Expand the basis when new tasks don’t fit This dramatically reduces storage and enables knowledge transfer between related tasks. UWSH: Universal Weight Subspace Hypothesis The UWSH paper goes further: models of the same architecture naturally converge to similar low-dimensional spectral subspaces. These subspaces are stable across tasks, initializations, and even modalities. The implication: continual learning might mostly be coefficient updates in a frozen basis, not constant retraining. The Architecture Sleepy Coder uses a dual-language design: Rust handles the agent runtime, episode capture, and evaluation. Rust gives us determinism and reproducibility—critical for measuring improvement. Python handles training, embeddings, and visualization. The HuggingFace/PEFT ecosystem makes parameter-efficient fine-tuning straightforward. Task System We use “Rust Koans”—small buggy code snippets that exercise common error patterns: Error Family Example Borrow checker Moved values, dangling borrows Lifetimes Missing annotations Trait bounds Missing implementations Result handling Unwrap/? misuse Type mismatch Iterator types, generics Each task has buggy code, expected error, and correct fix—enabling automated evaluation. Episode Capture Every agent interaction is logged: pub struct Episode { pub task_id: String, pub error_signature: String, // normalized pub diff_unified: String, // the fix pub passed: bool, pub steps_to_green: u32, // ... } Error signatures are normalized to enable clustering and repeat detection. Gated Promotion Not every training run produces a better model. We gate deployment on: Repeat error rate: Does the agent make the same mistakes? Steps to green: How many attempts to solve tasks? Regression check: Does a frozen suite still pass? Only models that improve without regressing get promoted. Current Status This project is work in progress. The infrastructure is largely built, but we don’t have conclusive results yet. Done: Agent runtime (Rust) Episode capture (SQLite) Task generator (Rust Koans) LoRA training pipeline (Python) Evaluation harness (Rust) In Progress: Running multi-day experiments Measuring repeat error rates Testing Share-style consolidation Open Questions: How many sleep cycles before measurable improvement? Does the shared basis approach scale? What’s the optimal task curriculum? Why This Matters If sleep learning works, it changes how we think about AI agents: Agents could specialize to your codebase, your error patterns, your style Mistakes become training data, not just frustration Small models could outperform large ones on familiar tasks Continual improvement replaces periodic retraining This is what “learning from experience” should mean for AI. What’s Next We’re running experiments now. Follow for updates as we collect results. The code is open source—contributions welcome. Sleepy Coder: Parameter-efficient continual learning for AI coding agents.","headline":"Sleepy Coder: Teaching AI Agents to Learn from Their Mistakes","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:5907/2026/03/30/sleepy-coder-continual-learning/"},"url":"http://localhost:5907/2026/03/30/sleepy-coder-continual-learning/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:5907/feed.xml" title="Software Wrighter Lab Blog" /><!-- Theme toggle script - load early to prevent flash -->
  <script>
    (function() {
      var stored = localStorage.getItem('sw-lab-theme');
      var theme = stored;
      if (!theme) {
        theme = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
      }
      document.documentElement.setAttribute('data-theme', theme);
    })();
  </script>
  <!-- Font size - load early to prevent flash -->
  <script>
    (function() {
      var NEW_DEFAULT = 110;
      var OLD_DEFAULT = 150;
      var stored = localStorage.getItem('sw-lab-font-size');
      var ack = localStorage.getItem('sw-lab-prefs-ack');
      var size = stored ? parseInt(stored, 10) : null;

      // Migration: if on old default and not acknowledged, use new default
      if (size === OLD_DEFAULT && ack !== 'true') {
        size = NEW_DEFAULT;
      } else if (size === null) {
        size = NEW_DEFAULT;
      }

      // Apply to post content when DOM is ready
      document.addEventListener('DOMContentLoaded', function() {
        var pc = document.querySelector('.post-content');
        if (pc) pc.style.fontSize = size + '%';
        var pl = document.querySelector('.post-list');
        if (pl) pl.style.fontSize = size + '%';
      });
    })();
  </script>
  <script src="/assets/js/theme-toggle.js" defer></script>
  <script src="/assets/js/font-size.js" defer></script>
  <script src="/assets/js/preferences.js" defer></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">
      <img src="/assets/images/site/logo.jpg" alt="Software Wrighter Lab Blog" class="site-logo">
      <span class="site-title-text">Software Wrighter Lab Blog</span>
    </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/abstracts/">Abstracts</a><a class="page-link" href="/index-all/">Index</a><a class="page-link" href="/series/">Series</a><a class="page-link" href="/tags/">Tags</a><a class="page-link" href="/categories/">Categories</a><a class="page-link" href="/search/">Search</a><div class="font-size-controls">
  <button class="font-size-btn" id="font-decrease" title="Decrease font size" aria-label="Decrease font size">A-</button>
  <button class="font-size-btn" id="font-reset" title="Reset font size" aria-label="Reset font size">A</button>
  <button class="font-size-btn" id="font-increase" title="Increase font size" aria-label="Increase font size">A+</button>
</div>
<button class="theme-toggle" aria-label="Switch theme" title="Switch theme">
  <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
  </svg>
  <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
  </svg>
</button>
</div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Sleepy Coder: Teaching AI Agents to Learn from Their Mistakes</h1><p class="post-meta">March 30, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">926 words</span> &bull; <span class="post-read-time">5 min read</span></em></p><div class="post-taxonomies"><span class="post-categories"><a href="/categories/#llm" class="category">llm</a><a href="/categories/#machine-learning" class="category">machine-learning</a><a href="/categories/#research" class="category">research</a></span><span class="post-tags"><a href="/tags/#sleepy-coder" class="tag">sleepy-coder</a><a href="/tags/#continual-learning" class="tag">continual-learning</a><a href="/tags/#lora" class="tag">lora</a><a href="/tags/#fine-tuning" class="tag">fine-tuning</a><a href="/tags/#ai-agents" class="tag">ai-agents</a><a href="/tags/#rust" class="tag">rust</a></span></div></header><nav class="toc" data-toc-id="default">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content e-content" itemprop="articleBody">
    <p><img src="/assets/images/posts/sleeper-dreaming.png" class="post-marker" alt="" /></p>

<p>What if your AI coding assistant could learn from its mistakes—not just for one session, but across training cycles?</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2602.06043">Share: Shared LoRA Subspaces</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/sleepy-coder">sleepy-coder</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td>Coming soon</td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="references">References</h2>

<table>
  <thead>
    <tr>
      <th>Concept</th>
      <th>Reference</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Share</strong></td>
      <td><a href="https://arxiv.org/abs/2602.06043">Shared LoRA Subspaces for almost Strict Continual Learning</a> (Kaushik et al., Johns Hopkins 2026)</td>
    </tr>
    <tr>
      <td><strong>UWSH</strong></td>
      <td><a href="https://arxiv.org/abs/2512.05117">Universal Weight Subspace Hypothesis</a> (Kaushik et al., Johns Hopkins 2025)</td>
    </tr>
    <tr>
      <td><strong>LoRA</strong></td>
      <td><a href="https://arxiv.org/abs/2106.09685">Low-Rank Adaptation of Large Language Models</a> (Hu et al., Microsoft 2021)</td>
    </tr>
  </tbody>
</table>

<div style="display:none">

</div>

<h2 id="the-problem">The Problem</h2>

<p>AI coding agents have a memory problem. They fix a bug today, then make the same mistake next week. Every session starts from the same frozen model. Nothing carries forward.</p>

<p>Current agents are stateless learners—they can use tools, follow instructions, and generate code, but they don’t improve from experience. The model that struggles with borrow checker errors on Monday is equally confused by Friday.</p>

<h2 id="sleep-learning">Sleep Learning</h2>

<p>The idea is simple: learn while you sleep.</p>

<p><strong>Day phase</strong>: The agent works on coding tasks. We log its failures—the error messages, the broken code, and the fixes that eventually worked.</p>

<p><strong>Sleep phase</strong>: Overnight, we fine-tune the model on those failures using parameter-efficient methods (LoRA).</p>

<p><strong>Eval phase</strong>: We test for improvement on new tasks and regression on old ones. Only if tests pass do we promote the new checkpoint.</p>

<p>Each morning, a new model wakes up a little better than before.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   DAY LOOP  │     │ SLEEP LOOP  │     │  EVAL LOOP  │
│ (Rust Agent)│ --&gt; │(Python Train)│ --&gt; │ (Rust Eval) │
└─────────────┘     └─────────────┘     └─────────────┘
       |                   |                   |
       v                   v                   v
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│episodes.sqlite│   │  adapter/   │     │metrics.jsonl│
│  (capture)  │     │(LoRA weights)│    │  (results)  │
└─────────────┘     └─────────────┘     └─────────────┘
</code></pre></div></div>

<h2 id="why-this-might-work">Why This Might Work</h2>

<p>Two recent papers from Johns Hopkins University (Kaushik, Chaudhari, Vaidya, Chellappa, Yuille, 2025-2026) suggest this approach is grounded in solid theory:</p>

<h3 id="share-shared-lora-subspaces">Share: Shared LoRA Subspaces</h3>

<p>The <a href="https://arxiv.org/abs/2602.06043">Share paper</a> shows that continual LoRA training can work with a single evolving shared low-rank basis. Instead of keeping separate adapters for each task, you:</p>

<ol>
  <li>Build a shared basis from past task adapters (via SVD)</li>
  <li>Store only tiny per-task coefficients</li>
  <li>Expand the basis when new tasks don’t fit</li>
</ol>

<p>This dramatically reduces storage and enables knowledge transfer between related tasks.</p>

<h3 id="uwsh-universal-weight-subspace-hypothesis">UWSH: Universal Weight Subspace Hypothesis</h3>

<p>The <a href="https://arxiv.org/abs/2512.05117">UWSH paper</a> goes further: models of the same architecture naturally converge to similar low-dimensional spectral subspaces. These subspaces are stable across tasks, initializations, and even modalities.</p>

<p>The implication: continual learning might mostly be coefficient updates in a frozen basis, not constant retraining.</p>

<h2 id="the-architecture">The Architecture</h2>

<p>Sleepy Coder uses a dual-language design:</p>

<p><strong>Rust</strong> handles the agent runtime, episode capture, and evaluation. Rust gives us determinism and reproducibility—critical for measuring improvement.</p>

<p><strong>Python</strong> handles training, embeddings, and visualization. The HuggingFace/PEFT ecosystem makes parameter-efficient fine-tuning straightforward.</p>

<h3 id="task-system">Task System</h3>

<p>We use “Rust Koans”—small buggy code snippets that exercise common error patterns:</p>

<table>
  <thead>
    <tr>
      <th>Error Family</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Borrow checker</td>
      <td>Moved values, dangling borrows</td>
    </tr>
    <tr>
      <td>Lifetimes</td>
      <td>Missing annotations</td>
    </tr>
    <tr>
      <td>Trait bounds</td>
      <td>Missing implementations</td>
    </tr>
    <tr>
      <td>Result handling</td>
      <td>Unwrap/? misuse</td>
    </tr>
    <tr>
      <td>Type mismatch</td>
      <td>Iterator types, generics</td>
    </tr>
  </tbody>
</table>

<p>Each task has buggy code, expected error, and correct fix—enabling automated evaluation.</p>

<h3 id="episode-capture">Episode Capture</h3>

<p>Every agent interaction is logged:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">pub</span> <span class="k">struct</span> <span class="n">Episode</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="n">task_id</span><span class="p">:</span> <span class="nb">String</span><span class="p">,</span>
    <span class="k">pub</span> <span class="n">error_signature</span><span class="p">:</span> <span class="nb">String</span><span class="p">,</span>  <span class="c1">// normalized</span>
    <span class="k">pub</span> <span class="n">diff_unified</span><span class="p">:</span> <span class="nb">String</span><span class="p">,</span>     <span class="c1">// the fix</span>
    <span class="k">pub</span> <span class="n">passed</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="k">pub</span> <span class="n">steps_to_green</span><span class="p">:</span> <span class="nb">u32</span><span class="p">,</span>
    <span class="c1">// ...</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Error signatures are normalized to enable clustering and repeat detection.</p>

<h3 id="gated-promotion">Gated Promotion</h3>

<p>Not every training run produces a better model. We gate deployment on:</p>

<ul>
  <li><strong>Repeat error rate</strong>: Does the agent make the same mistakes?</li>
  <li><strong>Steps to green</strong>: How many attempts to solve tasks?</li>
  <li><strong>Regression check</strong>: Does a frozen suite still pass?</li>
</ul>

<p>Only models that improve without regressing get promoted.</p>

<h2 id="current-status">Current Status</h2>

<p>This project is work in progress. The infrastructure is largely built, but we don’t have conclusive results yet.</p>

<p><strong>Done:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Agent runtime (Rust)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Episode capture (SQLite)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Task generator (Rust Koans)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />LoRA training pipeline (Python)</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Evaluation harness (Rust)</li>
</ul>

<p><strong>In Progress:</strong></p>
<ul class="task-list">
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Running multi-day experiments</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Measuring repeat error rates</li>
  <li class="task-list-item"><input type="checkbox" class="task-list-item-checkbox" disabled="disabled" />Testing Share-style consolidation</li>
</ul>

<p><strong>Open Questions:</strong></p>
<ul>
  <li>How many sleep cycles before measurable improvement?</li>
  <li>Does the shared basis approach scale?</li>
  <li>What’s the optimal task curriculum?</li>
</ul>

<h2 id="why-this-matters">Why This Matters</h2>

<p>If sleep learning works, it changes how we think about AI agents:</p>

<ol>
  <li><strong>Agents could specialize</strong> to your codebase, your error patterns, your style</li>
  <li><strong>Mistakes become training data</strong>, not just frustration</li>
  <li><strong>Small models could outperform large ones</strong> on familiar tasks</li>
  <li><strong>Continual improvement</strong> replaces periodic retraining</li>
</ol>

<p>This is what “learning from experience” should mean for AI.</p>

<h2 id="whats-next">What’s Next</h2>

<p>We’re running experiments now. Follow for updates as we collect results.</p>

<p>The code is open source—contributions welcome.</p>

<hr />

<p><em>Sleepy Coder: Parameter-efficient continual learning for AI coding agents.</em></p>

  </div><div class="series-nav">
    <p><em>Part  of the  series. <a href="/series/#">View all parts</a></em></p>
  </div>





<img src="/assets/images/site/post-separator.png" class="post-separator" alt=""><a class="u-url" href="/2026/03/30/sleepy-coder-continual-learning/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Software Wrighter Lab Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Mike Wright</li><li><a class="u-email" href="mailto:"></a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/softwarewrighter"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">softwarewrighter</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>AI coding agents, systems programming, and practical machine learning</p>
      </div>
    </div>

    <div class="footer-copyright">
      <p>Copyright &copy; 2026 Michael A. Wright</p>
    </div>

  </div>

  <button id="scroll-to-top" aria-label="Scroll to top" title="Scroll to top">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <polyline points="18 15 12 9 6 15"></polyline>
    </svg>
  </button>

  <style>
  #scroll-to-top {
    position: fixed;
    bottom: 2rem;
    right: 2rem;
    width: 44px;
    height: 44px;
    border-radius: 50%;
    border: none;
    background: var(--brand-color, #2a7ae2);
    color: white;
    cursor: pointer;
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.3s, visibility 0.3s, transform 0.2s;
    z-index: 1000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
  }

  #scroll-to-top:hover {
    transform: scale(1.1);
  }

  #scroll-to-top.visible {
    opacity: 1;
    visibility: visible;
  }

  #scroll-to-top svg {
    width: 24px;
    height: 24px;
  }
  </style>

  <script>
  (function() {
    const btn = document.getElementById('scroll-to-top');
    if (!btn) return;

    window.addEventListener('scroll', function() {
      if (window.scrollY > 300) {
        btn.classList.add('visible');
      } else {
        btn.classList.remove('visible');
      }
    });

    btn.addEventListener('click', function() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    });
  })();
  </script>

</footer>
</body>

</html>
