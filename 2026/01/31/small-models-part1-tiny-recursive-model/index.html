<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Small Models (1/6): 976 Parameters Beat Billions | Software Wrighter Lab Blog</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Small Models (1/6): 976 Parameters Beat Billions" />
<meta name="author" content="Software Wrighter" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The best large language models score zero on hard mazes. A model with under 1,000 parameters scores 85 percent. This is Part 1 of the Small Models, Big Brains series, exploring how tiny models with clever architectures outperform massive ones on specific tasks. Resource Link Paper Tiny Recursive Model Code train-trm Video 976 parameters is more than billions?! Why LLMs Fail at Mazes Large language models generate one token at a time. They cannot backtrack. One wrong move and the entire solution fails. Maze solving requires: Exploring dead ends Backtracking when stuck Maintaining spatial awareness Planning multiple steps ahead Autoregressive generation is fundamentally incompatible with these requirements. Meet TRM: The Tiny Recursive Model The Tiny Recursive Model uses under 1,000 parameters. Instead of being bigger, it thinks in loops. Input → Think → Act → Think → Act → ... → Output A simple two-layer network that iterates until the solution emerges. The Architecture TRM alternates between two phases: Phase Purpose Think Update internal latent state by processing input, current answer, and previous state Act Update the answer based on the refined latent state This process repeats for multiple cycles, progressively improving the output. TRMConfig { input_dim: 5, output_dim: 5, hidden_dim: 16, latent_dim: 16, l_layers: 2, // Network depth h_cycles: 3, // Outer think-act cycles l_cycles: 4, // Inner think cycles } The Secret: Deep Supervision The key insight isn’t just recursion—it’s supervising every step, not just the final answer. Traditional training: Input → [black box] → Final Output → Loss TRM training: Input → Step 1 → Loss₁ → Step 2 → Loss₂ → Step 3 → Loss₃ → ... → Final → Loss_n Every iteration gets feedback. The model learns to make progress at each step. Results Model Maze Accuracy GPT-4 ~0% on hard mazes Claude ~0% on hard mazes TRM (976 params) 85% Iteration beats scale. Running the Code The train-trm repo provides a complete Rust implementation: # Clone and build git clone https://github.com/softwarewrighter/train-trm cd train-trm ./scripts/build.sh --release # Train a model ./scripts/train.sh --epochs 1000 --lr 0.01 # Evaluate ./scripts/eval.sh # Or launch the web UI cargo install --locked trunk ./scripts/web-serve.sh The web UI includes interactive maze visualization with solution paths and real-time training charts. Implementation Details Metric Value Primary Language Rust Source Files 21 .rs files Estimated Size ~2.5 KLOC Also Includes HTML (web UI), Shell scripts Build System Cargo + Trunk (WASM) Dependencies ndarray, serde, clap, wasm-bindgen Good for you if: You want to learn Rust ML from scratch, experiment with recursive architectures, or need a web-based training visualization. Complexity: Moderate. Clean Rust code with good documentation. The neural network is implemented from scratch (no PyTorch/TensorFlow), making it educational but requiring Rust familiarity. Key Takeaways Parameter count isn’t everything. Architecture and training strategy matter more for certain tasks. Recursion enables backtracking. By iterating, TRM can explore and refine solutions. Deep supervision accelerates learning. Feedback at every step, not just the end. Task-specific models excel. TRM isn’t a general-purpose LLM—it’s optimized for maze-like reasoning. What’s Next Part 2 explores MobileLLM and running AI completely offline on your Android phone. Resources TRM Paper train-trm Repository Video: 976 parameters is more than billions?! *Part 1 of 6 in the Small Models, Big Brains series. View all parts Next: Part 2 →*" />
<meta property="og:description" content="The best large language models score zero on hard mazes. A model with under 1,000 parameters scores 85 percent. This is Part 1 of the Small Models, Big Brains series, exploring how tiny models with clever architectures outperform massive ones on specific tasks. Resource Link Paper Tiny Recursive Model Code train-trm Video 976 parameters is more than billions?! Why LLMs Fail at Mazes Large language models generate one token at a time. They cannot backtrack. One wrong move and the entire solution fails. Maze solving requires: Exploring dead ends Backtracking when stuck Maintaining spatial awareness Planning multiple steps ahead Autoregressive generation is fundamentally incompatible with these requirements. Meet TRM: The Tiny Recursive Model The Tiny Recursive Model uses under 1,000 parameters. Instead of being bigger, it thinks in loops. Input → Think → Act → Think → Act → ... → Output A simple two-layer network that iterates until the solution emerges. The Architecture TRM alternates between two phases: Phase Purpose Think Update internal latent state by processing input, current answer, and previous state Act Update the answer based on the refined latent state This process repeats for multiple cycles, progressively improving the output. TRMConfig { input_dim: 5, output_dim: 5, hidden_dim: 16, latent_dim: 16, l_layers: 2, // Network depth h_cycles: 3, // Outer think-act cycles l_cycles: 4, // Inner think cycles } The Secret: Deep Supervision The key insight isn’t just recursion—it’s supervising every step, not just the final answer. Traditional training: Input → [black box] → Final Output → Loss TRM training: Input → Step 1 → Loss₁ → Step 2 → Loss₂ → Step 3 → Loss₃ → ... → Final → Loss_n Every iteration gets feedback. The model learns to make progress at each step. Results Model Maze Accuracy GPT-4 ~0% on hard mazes Claude ~0% on hard mazes TRM (976 params) 85% Iteration beats scale. Running the Code The train-trm repo provides a complete Rust implementation: # Clone and build git clone https://github.com/softwarewrighter/train-trm cd train-trm ./scripts/build.sh --release # Train a model ./scripts/train.sh --epochs 1000 --lr 0.01 # Evaluate ./scripts/eval.sh # Or launch the web UI cargo install --locked trunk ./scripts/web-serve.sh The web UI includes interactive maze visualization with solution paths and real-time training charts. Implementation Details Metric Value Primary Language Rust Source Files 21 .rs files Estimated Size ~2.5 KLOC Also Includes HTML (web UI), Shell scripts Build System Cargo + Trunk (WASM) Dependencies ndarray, serde, clap, wasm-bindgen Good for you if: You want to learn Rust ML from scratch, experiment with recursive architectures, or need a web-based training visualization. Complexity: Moderate. Clean Rust code with good documentation. The neural network is implemented from scratch (no PyTorch/TensorFlow), making it educational but requiring Rust familiarity. Key Takeaways Parameter count isn’t everything. Architecture and training strategy matter more for certain tasks. Recursion enables backtracking. By iterating, TRM can explore and refine solutions. Deep supervision accelerates learning. Feedback at every step, not just the end. Task-specific models excel. TRM isn’t a general-purpose LLM—it’s optimized for maze-like reasoning. What’s Next Part 2 explores MobileLLM and running AI completely offline on your Android phone. Resources TRM Paper train-trm Repository Video: 976 parameters is more than billions?! *Part 1 of 6 in the Small Models, Big Brains series. View all parts Next: Part 2 →*" />
<link rel="canonical" href="https://software-wrighter-lab.github.io/2026/01/31/small-models-part1-tiny-recursive-model/" />
<meta property="og:url" content="https://software-wrighter-lab.github.io/2026/01/31/small-models-part1-tiny-recursive-model/" />
<meta property="og:site_name" content="Software Wrighter Lab Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2026-01-31T00:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Small Models (1/6): 976 Parameters Beat Billions" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Software Wrighter"},"dateModified":"2026-01-31T00:00:00-08:00","datePublished":"2026-01-31T00:00:00-08:00","description":"The best large language models score zero on hard mazes. A model with under 1,000 parameters scores 85 percent. This is Part 1 of the Small Models, Big Brains series, exploring how tiny models with clever architectures outperform massive ones on specific tasks. Resource Link Paper Tiny Recursive Model Code train-trm Video 976 parameters is more than billions?! Why LLMs Fail at Mazes Large language models generate one token at a time. They cannot backtrack. One wrong move and the entire solution fails. Maze solving requires: Exploring dead ends Backtracking when stuck Maintaining spatial awareness Planning multiple steps ahead Autoregressive generation is fundamentally incompatible with these requirements. Meet TRM: The Tiny Recursive Model The Tiny Recursive Model uses under 1,000 parameters. Instead of being bigger, it thinks in loops. Input → Think → Act → Think → Act → ... → Output A simple two-layer network that iterates until the solution emerges. The Architecture TRM alternates between two phases: Phase Purpose Think Update internal latent state by processing input, current answer, and previous state Act Update the answer based on the refined latent state This process repeats for multiple cycles, progressively improving the output. TRMConfig { input_dim: 5, output_dim: 5, hidden_dim: 16, latent_dim: 16, l_layers: 2, // Network depth h_cycles: 3, // Outer think-act cycles l_cycles: 4, // Inner think cycles } The Secret: Deep Supervision The key insight isn’t just recursion—it’s supervising every step, not just the final answer. Traditional training: Input → [black box] → Final Output → Loss TRM training: Input → Step 1 → Loss₁ → Step 2 → Loss₂ → Step 3 → Loss₃ → ... → Final → Loss_n Every iteration gets feedback. The model learns to make progress at each step. Results Model Maze Accuracy GPT-4 ~0% on hard mazes Claude ~0% on hard mazes TRM (976 params) 85% Iteration beats scale. Running the Code The train-trm repo provides a complete Rust implementation: # Clone and build git clone https://github.com/softwarewrighter/train-trm cd train-trm ./scripts/build.sh --release # Train a model ./scripts/train.sh --epochs 1000 --lr 0.01 # Evaluate ./scripts/eval.sh # Or launch the web UI cargo install --locked trunk ./scripts/web-serve.sh The web UI includes interactive maze visualization with solution paths and real-time training charts. Implementation Details Metric Value Primary Language Rust Source Files 21 .rs files Estimated Size ~2.5 KLOC Also Includes HTML (web UI), Shell scripts Build System Cargo + Trunk (WASM) Dependencies ndarray, serde, clap, wasm-bindgen Good for you if: You want to learn Rust ML from scratch, experiment with recursive architectures, or need a web-based training visualization. Complexity: Moderate. Clean Rust code with good documentation. The neural network is implemented from scratch (no PyTorch/TensorFlow), making it educational but requiring Rust familiarity. Key Takeaways Parameter count isn’t everything. Architecture and training strategy matter more for certain tasks. Recursion enables backtracking. By iterating, TRM can explore and refine solutions. Deep supervision accelerates learning. Feedback at every step, not just the end. Task-specific models excel. TRM isn’t a general-purpose LLM—it’s optimized for maze-like reasoning. What’s Next Part 2 explores MobileLLM and running AI completely offline on your Android phone. Resources TRM Paper train-trm Repository Video: 976 parameters is more than billions?! *Part 1 of 6 in the Small Models, Big Brains series. View all parts Next: Part 2 →*","headline":"Small Models (1/6): 976 Parameters Beat Billions","mainEntityOfPage":{"@type":"WebPage","@id":"https://software-wrighter-lab.github.io/2026/01/31/small-models-part1-tiny-recursive-model/"},"url":"https://software-wrighter-lab.github.io/2026/01/31/small-models-part1-tiny-recursive-model/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://software-wrighter-lab.github.io/feed.xml" title="Software Wrighter Lab Blog" /><!-- Theme toggle script - load early to prevent flash -->
  <script>
    (function() {
      var stored = localStorage.getItem('sw-lab-theme');
      var theme = stored;
      if (!theme) {
        theme = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';
      }
      document.documentElement.setAttribute('data-theme', theme);
    })();
  </script>
  <!-- Font size - load early to prevent flash -->
  <script>
    (function() {
      var NEW_DEFAULT = 110;
      var OLD_DEFAULT = 150;
      var stored = localStorage.getItem('sw-lab-font-size');
      var ack = localStorage.getItem('sw-lab-prefs-ack');
      var size = stored ? parseInt(stored, 10) : null;

      // Migration: if on old default and not acknowledged, use new default
      if (size === OLD_DEFAULT && ack !== 'true') {
        size = NEW_DEFAULT;
      } else if (size === null) {
        size = NEW_DEFAULT;
      }

      // Apply to post content when DOM is ready
      document.addEventListener('DOMContentLoaded', function() {
        var pc = document.querySelector('.post-content');
        if (pc) pc.style.fontSize = size + '%';
        var pl = document.querySelector('.post-list');
        if (pl) pl.style.fontSize = size + '%';
      });
    })();
  </script>
  <script src="/assets/js/theme-toggle.js" defer></script>
  <script src="/assets/js/font-size.js" defer></script>
  <script src="/assets/js/preferences.js" defer></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">
      <img src="/assets/images/site/logo.jpg" alt="Software Wrighter Lab Blog" class="site-logo">
      <span class="site-title-text">Software Wrighter Lab Blog</span>
    </a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/abstracts/">Abstracts</a><a class="page-link" href="/index-all/">Index</a><a class="page-link" href="/series/">Series</a><a class="page-link" href="/tags/">Tags</a><a class="page-link" href="/categories/">Categories</a><a class="page-link" href="/search/">Search</a><div class="font-size-controls">
  <button class="font-size-btn" id="font-decrease" title="Decrease font size" aria-label="Decrease font size">A-</button>
  <button class="font-size-btn" id="font-reset" title="Reset font size" aria-label="Reset font size">A</button>
  <button class="font-size-btn" id="font-increase" title="Increase font size" aria-label="Increase font size">A+</button>
</div>
<button class="theme-toggle" aria-label="Switch theme" title="Switch theme">
  <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <circle cx="12" cy="12" r="5"></circle>
    <line x1="12" y1="1" x2="12" y2="3"></line>
    <line x1="12" y1="21" x2="12" y2="23"></line>
    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
    <line x1="1" y1="12" x2="3" y2="12"></line>
    <line x1="21" y1="12" x2="23" y2="12"></line>
    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
  </svg>
  <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
  </svg>
</button>
</div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Small Models (1/6): 976 Parameters Beat Billions</h1><p class="post-meta">January 31, 2026 &bull; Software Wrighter</p>
<p class="post-reading-info"><em><span class="post-word-count">729 words</span> &bull; <span class="post-read-time">4 min read</span></em> &bull; <span class="abstract-toggle" onclick="this.classList.toggle('open'); this.parentElement.nextElementSibling.classList.toggle('open')">Abstract</span></p><div class="post-meta-abstract">The best LLMs score zero on hard mazes. A model with 976 parameters scores 85%. The Tiny Recursive Model uses think-act cycles with deep supervision, proving iteration beats scale for tasks requiring backtracking and spatial reasoning.</div><div class="post-taxonomies"><span class="post-categories"><a href="/categories/#llm" class="category">llm</a><a href="/categories/#machine-learning" class="category">machine-learning</a><a href="/categories/#research" class="category">research</a></span><span class="post-tags"><a href="/tags/#trm" class="tag">trm</a><a href="/tags/#tiny-recursive-model" class="tag">tiny-recursive-model</a><a href="/tags/#maze-solving" class="tag">maze-solving</a><a href="/tags/#recursive-reasoning" class="tag">recursive-reasoning</a></span></div></header><nav class="toc" data-toc-id="default">
  <h4>Contents</h4>
  <ul class="toc-list"></ul>
</nav>

<style>
/* Post content flows around floated TOC */
.post-content {
  overflow: hidden;
}

.toc {
  background: #fffde7;
  border-radius: 8px;
  padding: 1rem 1.5rem;
  margin: 0 1.5rem 1rem 0;
  float: left;
  max-width: 280px;
}

[data-theme="dark"] .toc {
  background: #3d3a00;
}

@media (prefers-color-scheme: dark) {
  :root:not([data-theme="light"]) .toc {
    background: #3d3a00;
  }
}

.toc h4 {
  margin: 0 0 0.75rem 0;
  font-size: 1rem;
}

.toc ul {
  margin: 0;
  padding-left: 1.25rem;
  list-style-type: disc;
}

.toc li {
  margin: 0.25rem 0;
}

.toc li.toc-h3 {
  margin-left: 1rem;
  font-size: 0.95em;
}

.toc a {
  text-decoration: none;
}

.toc a:hover {
  text-decoration: underline;
}


@media (max-width: 600px) {
  .toc {
    float: none;
    max-width: 100%;
    margin: 0 0 1.5rem 0;
  }
}

/* References section that floats alongside TOC */
.references-float {
  overflow: hidden; /* contain floated content */
}

.references-float h2 {
  margin-top: 0;
  font-size: 1.25rem;
}

.references-float table {
  font-size: 0.9em;
  width: 100%;
}

.references-float td, .references-float th {
  padding: 0.4rem 0.6rem;
  vertical-align: top;
}

.references-float td:first-child {
  white-space: nowrap;
  font-weight: bold;
  width: 1%;
}

@media (max-width: 600px) {
  .references-float table {
    font-size: 0.85em;
  }
}
</style>

<script>
(function() {
  // Run after DOM is ready
  function initTOC() {
    document.querySelectorAll('.toc').forEach(function(toc) {
      if (toc.dataset.initialized) return;
      toc.dataset.initialized = 'true';

      const tocList = toc.querySelector('.toc-list');
      if (!tocList) return;

      // Find the associated post-content (next sibling or parent's post-content)
      let article = toc.nextElementSibling;
      while (article && !article.classList.contains('post-content')) {
        article = article.nextElementSibling;
      }
      // Fallback: look for .post-content in the document (single post page)
      if (!article) {
        article = document.querySelector('.post-content');
      }

      if (!article) {
        toc.style.display = 'none';
        return;
      }

      const headings = article.querySelectorAll('h2, h3');

      if (headings.length < 3) {
        toc.style.display = 'none';
        return;
      }

      const tocId = toc.dataset.tocId || Math.random().toString(36).substr(2, 9);

      headings.forEach(function(heading, index) {
        // Add unique ID if missing
        if (!heading.id) {
          heading.id = 'toc-' + tocId + '-heading-' + index;
        }

        const li = document.createElement('li');
        li.className = 'toc-' + heading.tagName.toLowerCase();

        const a = document.createElement('a');
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        li.appendChild(a);
        tocList.appendChild(li);
      });
    });
  }

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initTOC);
  } else {
    initTOC();
  }
})();
</script>
<div class="post-content e-content" itemprop="articleBody">
    <p><img src="/assets/images/site/post-marker-coffee-pot.png?ts=1769815140000" class="post-marker" alt="" /></p>

<p>The best large language models score zero on hard mazes. A model with under 1,000 parameters scores 85 percent.</p>

<p>This is Part 1 of the <strong>Small Models, Big Brains</strong> series, exploring how tiny models with clever architectures outperform massive ones on specific tasks.</p>

<div class="resource-box">

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Link</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><strong>Paper</strong></td>
        <td><a href="https://arxiv.org/abs/2510.04871">Tiny Recursive Model</a></td>
      </tr>
      <tr>
        <td><strong>Code</strong></td>
        <td><a href="https://github.com/softwarewrighter/train-trm">train-trm</a></td>
      </tr>
      <tr>
        <td><strong>Video</strong></td>
        <td><a href="https://www.youtube.com/shorts/O6U06cGkKc4">976 parameters is more than billions?!</a><br /><a href="https://www.youtube.com/shorts/O6U06cGkKc4"><img src="https://img.youtube.com/vi/O6U06cGkKc4/mqdefault.jpg" alt="Video" class="video-thumb" /></a></td>
      </tr>
    </tbody>
  </table>

</div>

<h2 id="why-llms-fail-at-mazes">Why LLMs Fail at Mazes</h2>

<p>Large language models generate one token at a time. They cannot backtrack. One wrong move and the entire solution fails.</p>

<p>Maze solving requires:</p>
<ul>
  <li>Exploring dead ends</li>
  <li>Backtracking when stuck</li>
  <li>Maintaining spatial awareness</li>
  <li>Planning multiple steps ahead</li>
</ul>

<p>Autoregressive generation is fundamentally incompatible with these requirements.</p>

<h2 id="meet-trm-the-tiny-recursive-model">Meet TRM: The Tiny Recursive Model</h2>

<p>The Tiny Recursive Model uses under 1,000 parameters. Instead of being bigger, it thinks in loops.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input → Think → Act → Think → Act → ... → Output
</code></pre></div></div>

<p>A simple two-layer network that iterates until the solution emerges.</p>

<h3 id="the-architecture">The Architecture</h3>

<p>TRM alternates between two phases:</p>

<table>
  <thead>
    <tr>
      <th>Phase</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Think</strong></td>
      <td>Update internal latent state by processing input, current answer, and previous state</td>
    </tr>
    <tr>
      <td><strong>Act</strong></td>
      <td>Update the answer based on the refined latent state</td>
    </tr>
  </tbody>
</table>

<p>This process repeats for multiple cycles, progressively improving the output.</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">TRMConfig</span> <span class="p">{</span>
    <span class="n">input_dim</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">output_dim</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">latent_dim</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
    <span class="n">l_layers</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>      <span class="c1">// Network depth</span>
    <span class="n">h_cycles</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>      <span class="c1">// Outer think-act cycles</span>
    <span class="n">l_cycles</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>      <span class="c1">// Inner think cycles</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="the-secret-deep-supervision">The Secret: Deep Supervision</h2>

<p>The key insight isn’t just recursion—it’s supervising every step, not just the final answer.</p>

<p>Traditional training:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input → [black box] → Final Output → Loss
</code></pre></div></div>

<p>TRM training:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input → Step 1 → Loss₁
      → Step 2 → Loss₂
      → Step 3 → Loss₃
      → ...
      → Final  → Loss_n
</code></pre></div></div>

<p>Every iteration gets feedback. The model learns to make progress at each step.</p>

<h2 id="results">Results</h2>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Maze Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GPT-4</td>
      <td>~0% on hard mazes</td>
    </tr>
    <tr>
      <td>Claude</td>
      <td>~0% on hard mazes</td>
    </tr>
    <tr>
      <td><strong>TRM (976 params)</strong></td>
      <td><strong>85%</strong></td>
    </tr>
  </tbody>
</table>

<p>Iteration beats scale.</p>

<h2 id="running-the-code">Running the Code</h2>

<p>The <a href="https://github.com/softwarewrighter/train-trm">train-trm</a> repo provides a complete Rust implementation:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Clone and build</span>
git clone https://github.com/softwarewrighter/train-trm
<span class="nb">cd </span>train-trm
./scripts/build.sh <span class="nt">--release</span>

<span class="c"># Train a model</span>
./scripts/train.sh <span class="nt">--epochs</span> 1000 <span class="nt">--lr</span> 0.01

<span class="c"># Evaluate</span>
./scripts/eval.sh

<span class="c"># Or launch the web UI</span>
cargo <span class="nb">install</span> <span class="nt">--locked</span> trunk
./scripts/web-serve.sh
</code></pre></div></div>

<p>The web UI includes interactive maze visualization with solution paths and real-time training charts.</p>

<h2 id="implementation-details">Implementation Details</h2>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Primary Language</strong></td>
      <td>Rust</td>
    </tr>
    <tr>
      <td><strong>Source Files</strong></td>
      <td>21 <code class="language-plaintext highlighter-rouge">.rs</code> files</td>
    </tr>
    <tr>
      <td><strong>Estimated Size</strong></td>
      <td>~2.5 KLOC</td>
    </tr>
    <tr>
      <td><strong>Also Includes</strong></td>
      <td>HTML (web UI), Shell scripts</td>
    </tr>
    <tr>
      <td><strong>Build System</strong></td>
      <td>Cargo + Trunk (WASM)</td>
    </tr>
    <tr>
      <td><strong>Dependencies</strong></td>
      <td>ndarray, serde, clap, wasm-bindgen</td>
    </tr>
  </tbody>
</table>

<p><strong>Good for you if:</strong> You want to learn Rust ML from scratch, experiment with recursive architectures, or need a web-based training visualization.</p>

<p><strong>Complexity:</strong> Moderate. Clean Rust code with good documentation. The neural network is implemented from scratch (no PyTorch/TensorFlow), making it educational but requiring Rust familiarity.</p>

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>Parameter count isn’t everything.</strong> Architecture and training strategy matter more for certain tasks.</p>
  </li>
  <li>
    <p><strong>Recursion enables backtracking.</strong> By iterating, TRM can explore and refine solutions.</p>
  </li>
  <li>
    <p><strong>Deep supervision accelerates learning.</strong> Feedback at every step, not just the end.</p>
  </li>
  <li>
    <p><strong>Task-specific models excel.</strong> TRM isn’t a general-purpose LLM—it’s optimized for maze-like reasoning.</p>
  </li>
</ol>

<h2 id="whats-next">What’s Next</h2>

<p>Part 2 explores <strong>MobileLLM</strong> and running AI completely offline on your Android phone.</p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2510.04871">TRM Paper</a></li>
  <li><a href="https://github.com/softwarewrighter/train-trm">train-trm Repository</a></li>
  <li><a href="https://www.youtube.com/shorts/O6U06cGkKc4">Video: 976 parameters is more than billions?!</a></li>
</ul>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Part 1 of 6 in the Small Models, Big Brains series. <a href="/series/#small-models-big-brains">View all parts</a></td>
      <td><a href="/2026/02/01/small-models-part2-pocket-llm/">Next: Part 2 →</a>*</td>
    </tr>
  </tbody>
</table>

  </div>





<div class="youtube-embed-container" id="yt-container-O6U06cGkKc4">
  <h3>Watch the Video</h3>
  <div class="youtube-embed-wrapper">
    <iframe
      id="yt-player-O6U06cGkKc4"
      src="https://www.youtube.com/embed/O6U06cGkKc4?enablejsapi=1&mute=1&autoplay=1&cc_load_policy=1"
      title="YouTube video player"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      allowfullscreen
      loading="lazy">
    </iframe>
  </div>
  <p class="youtube-embed-note">Unmute to hear narration.</p>
</div>

<style>
.youtube-embed-container {
  margin: 2rem auto;
  padding: 1.5rem;
  background: var(--code-background-color, #f5f5f5);
  border-radius: 8px;
  width: 66%;
}

.youtube-embed-container h3 {
  margin-top: 0;
  margin-bottom: 1rem;
}

.youtube-embed-wrapper {
  position: relative;
  width: 100%;
  padding-bottom: 56.25%; /* 16:9 aspect ratio */
  height: 0;
  overflow: hidden;
}

.youtube-embed-wrapper iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  border-radius: 4px;
}

.youtube-embed-note {
  margin-top: 0.75rem;
  margin-bottom: 0;
  font-size: 0.85rem;
  color: var(--text-muted-color, #666);
  font-style: italic;
}
</style>

<script>
(function() {
  const containerId = 'yt-container-O6U06cGkKc4';
  const playerId = 'yt-player-O6U06cGkKc4';

  // Load YouTube IFrame API if not already loaded
  if (!window.YT) {
    const tag = document.createElement('script');
    tag.src = 'https://www.youtube.com/iframe_api';
    const firstScriptTag = document.getElementsByTagName('script')[0];
    firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);
  }

  let player;
  let isPlaying = false;

  function onYouTubeIframeAPIReady() {
    player = new YT.Player(playerId, {
      events: {
        'onReady': onPlayerReady
      }
    });
  }

  function onPlayerReady(event) {
    setupIntersectionObserver();
  }

  function setupIntersectionObserver() {
    const container = document.getElementById(containerId);
    if (!container) return;

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting && entry.intersectionRatio >= 0.5) {
          if (!isPlaying && player && player.playVideo) {
            player.playVideo();
            isPlaying = true;
          }
        } else {
          if (isPlaying && player && player.pauseVideo) {
            player.pauseVideo();
            isPlaying = false;
          }
        }
      });
    }, {
      threshold: [0.5]
    });

    observer.observe(container);
  }

  // Handle API ready callback
  if (window.YT && window.YT.Player) {
    onYouTubeIframeAPIReady();
  } else {
    window.onYouTubeIframeAPIReady = window.onYouTubeIframeAPIReady || onYouTubeIframeAPIReady;
    // Queue multiple players if needed
    const existingCallback = window.onYouTubeIframeAPIReady;
    window.onYouTubeIframeAPIReady = function() {
      if (existingCallback) existingCallback();
      onYouTubeIframeAPIReady();
    };
  }
})();
</script>

<img src="/assets/images/site/post-separator.png" class="post-separator" alt=""><a class="u-url" href="/2026/01/31/small-models-part1-tiny-recursive-model/" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Software Wrighter Lab Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Mike Wright</li><li><a class="u-email" href="mailto:"></a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/softwarewrighter"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">softwarewrighter</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>AI coding agents, systems programming, and practical machine learning</p>
      </div>
    </div>

    <div class="footer-copyright">
      <p>Copyright &copy; 2026 Michael A. Wright</p>
    </div>

  </div>

  <button id="scroll-to-top" aria-label="Scroll to top" title="Scroll to top">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <polyline points="18 15 12 9 6 15"></polyline>
    </svg>
  </button>

  <style>
  #scroll-to-top {
    position: fixed;
    bottom: 2rem;
    right: 2rem;
    width: 44px;
    height: 44px;
    border-radius: 50%;
    border: none;
    background: var(--brand-color, #2a7ae2);
    color: white;
    cursor: pointer;
    opacity: 0;
    visibility: hidden;
    transition: opacity 0.3s, visibility 0.3s, transform 0.2s;
    z-index: 1000;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.2);
  }

  #scroll-to-top:hover {
    transform: scale(1.1);
  }

  #scroll-to-top.visible {
    opacity: 1;
    visibility: visible;
  }

  #scroll-to-top svg {
    width: 24px;
    height: 24px;
  }
  </style>

  <script>
  (function() {
    const btn = document.getElementById('scroll-to-top');
    if (!btn) return;

    window.addEventListener('scroll', function() {
      if (window.scrollY > 300) {
        btn.classList.add('visible');
      } else {
        btn.classList.remove('visible');
      }
    });

    btn.addEventListener('click', function() {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    });
  })();
  </script>

</footer>
</body>

</html>
