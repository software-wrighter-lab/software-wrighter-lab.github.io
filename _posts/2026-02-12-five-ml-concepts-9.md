---
layout: post
title: "Five ML Concepts - #9"
date: 2026-02-12 00:00:00 -0800
categories: [llm, machine-learning, explainers]
tags: [five-ml-concepts, dropout, rlhf, inference, quantization, flash-attention, ml-concepts]
keywords: "dropout, RLHF, reinforcement learning from human feedback, inference, quantization, Flash Attention, machine learning concepts, 30-second explainers"
author: Software Wrighter
series: "Five ML Concepts"
series_part: 9
video_url: "https://www.youtube.com/shorts/C5ICKluukxI"
video_title: "Five ML Concepts - #9"
papers:
  - title: "Training language models to follow instructions with human feedback"
    url: "https://arxiv.org/abs/2203.02155"
  - title: "A Survey of Quantization Methods for Efficient Neural Network Inference"
    url: "https://arxiv.org/abs/2103.13630"
  - title: "FlashAttention: Fast and Memory-Efficient Exact Attention"
    url: "https://arxiv.org/abs/2205.14135"
---

<img src="/assets/images/posts/block-nine.png" class="post-marker" alt="">

5 machine learning concepts. Under 30 seconds each.

<div class="resource-box" markdown="1">

| Resource | Link |
|----------|------|
| **Papers** | Links in References section |
| **Video** | [Five ML Concepts #9](https://www.youtube.com/shorts/C5ICKluukxI)<br>[![Video](https://img.youtube.com/vi/C5ICKluukxI/mqdefault.jpg){: .video-thumb}](https://www.youtube.com/shorts/C5ICKluukxI) |

</div>

<div class="references-section" markdown="1">

## References

| Concept | Reference |
|---------|-----------|
| **Dropout** | [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html) (Srivastava et al. 2014) |
| **RLHF** | [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) (Ouyang et al. 2022) |
| **Inference** | [Deep Learning](https://www.deeplearningbook.org/) (Goodfellow et al. 2016), Chapter 5 |
| **Quantization** | [A Survey of Quantization Methods for Efficient Neural Network Inference](https://arxiv.org/abs/2103.13630) (Gholami et al. 2021) |
| **Flash Attention** | [FlashAttention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135) (Dao et al. 2022) |

</div>

<div style="display:none">

</div>

## Today's Five

### 1. Dropout

**A regularization technique that randomly disables units during training.** This encourages the network to rely on multiple pathways instead of memorizing patterns.

It helps reduce overfitting, especially in large models.

> Like training a team where random members sit out each practice, so no one becomes a single point of failure.

### 2. RLHF (Reinforcement Learning from Human Feedback)

**A training approach where humans rank or compare model outputs to produce a reward signal.** The model is then optimized to better match human preferences.

This technique is central to aligning language models with human intent.

> Like teaching by grading essays instead of dictating every word.

### 3. Inference

**The process of running a trained model to make predictions on new data.** Training updates the model's parameters; inference uses them.

The distinction matters for optimization, deployment, and cost.

> Like the difference between studying for an exam and actually taking it.

### 4. Quantization

**Reducing the numerical precision used to store and compute model weights.** This can shrink model size and speed up inference, sometimes with a small accuracy tradeoff.

Essential for deploying large models on limited hardware.

> Like compressing a high-resolution photo into a smaller file that still looks good.

### 5. Flash Attention

**An optimized attention algorithm designed to reduce memory usage.** It avoids materializing the full attention matrix by computing attention in blocks.

This enables longer sequences and faster training.

> Like reading a book chapter by chapter instead of photocopying the whole thing first.

## Quick Reference

| Concept | One-liner |
|---------|-----------|
| **Dropout** | Random disabling to prevent overfitting |
| **RLHF** | Learn from human preference comparisons |
| **Inference** | Using a trained model for predictions |
| **Quantization** | Lower precision for smaller, faster models |
| **Flash Attention** | Block-wise attention for memory efficiency |

---

*Part 9 of the Five ML Concepts series. [View all parts](/series/#five-ml-concepts) | [Next: #10 â†’](/2026/02/13/five-ml-concepts-10/)*

*Short, accurate ML explainers. Follow for more.*
