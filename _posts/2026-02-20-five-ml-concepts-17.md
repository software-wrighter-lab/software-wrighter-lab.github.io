---
layout: post
title: "Five ML Concepts - #17"
date: 2026-02-20 00:00:00 -0800
categories: [llm, machine-learning, explainers]
tags: [five-ml-concepts, benchmark-leakage, concept-drift, data-drift, weight-decay, scaling-laws, shadow-deployment, ml-concepts]
keywords: "benchmark leakage, concept drift, data drift, weight decay, L2 regularization, scaling laws, shadow deployment, machine learning concepts, 30-second explainers"
author: Software Wrighter
series: "Five ML Concepts"
series_part: 17
video_url: "https://www.youtube.com/shorts/Xk2hkc0bgi4"
papers:
  - title: "Rethinking the Inception Architecture for Computer Vision"
    url: "https://arxiv.org/abs/1512.00567"
  - title: "Decoupled Weight Decay Regularization"
    url: "https://arxiv.org/abs/1711.05101"
  - title: "Scaling Laws for Neural Language Models"
    url: "https://arxiv.org/abs/2001.08361"
---

<img src="/assets/images/posts/block-seventeen.png" class="post-marker" alt="">

5 machine learning concepts. Under 30 seconds each.

<div class="resource-box" markdown="1">

| Resource | Link |
|----------|------|
| **Papers** | Links in References section |
| **Video** | [Five ML Concepts #17](https://www.youtube.com/shorts/Xk2hkc0bgi4)<br>[![Video](https://img.youtube.com/vi/Xk2hkc0bgi4/mqdefault.jpg){: .video-thumb}](https://www.youtube.com/shorts/Xk2hkc0bgi4) |

</div>

<div class="references-section" markdown="1">

## References

| Concept | Reference |
|---------|-----------|
| **Benchmark Leakage** | [Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567) (Szegedy et al. 2016) |
| **Concept/Data Drift** | [Learning under Concept Drift: A Review](https://ieeexplore.ieee.org/document/8496795) (Lu et al. 2018) |
| **Weight Decay** | [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101) (Loshchilov & Hutter 2019) |
| **Scaling Laws** | [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) (Kaplan et al. 2020) |
| **Shadow Deployment** | [Reliable Machine Learning](https://www.oreilly.com/library/view/reliable-machine-learning/9781098106218/) (Cathy Chen et al. 2022) |

</div>

## Today's Five

### 1. Benchmark Leakage

**When benchmark or test data influences training, tuning, or model selection, evaluation results become unreliable.** This inflates reported performance beyond real-world capability.

Strict separation between development and evaluation data is essential for honest assessment.

> Like practicing with the exact questions that will appear on the final exam.

### 2. Concept Drift vs Data Drift

**Data drift occurs when input distributions change. Concept drift occurs when the relationship between inputs and outputs changes.** Both can degrade model performance over time.

Data drift: customers buy different products. Concept drift: what "good" means has changed.

> Like customers buying different products versus products changing what they mean.

### 3. Weight Decay

**A regularization method that penalizes large weights, often implemented as L2 regularization.** This encourages simpler models that generalize better.

Weight decay adds a term proportional to the squared magnitude of weights to the loss function.

> Like encouraging shorter, simpler answers instead of overly complicated ones.

### 4. Scaling Laws

**Empirical relationships showing how performance tends to improve as model size, data, or compute increase.** These relationships follow predictable power-law curves.

Scaling laws help predict resource requirements for target performance levels.

> Like noticing that adding horsepower often increases a car's speed, but with diminishing returns.

### 5. Shadow Deployment

**Running a new model in parallel with production without affecting live user decisions.** The shadow model processes real traffic but its outputs are only logged, not served.

This allows safe evaluation before full deployment.

> Like a new chef preparing the same dishes in the back kitchen before serving customers.

## Quick Reference

| Concept | One-liner |
|---------|-----------|
| **Benchmark Leakage** | Test data contaminating training/selection |
| **Concept vs Data Drift** | Changed relationships vs changed inputs |
| **Weight Decay** | L2 penalty discourages large weights |
| **Scaling Laws** | Performance scales predictably with resources |
| **Shadow Deployment** | Test safely alongside production |

---

*Part 17 of the Five ML Concepts series. [View all parts](/series/#five-ml-concepts) | [Next: #18 â†’](/2026/02/21/five-ml-concepts-18/)*

*Short, accurate ML explainers. Follow for more.*

