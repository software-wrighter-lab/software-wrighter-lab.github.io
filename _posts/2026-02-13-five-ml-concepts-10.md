---
layout: post
title: "Five ML Concepts - #10"
date: 2026-02-13 00:00:00 -0800
categories: [llm, machine-learning, explainers]
tags: [five-ml-concepts, cnn, encoder-decoder, rag, few-shot-learning, distillation, ml-concepts]
keywords: "CNN, convolutional neural network, encoder-decoder, RAG, retrieval-augmented generation, few-shot learning, distillation, machine learning concepts, 30-second explainers"
author: Software Wrighter
series: "Five ML Concepts"
series_part: 10
video_url: "https://www.youtube.com/shorts/lVPnKvi9VdA"
papers:
  - title: "Sequence to Sequence Learning with Neural Networks"
    url: "https://arxiv.org/abs/1409.3215"
  - title: "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
    url: "https://arxiv.org/abs/2005.11401"
  - title: "Language Models are Few-Shot Learners"
    url: "https://arxiv.org/abs/2005.14165"
  - title: "Distilling the Knowledge in a Neural Network"
    url: "https://arxiv.org/abs/1503.02531"
---

<img src="/assets/images/posts/block-ten.png" class="post-marker" alt="">

5 machine learning concepts. Under 30 seconds each.

<div class="resource-box" markdown="1">

| Resource | Link |
|----------|------|
| **Papers** | Links in References section |
| **Video** | [Five ML Concepts #10](https://www.youtube.com/shorts/lVPnKvi9VdA)<br>[![Video](https://img.youtube.com/vi/lVPnKvi9VdA/mqdefault.jpg){: .video-thumb}](https://www.youtube.com/shorts/lVPnKvi9VdA) |

</div>

<div class="references-section" markdown="1">

## References

| Concept | Reference |
|---------|-----------|
| **CNN** | [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) (Krizhevsky et al. 2012) |
| **Encoder-Decoder** | [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) (Sutskever et al. 2014) |
| **RAG** | [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) (Lewis et al. 2020) |
| **Few-shot Learning** | [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) (Brown et al. 2020) |
| **Distillation** | [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) (Hinton et al. 2015) |

</div>

<div style="display:none">

</div>

## Today's Five

### 1. CNN (Convolutional Neural Network)

**Networks designed for image data that use small filters sliding across an image to detect edges, textures, and shapes.** Early layers find simple patterns, while deeper layers recognize complex objects.

CNNs are a foundation of modern computer vision.

> Like scanning a photo with a magnifying glass that learns to recognize patterns at different scales.

### 2. Encoder-Decoder

**A model architecture with two parts: the encoder compresses input into a representation, and the decoder generates an output from that representation.** This pattern is common in translation, summarization, and speech systems.

The representation acts as a bottleneck that captures essential information.

> Like summarizing a book into notes, then writing a new version from those notes.

### 3. RAG (Retrieval-Augmented Generation)

**Instead of relying only on learned parameters, the model retrieves relevant documents and uses them during generation.** This helps ground responses in external information and can reduce hallucinations.

RAG combines the strengths of retrieval systems and generative models.

> Like an open-book exam where you can look up facts instead of relying purely on memory.

### 4. Few-shot Learning

**Adapting behavior from just a few examples provided directly in the prompt.** Instead of retraining, the model infers the pattern and applies it to new inputs.

Zero-shot learning relies only on instructions, without examples.

> Like learning a card game by watching a few hands before playing.

### 5. Distillation

**Transferring knowledge from a large teacher model to a smaller student.** The student learns to match the teacher's outputs, not its internal weights.

This produces models that are smaller and cheaper while retaining much of the original capability.

> Like an apprentice learning by imitating a master's finished work, not by copying their brain.

## Quick Reference

| Concept | One-liner |
|---------|-----------|
| **CNN** | Sliding filters for hierarchical image features |
| **Encoder-Decoder** | Compress input, then generate output |
| **RAG** | Retrieve context before generating |
| **Few-shot Learning** | Learn from examples in the prompt |
| **Distillation** | Small student mimics large teacher |

---

*Part 10 of the Five ML Concepts series. [View all parts](/series/#five-ml-concepts) | [Next: #11 â†’](/2026/02/14/five-ml-concepts-11/)*

*Short, accurate ML explainers. Follow for more.*
