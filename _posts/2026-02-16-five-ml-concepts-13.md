---
layout: post
title: "Five ML Concepts - #13"
date: 2026-02-16 00:00:00 -0800
categories: [llm, machine-learning, explainers]
tags: [five-ml-concepts, calibration, shortcut-learning, early-stopping, universal-approximation, checkpointing, ml-concepts]
keywords: "calibration, shortcut learning, early stopping, universal approximation theorem, checkpointing, machine learning concepts, 30-second explainers"
author: Software Wrighter
series: "Five ML Concepts"
series_part: 13
video_url: "https://www.youtube.com/shorts/WPAk79_UONE"
---

<img src="/assets/images/posts/block-thirteen.png" class="post-marker" alt="">

5 machine learning concepts. Under 30 seconds each.

<div class="resource-box" markdown="1">

| Resource | Link |
|----------|------|
| **Papers** | Links in References section |
| **Video** | Coming soon |

</div>

<div class="references-section" markdown="1">

## References

| Concept | Reference |
|---------|-----------|
| **Calibration** | [On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599) (Guo et al. 2017) |
| **Shortcut Learning** | [Shortcut Learning in Deep Neural Networks](https://arxiv.org/abs/2004.07780) (Geirhos et al. 2020) |
| **Early Stopping** | [Early Stopping - But When?](https://page.mi.fu-berlin.de/prechelt/Biblio/stop_neurips98.pdf) (Prechelt 1998) |
| **Universal Approximation** | [Approximation by Superpositions of a Sigmoidal Function](https://www.sciencedirect.com/science/article/abs/pii/0893608089900208) (Cybenko 1989) |
| **Checkpointing** | [Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/abs/1604.06174) (Chen et al. 2016) |

</div>

## Today's Five

### 1. Calibration

**How well a model's predicted probabilities match real-world outcomes.** If a model predicts 70% confidence many times, it should be correct about 70% of those cases.

Well-calibrated models enable better decision-making under uncertainty.

> Like a weather forecast that predicts rain 30% of the time and is right about 30% of those forecasts.

### 2. Shortcut Learning

**When models rely on superficial patterns instead of meaningful features.** For example, identifying cows by detecting grass and failing when cows appear indoors.

Shortcuts can inflate benchmark scores while masking poor real-world performance.

> Like passing a test by memorizing answer positions instead of learning the material.

### 3. Early Stopping

**Training is stopped when validation performance stops improving.** This helps prevent overfitting by halting before the model memorizes training data.

Patience hyperparameters control how long to wait before stopping.

> Like knowing when to stop practicing before you start reinforcing mistakes.

### 4. Universal Approximation

**The theorem stating that neural networks can approximate any continuous function, given enough capacity.** In practice, finding the right weights through optimization is the challenge.

The theorem guarantees existence, not learnability.

> Like having enough Lego blocks to build almost any shape—assembly is still hard.

### 5. Checkpointing

**Saving the model's state during training.** This allows recovery from interruptions and comparison across training stages.

Checkpoints also enable selecting the best model rather than just the final one.

> Like saving your game progress so you can reload if something goes wrong.

## Quick Reference

| Concept | One-liner |
|---------|-----------|
| **Calibration** | Predicted probabilities match outcomes |
| **Shortcut Learning** | Exploiting spurious patterns |
| **Early Stopping** | Stop when validation plateaus |
| **Universal Approximation** | NNs can approximate any function |
| **Checkpointing** | Save model state during training |

---

*Part 13 of the Five ML Concepts series. [View all parts](/series/#five-ml-concepts) | [Next: #14 →](/2026/02/17/five-ml-concepts-14/)*

*Short, accurate ML explainers. Follow for more.*
